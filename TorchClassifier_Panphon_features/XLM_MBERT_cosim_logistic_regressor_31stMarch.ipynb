{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\\\n",
    "    , BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "# Setting PyTorch's required configuration variables for reproducibility.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE_TRAINED_MODEL = 'bert-base-multilingual-cased'\n",
    "PRE_TRAINED_MODEL = 'xlm-mlm-100-1280'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a512c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 5\n",
    "NUM_EPOCHS = 2000  # default maximum number of epochs\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "BS = 8  # batch size\n",
    "INITIAL_LR = 1e-5  # initial learning rate\n",
    "save_epochs = [1, 2, 3, 4, 5, 6, 7]  # these are the epoch numbers (starting from 1) to test the model on the test set\n",
    "# and save the model checkpoint.\n",
    "EARLY_STOP_PATIENCE = 30  # If model does not improve for this number of epochs, training stops.\n",
    "\n",
    "# Setting GPU cards to use for training the model. Make sure you read our paper to figure out if you have enough GPU\n",
    "# memory. If not, you can change all of them to 'cpu' to use CPU instead of GPU. By the way, two 24 GB GPU cards are\n",
    "# enough for current configuration, but in case of developing based on this you may need more (that's why there are\n",
    "# three cards declared here)\n",
    "# CUDA_0 = 'cuda:1'\n",
    "# CUDA_1 = 'cuda:1'\n",
    "# CUDA_2 = 'cuda:1'\n",
    "args = sys.argv\n",
    "epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/train_final.csv\")\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7173879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1 = list(original_df[\"loan_word\"])\n",
    "#l2 = list(original_df[\"original_word\"])\n",
    "#get the cosine similarities from M BERT from the train set\n",
    "l1 = list(original_df[\"loan_word\"])\n",
    "l2 = list(original_df[\"original_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df.loc[original_df[\"original_word\"]!='Refulgent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 512\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "#CUDA_0 = 'cuda:1'\n",
    "#CUDA_1 = 'cuda:1'\n",
    "#CUDA_2 = 'cuda:1'\n",
    "CUDA_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#CUDA_1 = 'cuda:0'\n",
    "#CUDA_2 = 'cuda:0'\n",
    "\n",
    "# The function for printing in both console and a given log file.\n",
    "def myprint(mystr, logfile):\n",
    "    print(mystr)\n",
    "    print(mystr, file=logfile)\n",
    "\n",
    "\n",
    "# The function for loading datasets from parallel tsv files and returning texts in lists.\n",
    "def load_data(file_name):\n",
    "    try:\n",
    "        # f = open(file_name)\n",
    "        f = pd.read_csv(file_name, sep='\\t', names=['l1_text', 'l2_text'])#, 'extra'])\n",
    "    except:\n",
    "        print('my log: could not read file')\n",
    "        exit()\n",
    "    print(\"This many number of rows were removed from \" + file_name.split(\"/\")[-1] + \" due to having missing values: \",\n",
    "          f.shape[0] - f.dropna().shape[0])\n",
    "    f.dropna(inplace=True)\n",
    "    l1_texts = f['l1_text'].values.tolist()\n",
    "    l2_texts = f['l2_text'].values.tolist()\n",
    "    print(len(l1_texts), len(l2_texts))\n",
    "    print(l1_texts[500])\n",
    "    print(\"\\n\")\n",
    "    print(l2_texts[500])\n",
    "    return l1_texts, l2_texts\n",
    "\n",
    "\n",
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyDataset1(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    # Each component other than the Transformer, are in a sequential layer (it is not required obviously, but it is\n",
    "    # possible to stack them with other layers if desired)\n",
    "    def __init__(self, base_model, n_classes, dropout=0.05):\n",
    "        super().__init__()\n",
    "        # self.base_model = base_model.to(CUDA_0)\n",
    "        self.transformation_learner = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU()\n",
    "        ).to(CUDA_0)\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        l1_pooler_output = input\n",
    "        # l2 = input2\n",
    "        # if 'l1_attention_mask' in kwargs:\n",
    "        #     l1_attention_mask = kwargs['l1_attention_mask']\n",
    "            # l2_attention_mask = kwargs['l2_attention_mask']\n",
    "        # else:\n",
    "        #     print(\"my err: attention mask is not set, error maybe\")\n",
    "        # here we use only the CLS token\n",
    "        # l1_pooler_output = self.base_model(l1.to(CUDA_0), attention_mask=l1_attention_mask.to(CUDA_0)).pooler_output\n",
    "        myoutput = self.transformation_learner(l1_pooler_output)\n",
    "        return myoutput\n",
    "\n",
    "\n",
    "# The function to compute and print the performance measure scores using sklearn implementations.\n",
    "def evaluate_model(labels, predictions, titlestr, logfile):\n",
    "    myprint(titlestr, logfile)\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    myprint(\"Confusion matrix- \\n\" + str(conf_matrix), logfile)\n",
    "    acc_score = accuracy_score(labels, predictions)\n",
    "    myprint('  Accuracy Score: {0:.2f}'.format(acc_score), logfile)\n",
    "    myprint('Report', logfile)\n",
    "    cls_rep = classification_report(labels, predictions)\n",
    "    myprint(cls_rep, logfile)\n",
    "    return f1_score(labels, predictions)  # return f-1 for positive class (sarcasm) as the early stopping measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c139b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"key word pair number: \",len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddd656",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l1, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings = tokenizer(l2, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst = []\n",
    "    sim_lst_test = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        #sim_lst.extend(list(sims))\n",
    "        sim_lst_test.extend(list(sims))\n",
    "print(len(sim_lst_test))\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l1, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(l2, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l1_encodings = tokenizer(text =l5,text_pair = l6 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l2_encodings = tokenizer(text =l6,text_pair = l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    \n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        print(l1_vector.shape)\n",
    "        #print(l1_vector[0][0].shape,l2_vector[1].shape )\n",
    "        sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi.extend(list(sims))\n",
    "#print(len(sim_lst_equi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mbert_cos_similarity'] = sim_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284df6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['mbert_cos_similarity'] = sim_lst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['xlm_cos_similarity'] = xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d00be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['xlm_cos_similarity'] =xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbe866",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "equiv = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/datasets/Hindi-Persian-Synonyms.csv\")\n",
    "rand = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/datasets/Hindi-Persian-Randoms.csv\")\n",
    "train = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/train_final.csv\")\n",
    "test =pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the English word 'Refulgent out of train set'\n",
    "\n",
    "train = train.loc[train['original_word'] !='Refulgent'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the list of orig and loan words for getting the embeddings from mbert, and XLM for train and test sets\n",
    "\n",
    "l1 = list(train[\"loan_word\"])\n",
    "l2 = list(train[\"original_word\"])\n",
    "\n",
    "#l1 = list(test[\"loan_word\"])\n",
    "#l2 = list(test[\"original_word\"])\n",
    "len(l1), len(l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e31d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "l5 = list(train['loan_word'])\n",
    "l6=list(train['original_word'])\n",
    "len(l5), len(l6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89290e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "l7 = l5[0:7]\n",
    "l8=l6[0:7]\n",
    "l7,l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "l1_encodings = tokenizer(text =l7,text_pair = l8 , truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "l2_encodings = tokenizer(text =l8,text_pair = l7, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "l2_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85eb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[0, 47790, 7657, 1, 179243, 167068, 1, 2]\n",
    "tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "l7, l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d643b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    #l1_encodings = tokenizer(l7, truncation=True, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\")\n",
    "    #l2_encodings = tokenizer(l8, truncation=True, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\")\n",
    "    l1_encodings = tokenizer(text =l7,text_pair = l8 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(text =l8,text_pair = l7, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi_exp = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0), output_hidden_states =True)[0]\n",
    "        print(len(l1_vector))\n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True)[0] \n",
    "        #print(l1_vector.hidden_states,l2_vector.hidden_states )\n",
    "        sims = cos_s(l1_vector[:,4,:],l2_vector[:,4,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi_exp.extend(list(sims))\n",
    "#print(len(sim_lst_equi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817eeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2aaddee",
   "metadata": {},
   "source": [
    "# Cosine similarities using XLM layers, middle and last. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(l6, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l1_encodings = tokenizer(text =l5,text_pair = l6 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l2_encodings = tokenizer(text =l6,text_pair = l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    \n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        #print(l1_vector[0][0].shape,l2_vector[1].shape )\n",
    "        sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi.extend(list(sims))\n",
    "#print(len(sim_lst_equi))\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0601e4",
   "metadata": {},
   "source": [
    "# Random words Cosine sim using SOS token no clear threshold/separation between equivalent words, no truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400ab4d",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using BOS/classification  token no clear threshold/separation between equivalent words, no truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333adc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cae76",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using middle token, some threshold emerges between cosine sims , no truncation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60217b3f",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using EOS token, huge peak with cos sim of 1 , no truncation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13788c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"xlm_cos_similarity\"] = xlm_sim_lst_equi\n",
    "equiv[\"mbert_cos_similarity\"] =sim_lst\n",
    "equiv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5907ac",
   "metadata": {},
   "source": [
    "# Mean and Std for XLM cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(xlm_sim_lst_equi)\n",
    " \n",
    "ordered_std = np.std(xlm_sim_lst_equi)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb19448",
   "metadata": {},
   "source": [
    "# Mean and Std for mbert cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(sim_lst)\n",
    " \n",
    "ordered_std = np.std(sim_lst)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d049c62",
   "metadata": {},
   "source": [
    "# Outliers from Z score for XLM cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ce552",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "xlm_ordered_outlier_idx = []\n",
    "for i in range(len(xlm_sim_lst_equi)):\n",
    "    z = abs(xlm_sim_lst_equi[i]-ordered_mean)/ordered_std\n",
    "    if z > threshold:\n",
    "        xlm_ordered_outlier_idx.append(i)\n",
    "print('outlier indices are', xlm_ordered_outlier_idx, len(xlm_ordered_outlier_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier = equiv.loc[xlm_ordered_outlier_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb8ddd",
   "metadata": {},
   "source": [
    "# Outliers with low cos sim from XLM and Mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos = xlm_equiv_outlier.loc[(xlm_equiv_outlier['xlm_cos_similarity'] <0.35) & (xlm_equiv_outlier['mbert_cos_similarity']<0.35)]\n",
    "equiv_outlier_lowcos.to_csv('Outliers_XLM_Mbert_lowcosim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_lowcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787b785",
   "metadata": {},
   "source": [
    "# Outliers with high cos sim from XLM and Mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fff3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a176961",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equiv.loc[(equiv['xlm_cos_similarity'] >0.68) & (equiv['mbert_cos_similarity']>0.6)]\n",
    "equiv_outlier_highcos.to_csv('Outliers_XLM_Mbert_highcosim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(equiv_outlier_highcos['xlm_cos_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e710dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_highcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016332b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43241525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f323611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b5243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier_lowcos = equiv.loc[equiv['xlm_cos_similarity'] <0.23]\n",
    "xlm_equiv_outlier_lowcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa10f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier_highcos = equiv.loc[equiv['xlm_cos_similarity'] ==1]\n",
    "xlm_equiv_outlier_highcos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56b2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06edc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf47435",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"xlm_cos_similarity\"] = xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85c949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66507599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00bcc509",
   "metadata": {},
   "source": [
    "# Cosine similarity and Outliers using M-bert cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(sim_lst_equi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"cos_similarity\"] = sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a089b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(sim_lst_equi)\n",
    " \n",
    "ordered_std = np.std(sim_lst_equi)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "ordered_outlier_idx = []\n",
    "for i in range(len(sim_lst_equi)):\n",
    "    z = abs(sim_lst_equi[i]-ordered_mean)/ordered_std\n",
    "    if z > threshold:\n",
    "        ordered_outlier_idx.append(i)\n",
    "print('outlier indice are', ordered_outlier_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c355cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_outlier_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "equov_outlier = equiv.loc[ordered_outlier_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "equov_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(equov_outlier['cos_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos = equov_outlier.loc[equov_outlier['cos_similarity'] <0.23]\n",
    "equiv_outlier_lowcos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos[[c for c in equiv_outlier_lowcos.columns if c in edit_dists_names]]\n",
    "equiv_outlier_lowcos = equiv_outlier_lowcos[[*edit_dists_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_dists_names = ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen',\n",
    "        ]\n",
    "dist_arr = np.array(edit_dists_names).reshape(-1,3)\n",
    "dist_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be505aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_lowcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equov_outlier.loc[equov_outlier['cos_similarity'] >0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ad94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equiv_outlier_highcos[[*edit_dists_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d25944",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_highcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbe88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac0b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e151a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e991a",
   "metadata": {},
   "source": [
    "# Get the logits for the train from the cnn/dnn for training the logistic regressor after padding to max length and concatenating loan and original word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ef9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a809e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda36dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get phonetic features using panPhon\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['features_loan'] = original_df.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "original_df['features_orig'] = original_df.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features for test set \n",
    "test['features_loan'] = original_df.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test['features_orig'] = original_df.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e39c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a flat list of the features for both orig and loan words\n",
    "#original_df['features_loan'] = original_df['features_loan'].apply(lambda x:sum(x, []))\n",
    "original_df['features_orig'] = original_df['features_orig'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd882a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['features_orig'] = test['features_orig'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['features_loan'] = test['features_loan'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a02d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_loan = original_df.loan_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()\n",
    "max_len_orig = original_df.original_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ec4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_loan = test.loan_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()\n",
    "max_len_orig = test.original_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_loan, max_len_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padarray(A, size):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train set \n",
    "pad_idx_loan = original_df['features_loan'].apply(len)\n",
    "pad_idx_orig = original_df['features_orig'].apply(len)\n",
    "pad_idx_loan = np.array(pad_idx_loan)\n",
    "pad_idx_orig = np.array(pad_idx_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for test set\n",
    "pad_idx_loan = test['features_loan'].apply(len)\n",
    "pad_idx_orig = test['features_orig'].apply(len)\n",
    "pad_idx_loan = np.array(pad_idx_loan)\n",
    "pad_idx_orig = np.array(pad_idx_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx_loan, pad_idx_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train \n",
    "# arr_loan = np.asarray(original_df['features_loan'])\n",
    "# arr_orig = np.asarray(original_df['features_orig'])\n",
    "#for test \n",
    "arr_loan = np.asarray(test['features_loan'])\n",
    "arr_orig = np.asarray(test['features_orig'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_loan = []\n",
    "for i in range(len(arr_loan)):\n",
    "    a = padarray(arr_loan[i], max_len_loan*24)\n",
    "    l_loan.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_orig = []\n",
    "for i in range(len(arr_orig)):\n",
    "    a = padarray(arr_orig[i], max_len_orig*24)\n",
    "    l_orig.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608367f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_comb = np.concatenate((l_loan, l_orig), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3feeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ca07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = np.array(test['label']).reshape((-1,1))\n",
    "lables.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9409d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.append(l_comb, lables, axis=1) \n",
    "#x_test = np.append(l_comb, lables, axis=1) \n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape\n",
    "%store x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42240b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dae8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f490851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 889)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07589d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits\n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92451ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = x[:,0:1056]\n",
    "# Y_train = x[:,1056] \n",
    "\n",
    "X_test = x_test[:,0:888]\n",
    "Y_test = x_test[:,888] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd732872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((503, 888), (503,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape\n",
    "X_test.shape, Y_test.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_test.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb15a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c1bdcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.tensor(X_train).to(device)\n",
    "#Y_train = torch.tensor(Y_train).to(device)\n",
    "\n",
    "X_test= torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7465040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e424d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train for 10000 epochs and get the logits \n",
    "test_losses = []\n",
    "train_losses = []\n",
    "test_accur = []\n",
    "train_accur = []\n",
    "logits = []\n",
    "for epoch in range(1000):\n",
    "\n",
    "#     y_pred = model(X_train.float())[0]\n",
    "#     logits = model(X_train.float())[1]\n",
    "    #getting logits for test set \n",
    "    y_pred = model(X_test.float())[0]\n",
    "    logits = model(X_test.float())[1]\n",
    "    #y_pred = model(X_train) \n",
    "    #print(y_pred)\n",
    "\n",
    "    #y_pred = torch.squeeze(y_pred)\n",
    "    #train_loss = criterion(y_pred, Y_train.float())\n",
    "    \n",
    "    test_loss = criterion(y_pred, Y_test.float())\n",
    "    #train_loss = criterion(y_pred, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        #train_acc,_ = calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "        #y_test_pred = model(X_test.float())\n",
    "        #y_test_pred = torch.squeeze(y_test_pred)\n",
    "         \n",
    "\n",
    "        #test_loss = criterion(y_test_pred, Y_test.float())\n",
    "\n",
    "        #test_acc, total_corr = calculate_accuracy(Y_test, y_test_pred)\n",
    "        #print(total_corr)\n",
    "        \n",
    "#         print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}Test  set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}\n",
    "# ''')\n",
    "        #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "        print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "        #train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #test_accur.append(test_acc)\n",
    "        #train_accur.append(train_acc)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #train_loss.backward()\n",
    "    test_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169af1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3339e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DNN_logits'] = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['DNN_logits'] = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(120, 1056, device=device)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f53424",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "825e80e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 2) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128,2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        x = torch.randn(33,33).view(-1,1,33,33)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.tanh(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9681d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0df0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#X_train = torch.tensor(X_train).view(-1,23,23).to(device)\n",
    "X_train_CNN = torch.tensor(X_train_CNN).view(-1,33,33).to(device)\n",
    "#X_train = torch.tensor(X_train).to(device)\n",
    "#Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_train_CNN = torch.tensor(Y_train_CNN).to(device)\n",
    "#X_test = torch.tensor(X_test).view(-1,23,23).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77ef412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#X_train = torch.tensor(X_train)\n",
    "#Y_train = torch.tensor(Y_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "Y_test = torch.tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc9f02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_CNN = F.pad(X_train,pad =(0, 1089-X_train.shape[1]), value=0)\n",
    "# Y_train_CNN = Y_train\n",
    "\n",
    "X_test_CNN = F.pad(X_test,pad =(0, 900-X_test.shape[1]), value=0)\n",
    "Y_test_CNN = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4d = torch.ones(3, 3)\n",
    "p1d = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4d = F.pad(t4d,pad =(0, 10-t4d.shape[1]), value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0098167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([503, 900]), torch.Size([503]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CNN.shape, Y_train_CNN.shape\n",
    "X_test_CNN.shape, Y_test_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b756bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aad800b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "662be602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.7063742876052856\n",
      "Epoch: 1. Loss: 0.5991889238357544\n",
      "Epoch: 2. Loss: 0.5888549089431763\n",
      "Epoch: 3. Loss: 0.6023119688034058\n",
      "Epoch: 4. Loss: 0.5809440016746521\n",
      "Epoch: 5. Loss: 0.5836127996444702\n",
      "Epoch: 6. Loss: 0.5859370231628418\n",
      "Epoch: 7. Loss: 0.5740256905555725\n",
      "Epoch: 8. Loss: 0.5802811980247498\n",
      "Epoch: 9. Loss: 0.5800039768218994\n",
      "Epoch: 10. Loss: 0.5723046064376831\n",
      "Epoch: 11. Loss: 0.5765231251716614\n",
      "Epoch: 12. Loss: 0.5774567127227783\n",
      "Epoch: 13. Loss: 0.5720203518867493\n",
      "Epoch: 14. Loss: 0.5728488564491272\n",
      "Epoch: 15. Loss: 0.5752995014190674\n",
      "Epoch: 16. Loss: 0.5717333555221558\n",
      "Epoch: 17. Loss: 0.5702516436576843\n",
      "Epoch: 18. Loss: 0.5723554491996765\n",
      "Epoch: 19. Loss: 0.5715431571006775\n",
      "Epoch: 20. Loss: 0.5691465735435486\n",
      "Epoch: 21. Loss: 0.5697203874588013\n",
      "Epoch: 22. Loss: 0.570658802986145\n",
      "Epoch: 23. Loss: 0.5690457820892334\n",
      "Epoch: 24. Loss: 0.5681570172309875\n",
      "Epoch: 25. Loss: 0.5691103339195251\n",
      "Epoch: 26. Loss: 0.5686838626861572\n",
      "Epoch: 27. Loss: 0.5672973394393921\n",
      "Epoch: 28. Loss: 0.5675802230834961\n",
      "Epoch: 29. Loss: 0.5678008198738098\n",
      "Epoch: 30. Loss: 0.5666117072105408\n",
      "Epoch: 31. Loss: 0.566484272480011\n",
      "Epoch: 32. Loss: 0.5668365955352783\n",
      "Epoch: 33. Loss: 0.5659117698669434\n",
      "Epoch: 34. Loss: 0.5657070875167847\n",
      "Epoch: 35. Loss: 0.5659419298171997\n",
      "Epoch: 36. Loss: 0.5650790929794312\n",
      "Epoch: 37. Loss: 0.5650167465209961\n",
      "Epoch: 38. Loss: 0.5649537444114685\n",
      "Epoch: 39. Loss: 0.5641921162605286\n",
      "Epoch: 40. Loss: 0.5643022060394287\n",
      "Epoch: 41. Loss: 0.5638567805290222\n",
      "Epoch: 42. Loss: 0.563456118106842\n",
      "Epoch: 43. Loss: 0.5634534358978271\n",
      "Epoch: 44. Loss: 0.5628058314323425\n",
      "Epoch: 45. Loss: 0.5627897381782532\n",
      "Epoch: 46. Loss: 0.5622628927230835\n",
      "Epoch: 47. Loss: 0.5620249509811401\n",
      "Epoch: 48. Loss: 0.5616727471351624\n",
      "Epoch: 49. Loss: 0.5612698197364807\n",
      "Epoch: 50. Loss: 0.5609917640686035\n",
      "Epoch: 51. Loss: 0.5605315566062927\n",
      "Epoch: 52. Loss: 0.5602318048477173\n",
      "Epoch: 53. Loss: 0.559779167175293\n",
      "Epoch: 54. Loss: 0.5593882203102112\n",
      "Epoch: 55. Loss: 0.5590041279792786\n",
      "Epoch: 56. Loss: 0.5584695339202881\n",
      "Epoch: 57. Loss: 0.558163046836853\n",
      "Epoch: 58. Loss: 0.5575511455535889\n",
      "Epoch: 59. Loss: 0.5571339130401611\n",
      "Epoch: 60. Loss: 0.5566748380661011\n",
      "Epoch: 61. Loss: 0.5560216307640076\n",
      "Epoch: 62. Loss: 0.5555123686790466\n",
      "Epoch: 63. Loss: 0.5550437569618225\n",
      "Epoch: 64. Loss: 0.5544283986091614\n",
      "Epoch: 65. Loss: 0.5537225604057312\n",
      "Epoch: 66. Loss: 0.5530329942703247\n",
      "Epoch: 67. Loss: 0.5523741245269775\n",
      "Epoch: 68. Loss: 0.551754891872406\n",
      "Epoch: 69. Loss: 0.5513414740562439\n",
      "Epoch: 70. Loss: 0.5518696308135986\n",
      "Epoch: 71. Loss: 0.5556771159172058\n",
      "Epoch: 72. Loss: 0.5580469369888306\n",
      "Epoch: 73. Loss: 0.5527228116989136\n",
      "Epoch: 74. Loss: 0.5477553009986877\n",
      "Epoch: 75. Loss: 0.5535006523132324\n",
      "Epoch: 76. Loss: 0.5492805242538452\n",
      "Epoch: 77. Loss: 0.5465285778045654\n",
      "Epoch: 78. Loss: 0.5501961708068848\n",
      "Epoch: 79. Loss: 0.5442811250686646\n",
      "Epoch: 80. Loss: 0.5461419820785522\n",
      "Epoch: 81. Loss: 0.5444239377975464\n",
      "Epoch: 82. Loss: 0.542018473148346\n",
      "Epoch: 83. Loss: 0.5436442494392395\n",
      "Epoch: 84. Loss: 0.5398516654968262\n",
      "Epoch: 85. Loss: 0.5409814119338989\n",
      "Epoch: 86. Loss: 0.5395687818527222\n",
      "Epoch: 87. Loss: 0.5371172428131104\n",
      "Epoch: 88. Loss: 0.5383678078651428\n",
      "Epoch: 89. Loss: 0.5358542799949646\n",
      "Epoch: 90. Loss: 0.5339891314506531\n",
      "Epoch: 91. Loss: 0.5348808765411377\n",
      "Epoch: 92. Loss: 0.5330305695533752\n",
      "Epoch: 93. Loss: 0.5301397442817688\n",
      "Epoch: 94. Loss: 0.5302367806434631\n",
      "Epoch: 95. Loss: 0.5306974053382874\n",
      "Epoch: 96. Loss: 0.5283287167549133\n",
      "Epoch: 97. Loss: 0.5254581570625305\n",
      "Epoch: 98. Loss: 0.5235421657562256\n",
      "Epoch: 99. Loss: 0.522928774356842\n",
      "Epoch: 100. Loss: 0.5237753391265869\n",
      "Epoch: 101. Loss: 0.5267327427864075\n",
      "Epoch: 102. Loss: 0.536191999912262\n",
      "Epoch: 103. Loss: 0.5329877734184265\n",
      "Epoch: 104. Loss: 0.5224964618682861\n",
      "Epoch: 105. Loss: 0.5143390893936157\n",
      "Epoch: 106. Loss: 0.5230154395103455\n",
      "Epoch: 107. Loss: 0.5220897793769836\n",
      "Epoch: 108. Loss: 0.5101745128631592\n",
      "Epoch: 109. Loss: 0.5192849040031433\n",
      "Epoch: 110. Loss: 0.5166121125221252\n",
      "Epoch: 111. Loss: 0.5067592263221741\n",
      "Epoch: 112. Loss: 0.5167228579521179\n",
      "Epoch: 113. Loss: 0.5085276961326599\n",
      "Epoch: 114. Loss: 0.5045327544212341\n",
      "Epoch: 115. Loss: 0.510629415512085\n",
      "Epoch: 116. Loss: 0.5000598430633545\n",
      "Epoch: 117. Loss: 0.5016338229179382\n",
      "Epoch: 118. Loss: 0.5012493133544922\n",
      "Epoch: 119. Loss: 0.4937477707862854\n",
      "Epoch: 120. Loss: 0.4963429868221283\n",
      "Epoch: 121. Loss: 0.49302276968955994\n",
      "Epoch: 122. Loss: 0.48824992775917053\n",
      "Epoch: 123. Loss: 0.48943889141082764\n",
      "Epoch: 124. Loss: 0.4868309497833252\n",
      "Epoch: 125. Loss: 0.4822145402431488\n",
      "Epoch: 126. Loss: 0.48084959387779236\n",
      "Epoch: 127. Loss: 0.480779767036438\n",
      "Epoch: 128. Loss: 0.4792766869068146\n",
      "Epoch: 129. Loss: 0.47510865330696106\n",
      "Epoch: 130. Loss: 0.47125256061553955\n",
      "Epoch: 131. Loss: 0.4680013060569763\n",
      "Epoch: 132. Loss: 0.46533140540122986\n",
      "Epoch: 133. Loss: 0.46314820647239685\n",
      "Epoch: 134. Loss: 0.4628855586051941\n",
      "Epoch: 135. Loss: 0.47449278831481934\n",
      "Epoch: 136. Loss: 0.5258364677429199\n",
      "Epoch: 137. Loss: 0.6219280958175659\n",
      "Epoch: 138. Loss: 0.458312064409256\n",
      "Epoch: 139. Loss: 0.6472519636154175\n",
      "Epoch: 140. Loss: 0.4744885265827179\n",
      "Epoch: 141. Loss: 0.5714857578277588\n",
      "Epoch: 142. Loss: 0.46492111682891846\n",
      "Epoch: 143. Loss: 0.5458330512046814\n",
      "Epoch: 144. Loss: 0.4673401117324829\n",
      "Epoch: 145. Loss: 0.48980116844177246\n",
      "Epoch: 146. Loss: 0.5006341338157654\n",
      "Epoch: 147. Loss: 0.4545215666294098\n",
      "Epoch: 148. Loss: 0.48959341645240784\n",
      "Epoch: 149. Loss: 0.4716760814189911\n",
      "Epoch: 150. Loss: 0.4552289843559265\n",
      "Epoch: 151. Loss: 0.47783830761909485\n",
      "Epoch: 152. Loss: 0.4626513719558716\n",
      "Epoch: 153. Loss: 0.44972383975982666\n",
      "Epoch: 154. Loss: 0.466334730386734\n",
      "Epoch: 155. Loss: 0.452045202255249\n",
      "Epoch: 156. Loss: 0.4461938440799713\n",
      "Epoch: 157. Loss: 0.45656874775886536\n",
      "Epoch: 158. Loss: 0.4449490010738373\n",
      "Epoch: 159. Loss: 0.4400371015071869\n",
      "Epoch: 160. Loss: 0.4468187391757965\n",
      "Epoch: 161. Loss: 0.43491414189338684\n",
      "Epoch: 162. Loss: 0.43492379784584045\n",
      "Epoch: 163. Loss: 0.4371192455291748\n",
      "Epoch: 164. Loss: 0.42694732546806335\n",
      "Epoch: 165. Loss: 0.42929762601852417\n",
      "Epoch: 166. Loss: 0.42558324337005615\n",
      "Epoch: 167. Loss: 0.4197848439216614\n",
      "Epoch: 168. Loss: 0.42228415608406067\n",
      "Epoch: 169. Loss: 0.41449496150016785\n",
      "Epoch: 170. Loss: 0.4147135317325592\n",
      "Epoch: 171. Loss: 0.4108879566192627\n",
      "Epoch: 172. Loss: 0.40723758935928345\n",
      "Epoch: 173. Loss: 0.4063332974910736\n",
      "Epoch: 174. Loss: 0.4001844525337219\n",
      "Epoch: 175. Loss: 0.4006408452987671\n",
      "Epoch: 176. Loss: 0.39454713463783264\n",
      "Epoch: 177. Loss: 0.3943636417388916\n",
      "Epoch: 178. Loss: 0.388489693403244\n",
      "Epoch: 179. Loss: 0.3880200982093811\n",
      "Epoch: 180. Loss: 0.38251134753227234\n",
      "Epoch: 181. Loss: 0.3813500702381134\n",
      "Epoch: 182. Loss: 0.3766998052597046\n",
      "Epoch: 183. Loss: 0.3741397559642792\n",
      "Epoch: 184. Loss: 0.3712168037891388\n",
      "Epoch: 185. Loss: 0.36677977442741394\n",
      "Epoch: 186. Loss: 0.3652121126651764\n",
      "Epoch: 187. Loss: 0.36095958948135376\n",
      "Epoch: 188. Loss: 0.35694730281829834\n",
      "Epoch: 189. Loss: 0.3551248610019684\n",
      "Epoch: 190. Loss: 0.35168156027793884\n",
      "Epoch: 191. Loss: 0.3468281328678131\n",
      "Epoch: 192. Loss: 0.3436413109302521\n",
      "Epoch: 193. Loss: 0.3416060209274292\n",
      "Epoch: 194. Loss: 0.33940425515174866\n",
      "Epoch: 195. Loss: 0.337623655796051\n",
      "Epoch: 196. Loss: 0.33598896861076355\n",
      "Epoch: 197. Loss: 0.3388117551803589\n",
      "Epoch: 198. Loss: 0.34429648518562317\n",
      "Epoch: 199. Loss: 0.3666903078556061\n",
      "Epoch: 200. Loss: 0.35507112741470337\n",
      "Epoch: 201. Loss: 0.33794817328453064\n",
      "Epoch: 202. Loss: 0.31174519658088684\n",
      "Epoch: 203. Loss: 0.32367563247680664\n",
      "Epoch: 204. Loss: 0.3402240574359894\n",
      "Epoch: 205. Loss: 0.3092983365058899\n",
      "Epoch: 206. Loss: 0.3047051429748535\n",
      "Epoch: 207. Loss: 0.32255417108535767\n",
      "Epoch: 208. Loss: 0.3062802851200104\n",
      "Epoch: 209. Loss: 0.29159459471702576\n",
      "Epoch: 210. Loss: 0.29731351137161255\n",
      "Epoch: 211. Loss: 0.29956069588661194\n",
      "Epoch: 212. Loss: 0.2905411124229431\n",
      "Epoch: 213. Loss: 0.2797756493091583\n",
      "Epoch: 214. Loss: 0.28172579407691956\n",
      "Epoch: 215. Loss: 0.2879596948623657\n",
      "Epoch: 216. Loss: 0.2815564274787903\n",
      "Epoch: 217. Loss: 0.27208298444747925\n",
      "Epoch: 218. Loss: 0.26451221108436584\n",
      "Epoch: 219. Loss: 0.26404228806495667\n",
      "Epoch: 220. Loss: 0.26789918541908264\n",
      "Epoch: 221. Loss: 0.2701122462749481\n",
      "Epoch: 222. Loss: 0.27393046021461487\n",
      "Epoch: 223. Loss: 0.26761123538017273\n",
      "Epoch: 224. Loss: 0.26308244466781616\n",
      "Epoch: 225. Loss: 0.2519020736217499\n",
      "Epoch: 226. Loss: 0.243996262550354\n",
      "Epoch: 227. Loss: 0.2382836937904358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228. Loss: 0.2350544035434723\n",
      "Epoch: 229. Loss: 0.23377175629138947\n",
      "Epoch: 230. Loss: 0.23524489998817444\n",
      "Epoch: 231. Loss: 0.24560222029685974\n",
      "Epoch: 232. Loss: 0.27084094285964966\n",
      "Epoch: 233. Loss: 0.3504357635974884\n",
      "Epoch: 234. Loss: 0.30264776945114136\n",
      "Epoch: 235. Loss: 0.2538226544857025\n",
      "Epoch: 236. Loss: 0.219333678483963\n",
      "Epoch: 237. Loss: 0.26111456751823425\n",
      "Epoch: 238. Loss: 0.29375335574150085\n",
      "Epoch: 239. Loss: 0.2163359373807907\n",
      "Epoch: 240. Loss: 0.25017470121383667\n",
      "Epoch: 241. Loss: 0.287396103143692\n",
      "Epoch: 242. Loss: 0.21517322957515717\n",
      "Epoch: 243. Loss: 0.2548990845680237\n",
      "Epoch: 244. Loss: 0.27993494272232056\n",
      "Epoch: 245. Loss: 0.20637032389640808\n",
      "Epoch: 246. Loss: 0.2652670443058014\n",
      "Epoch: 247. Loss: 0.26243528723716736\n",
      "Epoch: 248. Loss: 0.20617039501667023\n",
      "Epoch: 249. Loss: 0.2793586850166321\n",
      "Epoch: 250. Loss: 0.21910782158374786\n",
      "Epoch: 251. Loss: 0.22069214284420013\n",
      "Epoch: 252. Loss: 0.2431306391954422\n",
      "Epoch: 253. Loss: 0.19229653477668762\n",
      "Epoch: 254. Loss: 0.2226877212524414\n",
      "Epoch: 255. Loss: 0.19772636890411377\n",
      "Epoch: 256. Loss: 0.19630500674247742\n",
      "Epoch: 257. Loss: 0.2054654210805893\n",
      "Epoch: 258. Loss: 0.18265962600708008\n",
      "Epoch: 259. Loss: 0.19920429587364197\n",
      "Epoch: 260. Loss: 0.1848173886537552\n",
      "Epoch: 261. Loss: 0.18323661386966705\n",
      "Epoch: 262. Loss: 0.18720276653766632\n",
      "Epoch: 263. Loss: 0.17378419637680054\n",
      "Epoch: 264. Loss: 0.17947591841220856\n",
      "Epoch: 265. Loss: 0.17333325743675232\n",
      "Epoch: 266. Loss: 0.16758351027965546\n",
      "Epoch: 267. Loss: 0.17131929099559784\n",
      "Epoch: 268. Loss: 0.16432376205921173\n",
      "Epoch: 269. Loss: 0.16168580949306488\n",
      "Epoch: 270. Loss: 0.16277886927127838\n",
      "Epoch: 271. Loss: 0.1572469025850296\n",
      "Epoch: 272. Loss: 0.15555204451084137\n",
      "Epoch: 273. Loss: 0.1552717089653015\n",
      "Epoch: 274. Loss: 0.15135692059993744\n",
      "Epoch: 275. Loss: 0.14862188696861267\n",
      "Epoch: 276. Loss: 0.14839793741703033\n",
      "Epoch: 277. Loss: 0.14556412398815155\n",
      "Epoch: 278. Loss: 0.14186991751194\n",
      "Epoch: 279. Loss: 0.1416911780834198\n",
      "Epoch: 280. Loss: 0.13983522355556488\n",
      "Epoch: 281. Loss: 0.1370861977338791\n",
      "Epoch: 282. Loss: 0.13418613374233246\n",
      "Epoch: 283. Loss: 0.13309799134731293\n",
      "Epoch: 284. Loss: 0.13217557966709137\n",
      "Epoch: 285. Loss: 0.13005799055099487\n",
      "Epoch: 286. Loss: 0.1277264654636383\n",
      "Epoch: 287. Loss: 0.12492086738348007\n",
      "Epoch: 288. Loss: 0.12348058819770813\n",
      "Epoch: 289. Loss: 0.12164338678121567\n",
      "Epoch: 290. Loss: 0.12083074450492859\n",
      "Epoch: 291. Loss: 0.11945243179798126\n",
      "Epoch: 292. Loss: 0.11909043788909912\n",
      "Epoch: 293. Loss: 0.11946693807840347\n",
      "Epoch: 294. Loss: 0.12238887697458267\n",
      "Epoch: 295. Loss: 0.1334080547094345\n",
      "Epoch: 296. Loss: 0.1527269184589386\n",
      "Epoch: 297. Loss: 0.20829539000988007\n",
      "Epoch: 298. Loss: 0.204876109957695\n",
      "Epoch: 299. Loss: 0.19703322649002075\n",
      "Epoch: 300. Loss: 0.11440888792276382\n",
      "Epoch: 301. Loss: 0.14415429532527924\n",
      "Epoch: 302. Loss: 0.20380768179893494\n",
      "Epoch: 303. Loss: 0.12160027772188187\n",
      "Epoch: 304. Loss: 0.12593001127243042\n",
      "Epoch: 305. Loss: 0.17986927926540375\n",
      "Epoch: 306. Loss: 0.11139162629842758\n",
      "Epoch: 307. Loss: 0.12361914664506912\n",
      "Epoch: 308. Loss: 0.1593824326992035\n",
      "Epoch: 309. Loss: 0.10122878849506378\n",
      "Epoch: 310. Loss: 0.13354402780532837\n",
      "Epoch: 311. Loss: 0.14679519832134247\n",
      "Epoch: 312. Loss: 0.09746409207582474\n",
      "Epoch: 313. Loss: 0.1469496488571167\n",
      "Epoch: 314. Loss: 0.1389865130186081\n",
      "Epoch: 315. Loss: 0.09820576012134552\n",
      "Epoch: 316. Loss: 0.15140211582183838\n",
      "Epoch: 317. Loss: 0.1199934259057045\n",
      "Epoch: 318. Loss: 0.10198411345481873\n",
      "Epoch: 319. Loss: 0.13844655454158783\n",
      "Epoch: 320. Loss: 0.0945851281285286\n",
      "Epoch: 321. Loss: 0.10752841085195541\n",
      "Epoch: 322. Loss: 0.10785914957523346\n",
      "Epoch: 323. Loss: 0.08487067371606827\n",
      "Epoch: 324. Loss: 0.10474878549575806\n",
      "Epoch: 325. Loss: 0.08475924283266068\n",
      "Epoch: 326. Loss: 0.09132371842861176\n",
      "Epoch: 327. Loss: 0.09253934025764465\n",
      "Epoch: 328. Loss: 0.08036726713180542\n",
      "Epoch: 329. Loss: 0.09307479858398438\n",
      "Epoch: 330. Loss: 0.08036988973617554\n",
      "Epoch: 331. Loss: 0.08160151541233063\n",
      "Epoch: 332. Loss: 0.08451845496892929\n",
      "Epoch: 333. Loss: 0.07440873980522156\n",
      "Epoch: 334. Loss: 0.08031819015741348\n",
      "Epoch: 335. Loss: 0.07495186477899551\n",
      "Epoch: 336. Loss: 0.07296991348266602\n",
      "Epoch: 337. Loss: 0.0758482813835144\n",
      "Epoch: 338. Loss: 0.06971532851457596\n",
      "Epoch: 339. Loss: 0.07216550409793854\n",
      "Epoch: 340. Loss: 0.07075934112071991\n",
      "Epoch: 341. Loss: 0.06708524376153946\n",
      "Epoch: 342. Loss: 0.06972954422235489\n",
      "Epoch: 343. Loss: 0.06627350300550461\n",
      "Epoch: 344. Loss: 0.06505148112773895\n",
      "Epoch: 345. Loss: 0.06632538139820099\n",
      "Epoch: 346. Loss: 0.0628383681178093\n",
      "Epoch: 347. Loss: 0.06282944977283478\n",
      "Epoch: 348. Loss: 0.06274189054965973\n",
      "Epoch: 349. Loss: 0.06001207232475281\n",
      "Epoch: 350. Loss: 0.06045519560575485\n",
      "Epoch: 351. Loss: 0.05956558510661125\n",
      "Epoch: 352. Loss: 0.05767276510596275\n",
      "Epoch: 353. Loss: 0.057896122336387634\n",
      "Epoch: 354. Loss: 0.05681803822517395\n",
      "Epoch: 355. Loss: 0.05546936020255089\n",
      "Epoch: 356. Loss: 0.05543026328086853\n",
      "Epoch: 357. Loss: 0.0544612891972065\n",
      "Epoch: 358. Loss: 0.05329565331339836\n",
      "Epoch: 359. Loss: 0.05310855805873871\n",
      "Epoch: 360. Loss: 0.05223390832543373\n",
      "Epoch: 361. Loss: 0.05123913660645485\n",
      "Epoch: 362. Loss: 0.0509282723069191\n",
      "Epoch: 363. Loss: 0.05018068850040436\n",
      "Epoch: 364. Loss: 0.04928892105817795\n",
      "Epoch: 365. Loss: 0.048815321177244186\n",
      "Epoch: 366. Loss: 0.048254888504743576\n",
      "Epoch: 367. Loss: 0.04741380363702774\n",
      "Epoch: 368. Loss: 0.046881139278411865\n",
      "Epoch: 369. Loss: 0.046397022902965546\n",
      "Epoch: 370. Loss: 0.04566025361418724\n",
      "Epoch: 371. Loss: 0.04504279047250748\n",
      "Epoch: 372. Loss: 0.044585615396499634\n",
      "Epoch: 373. Loss: 0.043999768793582916\n",
      "Epoch: 374. Loss: 0.04334402084350586\n",
      "Epoch: 375. Loss: 0.042840566486120224\n",
      "Epoch: 376. Loss: 0.042358338832855225\n",
      "Epoch: 377. Loss: 0.04177122563123703\n",
      "Epoch: 378. Loss: 0.04120638221502304\n",
      "Epoch: 379. Loss: 0.04074892774224281\n",
      "Epoch: 380. Loss: 0.04025474563241005\n",
      "Epoch: 381. Loss: 0.03971090540289879\n",
      "Epoch: 382. Loss: 0.03920953720808029\n",
      "Epoch: 383. Loss: 0.038761451840400696\n",
      "Epoch: 384. Loss: 0.038294270634651184\n",
      "Epoch: 385. Loss: 0.037791658192873\n",
      "Epoch: 386. Loss: 0.03732387349009514\n",
      "Epoch: 387. Loss: 0.03689242899417877\n",
      "Epoch: 388. Loss: 0.03645603731274605\n",
      "Epoch: 389. Loss: 0.03599383682012558\n",
      "Epoch: 390. Loss: 0.035550519824028015\n",
      "Epoch: 391. Loss: 0.035137202590703964\n",
      "Epoch: 392. Loss: 0.03473268076777458\n",
      "Epoch: 393. Loss: 0.03431011736392975\n",
      "Epoch: 394. Loss: 0.03388997167348862\n",
      "Epoch: 395. Loss: 0.0334879569709301\n",
      "Epoch: 396. Loss: 0.033103398978710175\n",
      "Epoch: 397. Loss: 0.032717473804950714\n",
      "Epoch: 398. Loss: 0.03233395889401436\n",
      "Epoch: 399. Loss: 0.031945109367370605\n",
      "Epoch: 400. Loss: 0.031571947038173676\n",
      "Epoch: 401. Loss: 0.031211279332637787\n",
      "Epoch: 402. Loss: 0.030853860080242157\n",
      "Epoch: 403. Loss: 0.03049370087683201\n",
      "Epoch: 404. Loss: 0.030139800161123276\n",
      "Epoch: 405. Loss: 0.029793953523039818\n",
      "Epoch: 406. Loss: 0.0294545479118824\n",
      "Epoch: 407. Loss: 0.029120666906237602\n",
      "Epoch: 408. Loss: 0.028792444616556168\n",
      "Epoch: 409. Loss: 0.028464043512940407\n",
      "Epoch: 410. Loss: 0.028141165152192116\n",
      "Epoch: 411. Loss: 0.027824467048048973\n",
      "Epoch: 412. Loss: 0.027517618611454964\n",
      "Epoch: 413. Loss: 0.02721456252038479\n",
      "Epoch: 414. Loss: 0.026915427297353745\n",
      "Epoch: 415. Loss: 0.02661556750535965\n",
      "Epoch: 416. Loss: 0.026320194825530052\n",
      "Epoch: 417. Loss: 0.026028865948319435\n",
      "Epoch: 418. Loss: 0.025744883343577385\n",
      "Epoch: 419. Loss: 0.025471005588769913\n",
      "Epoch: 420. Loss: 0.0251962561160326\n",
      "Epoch: 421. Loss: 0.024922380223870277\n",
      "Epoch: 422. Loss: 0.024653328582644463\n",
      "Epoch: 423. Loss: 0.024388214573264122\n",
      "Epoch: 424. Loss: 0.02412513829767704\n",
      "Epoch: 425. Loss: 0.023867178708314896\n",
      "Epoch: 426. Loss: 0.0236157588660717\n",
      "Epoch: 427. Loss: 0.023366345092654228\n",
      "Epoch: 428. Loss: 0.023122891783714294\n",
      "Epoch: 429. Loss: 0.022882129997015\n",
      "Epoch: 430. Loss: 0.022644001990556717\n",
      "Epoch: 431. Loss: 0.02240797132253647\n",
      "Epoch: 432. Loss: 0.022176798433065414\n",
      "Epoch: 433. Loss: 0.021947288885712624\n",
      "Epoch: 434. Loss: 0.02172069251537323\n",
      "Epoch: 435. Loss: 0.021497689187526703\n",
      "Epoch: 436. Loss: 0.021278349682688713\n",
      "Epoch: 437. Loss: 0.02106238529086113\n",
      "Epoch: 438. Loss: 0.020848896354436874\n",
      "Epoch: 439. Loss: 0.02063860557973385\n",
      "Epoch: 440. Loss: 0.020433496683835983\n",
      "Epoch: 441. Loss: 0.020226772874593735\n",
      "Epoch: 442. Loss: 0.020023228600621223\n",
      "Epoch: 443. Loss: 0.019825464114546776\n",
      "Epoch: 444. Loss: 0.019627880305051804\n",
      "Epoch: 445. Loss: 0.01943528652191162\n",
      "Epoch: 446. Loss: 0.019242659211158752\n",
      "Epoch: 447. Loss: 0.019053325057029724\n",
      "Epoch: 448. Loss: 0.018869750201702118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449. Loss: 0.018683956936001778\n",
      "Epoch: 450. Loss: 0.018504653126001358\n",
      "Epoch: 451. Loss: 0.018326686695218086\n",
      "Epoch: 452. Loss: 0.01814868301153183\n",
      "Epoch: 453. Loss: 0.01797441951930523\n",
      "Epoch: 454. Loss: 0.01780027523636818\n",
      "Epoch: 455. Loss: 0.017635511234402657\n",
      "Epoch: 456. Loss: 0.01746455207467079\n",
      "Epoch: 457. Loss: 0.017303453758358955\n",
      "Epoch: 458. Loss: 0.017136231064796448\n",
      "Epoch: 459. Loss: 0.016978837549686432\n",
      "Epoch: 460. Loss: 0.0168235432356596\n",
      "Epoch: 461. Loss: 0.016668274998664856\n",
      "Epoch: 462. Loss: 0.01651843450963497\n",
      "Epoch: 463. Loss: 0.016363240778446198\n",
      "Epoch: 464. Loss: 0.01621665433049202\n",
      "Epoch: 465. Loss: 0.01606292836368084\n",
      "Epoch: 466. Loss: 0.015917692333459854\n",
      "Epoch: 467. Loss: 0.015767589211463928\n",
      "Epoch: 468. Loss: 0.015628883615136147\n",
      "Epoch: 469. Loss: 0.015484248287975788\n",
      "Epoch: 470. Loss: 0.015348918735980988\n",
      "Epoch: 471. Loss: 0.015212695114314556\n",
      "Epoch: 472. Loss: 0.015080335550010204\n",
      "Epoch: 473. Loss: 0.014950591139495373\n",
      "Epoch: 474. Loss: 0.01481868140399456\n",
      "Epoch: 475. Loss: 0.014688237570226192\n",
      "Epoch: 476. Loss: 0.014557339251041412\n",
      "Epoch: 477. Loss: 0.014428840950131416\n",
      "Epoch: 478. Loss: 0.014300334267318249\n",
      "Epoch: 479. Loss: 0.014176400378346443\n",
      "Epoch: 480. Loss: 0.014052134938538074\n",
      "Epoch: 481. Loss: 0.013927310705184937\n",
      "Epoch: 482. Loss: 0.013808414340019226\n",
      "Epoch: 483. Loss: 0.013689156621694565\n",
      "Epoch: 484. Loss: 0.013575657270848751\n",
      "Epoch: 485. Loss: 0.013457844033837318\n",
      "Epoch: 486. Loss: 0.013346565887331963\n",
      "Epoch: 487. Loss: 0.013234047219157219\n",
      "Epoch: 488. Loss: 0.013121107593178749\n",
      "Epoch: 489. Loss: 0.0130141731351614\n",
      "Epoch: 490. Loss: 0.012904247269034386\n",
      "Epoch: 491. Loss: 0.012796216644346714\n",
      "Epoch: 492. Loss: 0.012692147865891457\n",
      "Epoch: 493. Loss: 0.012587184086441994\n",
      "Epoch: 494. Loss: 0.012486505322158337\n",
      "Epoch: 495. Loss: 0.012383724562823772\n",
      "Epoch: 496. Loss: 0.012283328920602798\n",
      "Epoch: 497. Loss: 0.012185889296233654\n",
      "Epoch: 498. Loss: 0.012086829170584679\n",
      "Epoch: 499. Loss: 0.011992054060101509\n",
      "Epoch: 500. Loss: 0.01189564447849989\n",
      "Epoch: 501. Loss: 0.011799466796219349\n",
      "Epoch: 502. Loss: 0.01170646958053112\n",
      "Epoch: 503. Loss: 0.011611275374889374\n",
      "Epoch: 504. Loss: 0.011519690975546837\n",
      "Epoch: 505. Loss: 0.011430138722062111\n",
      "Epoch: 506. Loss: 0.011341053061187267\n",
      "Epoch: 507. Loss: 0.011254305019974709\n",
      "Epoch: 508. Loss: 0.01116893719881773\n",
      "Epoch: 509. Loss: 0.011082896031439304\n",
      "Epoch: 510. Loss: 0.010998310521245003\n",
      "Epoch: 511. Loss: 0.010914045386016369\n",
      "Epoch: 512. Loss: 0.010829491540789604\n",
      "Epoch: 513. Loss: 0.010748162865638733\n",
      "Epoch: 514. Loss: 0.010664721950888634\n",
      "Epoch: 515. Loss: 0.01058630645275116\n",
      "Epoch: 516. Loss: 0.010504779405891895\n",
      "Epoch: 517. Loss: 0.01042612548917532\n",
      "Epoch: 518. Loss: 0.010349312797188759\n",
      "Epoch: 519. Loss: 0.010271930135786533\n",
      "Epoch: 520. Loss: 0.01019598450511694\n",
      "Epoch: 521. Loss: 0.010120480321347713\n",
      "Epoch: 522. Loss: 0.010046590119600296\n",
      "Epoch: 523. Loss: 0.009974203072488308\n",
      "Epoch: 524. Loss: 0.009899258613586426\n",
      "Epoch: 525. Loss: 0.009828081354498863\n",
      "Epoch: 526. Loss: 0.009756796061992645\n",
      "Epoch: 527. Loss: 0.009686659090220928\n",
      "Epoch: 528. Loss: 0.00961725227534771\n",
      "Epoch: 529. Loss: 0.009548596106469631\n",
      "Epoch: 530. Loss: 0.00947984866797924\n",
      "Epoch: 531. Loss: 0.00941300205886364\n",
      "Epoch: 532. Loss: 0.009346342645585537\n",
      "Epoch: 533. Loss: 0.009279985912144184\n",
      "Epoch: 534. Loss: 0.009215659461915493\n",
      "Epoch: 535. Loss: 0.009151259437203407\n",
      "Epoch: 536. Loss: 0.009087403304874897\n",
      "Epoch: 537. Loss: 0.009024310857057571\n",
      "Epoch: 538. Loss: 0.008961383253335953\n",
      "Epoch: 539. Loss: 0.008900144137442112\n",
      "Epoch: 540. Loss: 0.00883832760155201\n",
      "Epoch: 541. Loss: 0.008777230978012085\n",
      "Epoch: 542. Loss: 0.008718487806618214\n",
      "Epoch: 543. Loss: 0.008658530190587044\n",
      "Epoch: 544. Loss: 0.008599935099482536\n",
      "Epoch: 545. Loss: 0.008541694842278957\n",
      "Epoch: 546. Loss: 0.00848335400223732\n",
      "Epoch: 547. Loss: 0.008426559157669544\n",
      "Epoch: 548. Loss: 0.008371548727154732\n",
      "Epoch: 549. Loss: 0.008315623737871647\n",
      "Epoch: 550. Loss: 0.008260022848844528\n",
      "Epoch: 551. Loss: 0.008205564692616463\n",
      "Epoch: 552. Loss: 0.00815342552959919\n",
      "Epoch: 553. Loss: 0.008098501712083817\n",
      "Epoch: 554. Loss: 0.008045637048780918\n",
      "Epoch: 555. Loss: 0.007993903011083603\n",
      "Epoch: 556. Loss: 0.007941479794681072\n",
      "Epoch: 557. Loss: 0.007891072891652584\n",
      "Epoch: 558. Loss: 0.007838982157409191\n",
      "Epoch: 559. Loss: 0.007788647897541523\n",
      "Epoch: 560. Loss: 0.00774009944871068\n",
      "Epoch: 561. Loss: 0.0076908511109650135\n",
      "Epoch: 562. Loss: 0.0076402295380830765\n",
      "Epoch: 563. Loss: 0.00759363267570734\n",
      "Epoch: 564. Loss: 0.007545304484665394\n",
      "Epoch: 565. Loss: 0.007496947888284922\n",
      "Epoch: 566. Loss: 0.007450839038938284\n",
      "Epoch: 567. Loss: 0.0074045052751898766\n",
      "Epoch: 568. Loss: 0.0073586106300354\n",
      "Epoch: 569. Loss: 0.007313822396099567\n",
      "Epoch: 570. Loss: 0.007268713787198067\n",
      "Epoch: 571. Loss: 0.007223231717944145\n",
      "Epoch: 572. Loss: 0.007179539185017347\n",
      "Epoch: 573. Loss: 0.007135112304240465\n",
      "Epoch: 574. Loss: 0.0070906104519963264\n",
      "Epoch: 575. Loss: 0.00704843457788229\n",
      "Epoch: 576. Loss: 0.007005017250776291\n",
      "Epoch: 577. Loss: 0.006962867453694344\n",
      "Epoch: 578. Loss: 0.006919963285326958\n",
      "Epoch: 579. Loss: 0.006879439577460289\n",
      "Epoch: 580. Loss: 0.006838658824563026\n",
      "Epoch: 581. Loss: 0.006797602400183678\n",
      "Epoch: 582. Loss: 0.006757770664989948\n",
      "Epoch: 583. Loss: 0.00671630771830678\n",
      "Epoch: 584. Loss: 0.006676286458969116\n",
      "Epoch: 585. Loss: 0.006638389080762863\n",
      "Epoch: 586. Loss: 0.006597457453608513\n",
      "Epoch: 587. Loss: 0.006560161244124174\n",
      "Epoch: 588. Loss: 0.006521820556372404\n",
      "Epoch: 589. Loss: 0.006483296398073435\n",
      "Epoch: 590. Loss: 0.006445768754929304\n",
      "Epoch: 591. Loss: 0.006408168468624353\n",
      "Epoch: 592. Loss: 0.0063716997392475605\n",
      "Epoch: 593. Loss: 0.006335004698485136\n",
      "Epoch: 594. Loss: 0.0062988437712192535\n",
      "Epoch: 595. Loss: 0.006261963397264481\n",
      "Epoch: 596. Loss: 0.006227785721421242\n",
      "Epoch: 597. Loss: 0.0061921426095068455\n",
      "Epoch: 598. Loss: 0.0061561777256429195\n",
      "Epoch: 599. Loss: 0.006122775375843048\n",
      "Epoch: 600. Loss: 0.006087342742830515\n",
      "Epoch: 601. Loss: 0.00605400837957859\n",
      "Epoch: 602. Loss: 0.006018638610839844\n",
      "Epoch: 603. Loss: 0.0059864879585802555\n",
      "Epoch: 604. Loss: 0.005952552426606417\n",
      "Epoch: 605. Loss: 0.005919762421399355\n",
      "Epoch: 606. Loss: 0.005887839011847973\n",
      "Epoch: 607. Loss: 0.005854451097548008\n",
      "Epoch: 608. Loss: 0.005823204759508371\n",
      "Epoch: 609. Loss: 0.005791207309812307\n",
      "Epoch: 610. Loss: 0.0057588908821344376\n",
      "Epoch: 611. Loss: 0.005728444084525108\n",
      "Epoch: 612. Loss: 0.00569677772000432\n",
      "Epoch: 613. Loss: 0.005666406825184822\n",
      "Epoch: 614. Loss: 0.005636054091155529\n",
      "Epoch: 615. Loss: 0.005606075748801231\n",
      "Epoch: 616. Loss: 0.005575274582952261\n",
      "Epoch: 617. Loss: 0.0055463253520429134\n",
      "Epoch: 618. Loss: 0.005516563542187214\n",
      "Epoch: 619. Loss: 0.005487462040036917\n",
      "Epoch: 620. Loss: 0.005458533298224211\n",
      "Epoch: 621. Loss: 0.005430145189166069\n",
      "Epoch: 622. Loss: 0.005401812959462404\n",
      "Epoch: 623. Loss: 0.005373300053179264\n",
      "Epoch: 624. Loss: 0.005345846060663462\n",
      "Epoch: 625. Loss: 0.005318303592503071\n",
      "Epoch: 626. Loss: 0.005290015134960413\n",
      "Epoch: 627. Loss: 0.005262185353785753\n",
      "Epoch: 628. Loss: 0.0052354768849909306\n",
      "Epoch: 629. Loss: 0.005208845715969801\n",
      "Epoch: 630. Loss: 0.005182445049285889\n",
      "Epoch: 631. Loss: 0.005156198050826788\n",
      "Epoch: 632. Loss: 0.005129648372530937\n",
      "Epoch: 633. Loss: 0.005103269126266241\n",
      "Epoch: 634. Loss: 0.005077868700027466\n",
      "Epoch: 635. Loss: 0.005052131600677967\n",
      "Epoch: 636. Loss: 0.005026964470744133\n",
      "Epoch: 637. Loss: 0.005001164507120848\n",
      "Epoch: 638. Loss: 0.004976608790457249\n",
      "Epoch: 639. Loss: 0.004952010232955217\n",
      "Epoch: 640. Loss: 0.0049275001510977745\n",
      "Epoch: 641. Loss: 0.00490312185138464\n",
      "Epoch: 642. Loss: 0.004878319334238768\n",
      "Epoch: 643. Loss: 0.004853906575590372\n",
      "Epoch: 644. Loss: 0.004830505698919296\n",
      "Epoch: 645. Loss: 0.0048067825846374035\n",
      "Epoch: 646. Loss: 0.004783580545336008\n",
      "Epoch: 647. Loss: 0.004760636482387781\n",
      "Epoch: 648. Loss: 0.004737058188766241\n",
      "Epoch: 649. Loss: 0.004713784903287888\n",
      "Epoch: 650. Loss: 0.004691770300269127\n",
      "Epoch: 651. Loss: 0.00466923788189888\n",
      "Epoch: 652. Loss: 0.0046470072120428085\n",
      "Epoch: 653. Loss: 0.004624629858881235\n",
      "Epoch: 654. Loss: 0.004602290689945221\n",
      "Epoch: 655. Loss: 0.004580722656100988\n",
      "Epoch: 656. Loss: 0.00455888407304883\n",
      "Epoch: 657. Loss: 0.004537304863333702\n",
      "Epoch: 658. Loss: 0.004516538232564926\n",
      "Epoch: 659. Loss: 0.004494705703109503\n",
      "Epoch: 660. Loss: 0.004473974462598562\n",
      "Epoch: 661. Loss: 0.004452474880963564\n",
      "Epoch: 662. Loss: 0.004431973677128553\n",
      "Epoch: 663. Loss: 0.00441124988719821\n",
      "Epoch: 664. Loss: 0.00439092330634594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 665. Loss: 0.00437052920460701\n",
      "Epoch: 666. Loss: 0.004350023809820414\n",
      "Epoch: 667. Loss: 0.0043302434496581554\n",
      "Epoch: 668. Loss: 0.004310185555368662\n",
      "Epoch: 669. Loss: 0.004291048739105463\n",
      "Epoch: 670. Loss: 0.004270536825060844\n",
      "Epoch: 671. Loss: 0.0042519294656813145\n",
      "Epoch: 672. Loss: 0.004232298117130995\n",
      "Epoch: 673. Loss: 0.004213123116642237\n",
      "Epoch: 674. Loss: 0.004194237757474184\n",
      "Epoch: 675. Loss: 0.004175071604549885\n",
      "Epoch: 676. Loss: 0.004156874492764473\n",
      "Epoch: 677. Loss: 0.004138678312301636\n",
      "Epoch: 678. Loss: 0.004120091442018747\n",
      "Epoch: 679. Loss: 0.004101371858268976\n",
      "Epoch: 680. Loss: 0.004083589185029268\n",
      "Epoch: 681. Loss: 0.004065455868840218\n",
      "Epoch: 682. Loss: 0.004047291819006205\n",
      "Epoch: 683. Loss: 0.004029354080557823\n",
      "Epoch: 684. Loss: 0.004011897370219231\n",
      "Epoch: 685. Loss: 0.003994270693510771\n",
      "Epoch: 686. Loss: 0.003976707346737385\n",
      "Epoch: 687. Loss: 0.00395942572504282\n",
      "Epoch: 688. Loss: 0.003942322917282581\n",
      "Epoch: 689. Loss: 0.003925344906747341\n",
      "Epoch: 690. Loss: 0.003908164333552122\n",
      "Epoch: 691. Loss: 0.003891297383233905\n",
      "Epoch: 692. Loss: 0.003874781308695674\n",
      "Epoch: 693. Loss: 0.0038580792024731636\n",
      "Epoch: 694. Loss: 0.0038416204042732716\n",
      "Epoch: 695. Loss: 0.003825383260846138\n",
      "Epoch: 696. Loss: 0.003809161251410842\n",
      "Epoch: 697. Loss: 0.003792740637436509\n",
      "Epoch: 698. Loss: 0.003777147037908435\n",
      "Epoch: 699. Loss: 0.0037604314275085926\n",
      "Epoch: 700. Loss: 0.003744891146197915\n",
      "Epoch: 701. Loss: 0.0037297257222235203\n",
      "Epoch: 702. Loss: 0.00371330208145082\n",
      "Epoch: 703. Loss: 0.0036979152355343103\n",
      "Epoch: 704. Loss: 0.0036827067378908396\n",
      "Epoch: 705. Loss: 0.0036669543478637934\n",
      "Epoch: 706. Loss: 0.0036524364259094\n",
      "Epoch: 707. Loss: 0.003637082641944289\n",
      "Epoch: 708. Loss: 0.003622338641434908\n",
      "Epoch: 709. Loss: 0.0036073410883545876\n",
      "Epoch: 710. Loss: 0.0035930657759308815\n",
      "Epoch: 711. Loss: 0.0035782153718173504\n",
      "Epoch: 712. Loss: 0.003563573816791177\n",
      "Epoch: 713. Loss: 0.0035489669535309076\n",
      "Epoch: 714. Loss: 0.0035349137615412474\n",
      "Epoch: 715. Loss: 0.003519882680848241\n",
      "Epoch: 716. Loss: 0.0035059181973338127\n",
      "Epoch: 717. Loss: 0.003491707844659686\n",
      "Epoch: 718. Loss: 0.003477900754660368\n",
      "Epoch: 719. Loss: 0.003463889006525278\n",
      "Epoch: 720. Loss: 0.0034498213790357113\n",
      "Epoch: 721. Loss: 0.0034364883322268724\n",
      "Epoch: 722. Loss: 0.003422602778300643\n",
      "Epoch: 723. Loss: 0.003409209894016385\n",
      "Epoch: 724. Loss: 0.003395555540919304\n",
      "Epoch: 725. Loss: 0.0033829212188720703\n",
      "Epoch: 726. Loss: 0.0033688670955598354\n",
      "Epoch: 727. Loss: 0.00335559225641191\n",
      "Epoch: 728. Loss: 0.0033428804017603397\n",
      "Epoch: 729. Loss: 0.0033297163899987936\n",
      "Epoch: 730. Loss: 0.003316845279186964\n",
      "Epoch: 731. Loss: 0.0033040561247617006\n",
      "Epoch: 732. Loss: 0.003291103523224592\n",
      "Epoch: 733. Loss: 0.003278657328337431\n",
      "Epoch: 734. Loss: 0.003265590174123645\n",
      "Epoch: 735. Loss: 0.0032533048652112484\n",
      "Epoch: 736. Loss: 0.003240709425881505\n",
      "Epoch: 737. Loss: 0.0032285430934280157\n",
      "Epoch: 738. Loss: 0.0032161185517907143\n",
      "Epoch: 739. Loss: 0.003203914500772953\n",
      "Epoch: 740. Loss: 0.0031912261620163918\n",
      "Epoch: 741. Loss: 0.003179720137268305\n",
      "Epoch: 742. Loss: 0.003167639020830393\n",
      "Epoch: 743. Loss: 0.003155409824103117\n",
      "Epoch: 744. Loss: 0.0031439759768545628\n",
      "Epoch: 745. Loss: 0.003132069716230035\n",
      "Epoch: 746. Loss: 0.0031201099045574665\n",
      "Epoch: 747. Loss: 0.0031087538227438927\n",
      "Epoch: 748. Loss: 0.0030972121749073267\n",
      "Epoch: 749. Loss: 0.0030855408404022455\n",
      "Epoch: 750. Loss: 0.003074277425184846\n",
      "Epoch: 751. Loss: 0.003062996082007885\n",
      "Epoch: 752. Loss: 0.0030516379047185183\n",
      "Epoch: 753. Loss: 0.0030407446902245283\n",
      "Epoch: 754. Loss: 0.003029826795682311\n",
      "Epoch: 755. Loss: 0.003018599469214678\n",
      "Epoch: 756. Loss: 0.003007943043485284\n",
      "Epoch: 757. Loss: 0.002996742958202958\n",
      "Epoch: 758. Loss: 0.002986001782119274\n",
      "Epoch: 759. Loss: 0.0029749604873359203\n",
      "Epoch: 760. Loss: 0.002964647486805916\n",
      "Epoch: 761. Loss: 0.002953706542029977\n",
      "Epoch: 762. Loss: 0.0029432775918394327\n",
      "Epoch: 763. Loss: 0.0029322991613298655\n",
      "Epoch: 764. Loss: 0.0029220778960734606\n",
      "Epoch: 765. Loss: 0.002911785850301385\n",
      "Epoch: 766. Loss: 0.0029014393221586943\n",
      "Epoch: 767. Loss: 0.0028913586866110563\n",
      "Epoch: 768. Loss: 0.002880915068089962\n",
      "Epoch: 769. Loss: 0.002870911033824086\n",
      "Epoch: 770. Loss: 0.002861308166757226\n",
      "Epoch: 771. Loss: 0.0028510575648397207\n",
      "Epoch: 772. Loss: 0.002841226290911436\n",
      "Epoch: 773. Loss: 0.0028311654459685087\n",
      "Epoch: 774. Loss: 0.0028213185723870993\n",
      "Epoch: 775. Loss: 0.002811539452522993\n",
      "Epoch: 776. Loss: 0.0028018197044730186\n",
      "Epoch: 777. Loss: 0.002792579587548971\n",
      "Epoch: 778. Loss: 0.002782604191452265\n",
      "Epoch: 779. Loss: 0.002773039508610964\n",
      "Epoch: 780. Loss: 0.00276371487416327\n",
      "Epoch: 781. Loss: 0.002754251239821315\n",
      "Epoch: 782. Loss: 0.002744932658970356\n",
      "Epoch: 783. Loss: 0.0027354839257895947\n",
      "Epoch: 784. Loss: 0.002726215636357665\n",
      "Epoch: 785. Loss: 0.002717378316447139\n",
      "Epoch: 786. Loss: 0.002708208980038762\n",
      "Epoch: 787. Loss: 0.002698680153116584\n",
      "Epoch: 788. Loss: 0.002690074034035206\n",
      "Epoch: 789. Loss: 0.002680922392755747\n",
      "Epoch: 790. Loss: 0.0026717891450971365\n",
      "Epoch: 791. Loss: 0.0026628714986145496\n",
      "Epoch: 792. Loss: 0.0026543678250163794\n",
      "Epoch: 793. Loss: 0.0026450862642377615\n",
      "Epoch: 794. Loss: 0.0026364207733422518\n",
      "Epoch: 795. Loss: 0.0026279399171471596\n",
      "Epoch: 796. Loss: 0.0026189505588263273\n",
      "Epoch: 797. Loss: 0.0026105025317519903\n",
      "Epoch: 798. Loss: 0.0026019124779850245\n",
      "Epoch: 799. Loss: 0.002593580400571227\n",
      "Epoch: 800. Loss: 0.002584894420579076\n",
      "Epoch: 801. Loss: 0.0025765481404960155\n",
      "Epoch: 802. Loss: 0.002568056806921959\n",
      "Epoch: 803. Loss: 0.002560027875006199\n",
      "Epoch: 804. Loss: 0.0025518571492284536\n",
      "Epoch: 805. Loss: 0.0025432563852518797\n",
      "Epoch: 806. Loss: 0.002535508945584297\n",
      "Epoch: 807. Loss: 0.002527221804484725\n",
      "Epoch: 808. Loss: 0.002518921624869108\n",
      "Epoch: 809. Loss: 0.0025110242422670126\n",
      "Epoch: 810. Loss: 0.0025028521195054054\n",
      "Epoch: 811. Loss: 0.002495105378329754\n",
      "Epoch: 812. Loss: 0.002487163059413433\n",
      "Epoch: 813. Loss: 0.0024792617186903954\n",
      "Epoch: 814. Loss: 0.0024716563057154417\n",
      "Epoch: 815. Loss: 0.002463878132402897\n",
      "Epoch: 816. Loss: 0.0024562578182667494\n",
      "Epoch: 817. Loss: 0.002448470564559102\n",
      "Epoch: 818. Loss: 0.0024406888987869024\n",
      "Epoch: 819. Loss: 0.0024332385510206223\n",
      "Epoch: 820. Loss: 0.002425587037578225\n",
      "Epoch: 821. Loss: 0.002418026328086853\n",
      "Epoch: 822. Loss: 0.002410763641819358\n",
      "Epoch: 823. Loss: 0.002403226215392351\n",
      "Epoch: 824. Loss: 0.002395852003246546\n",
      "Epoch: 825. Loss: 0.0023882845416665077\n",
      "Epoch: 826. Loss: 0.002381431171670556\n",
      "Epoch: 827. Loss: 0.0023736068978905678\n",
      "Epoch: 828. Loss: 0.0023665681947022676\n",
      "Epoch: 829. Loss: 0.002359457314014435\n",
      "Epoch: 830. Loss: 0.0023523883428424597\n",
      "Epoch: 831. Loss: 0.002345087705180049\n",
      "Epoch: 832. Loss: 0.002337966114282608\n",
      "Epoch: 833. Loss: 0.00233102566562593\n",
      "Epoch: 834. Loss: 0.0023240859154611826\n",
      "Epoch: 835. Loss: 0.0023171058855950832\n",
      "Epoch: 836. Loss: 0.0023099910467863083\n",
      "Epoch: 837. Loss: 0.0023031251039355993\n",
      "Epoch: 838. Loss: 0.0022963560186326504\n",
      "Epoch: 839. Loss: 0.002289404394105077\n",
      "Epoch: 840. Loss: 0.0022825044579803944\n",
      "Epoch: 841. Loss: 0.002275803592056036\n",
      "Epoch: 842. Loss: 0.002269158838316798\n",
      "Epoch: 843. Loss: 0.0022624919656664133\n",
      "Epoch: 844. Loss: 0.002255662577226758\n",
      "Epoch: 845. Loss: 0.0022490175906568766\n",
      "Epoch: 846. Loss: 0.0022429218515753746\n",
      "Epoch: 847. Loss: 0.002235898980870843\n",
      "Epoch: 848. Loss: 0.0022295780945569277\n",
      "Epoch: 849. Loss: 0.002223326126113534\n",
      "Epoch: 850. Loss: 0.002216674154624343\n",
      "Epoch: 851. Loss: 0.0022104009985923767\n",
      "Epoch: 852. Loss: 0.0022038398310542107\n",
      "Epoch: 853. Loss: 0.0021975061390548944\n",
      "Epoch: 854. Loss: 0.0021910422947257757\n",
      "Epoch: 855. Loss: 0.002184997545555234\n",
      "Epoch: 856. Loss: 0.0021788233425468206\n",
      "Epoch: 857. Loss: 0.0021725541446357965\n",
      "Epoch: 858. Loss: 0.002166308695450425\n",
      "Epoch: 859. Loss: 0.0021601193584501743\n",
      "Epoch: 860. Loss: 0.0021539400331676006\n",
      "Epoch: 861. Loss: 0.0021478256676346064\n",
      "Epoch: 862. Loss: 0.0021418388932943344\n",
      "Epoch: 863. Loss: 0.002136098686605692\n",
      "Epoch: 864. Loss: 0.002129969419911504\n",
      "Epoch: 865. Loss: 0.002123881597071886\n",
      "Epoch: 866. Loss: 0.0021182377822697163\n",
      "Epoch: 867. Loss: 0.0021122898906469345\n",
      "Epoch: 868. Loss: 0.0021064479369670153\n",
      "Epoch: 869. Loss: 0.0021005328744649887\n",
      "Epoch: 870. Loss: 0.002094551455229521\n",
      "Epoch: 871. Loss: 0.002088908338919282\n",
      "Epoch: 872. Loss: 0.002082938328385353\n",
      "Epoch: 873. Loss: 0.002077145269140601\n",
      "Epoch: 874. Loss: 0.002071696799248457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 875. Loss: 0.0020662909373641014\n",
      "Epoch: 876. Loss: 0.0020604277960956097\n",
      "Epoch: 877. Loss: 0.0020547404419630766\n",
      "Epoch: 878. Loss: 0.002049083588644862\n",
      "Epoch: 879. Loss: 0.002043656073510647\n",
      "Epoch: 880. Loss: 0.0020381580106914043\n",
      "Epoch: 881. Loss: 0.0020327989477664232\n",
      "Epoch: 882. Loss: 0.0020271139219403267\n",
      "Epoch: 883. Loss: 0.002021652413532138\n",
      "Epoch: 884. Loss: 0.002016355050727725\n",
      "Epoch: 885. Loss: 0.002010958967730403\n",
      "Epoch: 886. Loss: 0.0020053263287991285\n",
      "Epoch: 887. Loss: 0.002000298583880067\n",
      "Epoch: 888. Loss: 0.0019949369598180056\n",
      "Epoch: 889. Loss: 0.0019896903540939093\n",
      "Epoch: 890. Loss: 0.0019842800684273243\n",
      "Epoch: 891. Loss: 0.0019791123922914267\n",
      "Epoch: 892. Loss: 0.001973886275663972\n",
      "Epoch: 893. Loss: 0.001968751195818186\n",
      "Epoch: 894. Loss: 0.0019637492951005697\n",
      "Epoch: 895. Loss: 0.001958570210263133\n",
      "Epoch: 896. Loss: 0.0019533904269337654\n",
      "Epoch: 897. Loss: 0.0019484043586999178\n",
      "Epoch: 898. Loss: 0.00194328383076936\n",
      "Epoch: 899. Loss: 0.001938361907377839\n",
      "Epoch: 900. Loss: 0.0019333697855472565\n",
      "Epoch: 901. Loss: 0.0019281437853351235\n",
      "Epoch: 902. Loss: 0.0019235623767599463\n",
      "Epoch: 903. Loss: 0.0019183852709829807\n",
      "Epoch: 904. Loss: 0.0019135498441755772\n",
      "Epoch: 905. Loss: 0.0019086097599938512\n",
      "Epoch: 906. Loss: 0.0019038463942706585\n",
      "Epoch: 907. Loss: 0.0018990790704265237\n",
      "Epoch: 908. Loss: 0.0018940299050882459\n",
      "Epoch: 909. Loss: 0.0018894807435572147\n",
      "Epoch: 910. Loss: 0.0018845185404643416\n",
      "Epoch: 911. Loss: 0.001879894989542663\n",
      "Epoch: 912. Loss: 0.0018749397713690996\n",
      "Epoch: 913. Loss: 0.0018704853719100356\n",
      "Epoch: 914. Loss: 0.0018657393520697951\n",
      "Epoch: 915. Loss: 0.0018610080005601048\n",
      "Epoch: 916. Loss: 0.0018562296172603965\n",
      "Epoch: 917. Loss: 0.0018516351701691747\n",
      "Epoch: 918. Loss: 0.0018474436365067959\n",
      "Epoch: 919. Loss: 0.001842800178565085\n",
      "Epoch: 920. Loss: 0.0018380810506641865\n",
      "Epoch: 921. Loss: 0.0018334893975406885\n",
      "Epoch: 922. Loss: 0.0018291501328349113\n",
      "Epoch: 923. Loss: 0.0018247191328555346\n",
      "Epoch: 924. Loss: 0.0018201596103608608\n",
      "Epoch: 925. Loss: 0.0018158163875341415\n",
      "Epoch: 926. Loss: 0.0018112959805876017\n",
      "Epoch: 927. Loss: 0.001806791638955474\n",
      "Epoch: 928. Loss: 0.001802382175810635\n",
      "Epoch: 929. Loss: 0.0017981035634875298\n",
      "Epoch: 930. Loss: 0.0017939041135832667\n",
      "Epoch: 931. Loss: 0.001789590809494257\n",
      "Epoch: 932. Loss: 0.0017851038137450814\n",
      "Epoch: 933. Loss: 0.0017808532575145364\n",
      "Epoch: 934. Loss: 0.0017765139928087592\n",
      "Epoch: 935. Loss: 0.0017724863719195127\n",
      "Epoch: 936. Loss: 0.0017683760961517692\n",
      "Epoch: 937. Loss: 0.0017640250734984875\n",
      "Epoch: 938. Loss: 0.0017597402911633253\n",
      "Epoch: 939. Loss: 0.001755627105012536\n",
      "Epoch: 940. Loss: 0.0017515989020466805\n",
      "Epoch: 941. Loss: 0.0017474585911259055\n",
      "Epoch: 942. Loss: 0.0017431825399398804\n",
      "Epoch: 943. Loss: 0.0017392770387232304\n",
      "Epoch: 944. Loss: 0.001735267462208867\n",
      "Epoch: 945. Loss: 0.00173117162194103\n",
      "Epoch: 946. Loss: 0.0017269387608394027\n",
      "Epoch: 947. Loss: 0.001723032328300178\n",
      "Epoch: 948. Loss: 0.001719102612696588\n",
      "Epoch: 949. Loss: 0.0017153595108538866\n",
      "Epoch: 950. Loss: 0.001711342018097639\n",
      "Epoch: 951. Loss: 0.0017073337221518159\n",
      "Epoch: 952. Loss: 0.001703341375105083\n",
      "Epoch: 953. Loss: 0.001699345652014017\n",
      "Epoch: 954. Loss: 0.0016955358441919088\n",
      "Epoch: 955. Loss: 0.0016917927423492074\n",
      "Epoch: 956. Loss: 0.0016878608148545027\n",
      "Epoch: 957. Loss: 0.0016839345917105675\n",
      "Epoch: 958. Loss: 0.001680089975707233\n",
      "Epoch: 959. Loss: 0.001676271203905344\n",
      "Epoch: 960. Loss: 0.0016726654721423984\n",
      "Epoch: 961. Loss: 0.001668737968429923\n",
      "Epoch: 962. Loss: 0.001665045041590929\n",
      "Epoch: 963. Loss: 0.001661446993239224\n",
      "Epoch: 964. Loss: 0.0016575714107602835\n",
      "Epoch: 965. Loss: 0.001653931918554008\n",
      "Epoch: 966. Loss: 0.0016503608785569668\n",
      "Epoch: 967. Loss: 0.001646566204726696\n",
      "Epoch: 968. Loss: 0.0016430424293503165\n",
      "Epoch: 969. Loss: 0.0016392372781410813\n",
      "Epoch: 970. Loss: 0.0016357825370505452\n",
      "Epoch: 971. Loss: 0.0016320967115461826\n",
      "Epoch: 972. Loss: 0.0016284884186461568\n",
      "Epoch: 973. Loss: 0.0016248720930889249\n",
      "Epoch: 974. Loss: 0.0016213832423090935\n",
      "Epoch: 975. Loss: 0.0016182055696845055\n",
      "Epoch: 976. Loss: 0.001614436856471002\n",
      "Epoch: 977. Loss: 0.001610795734450221\n",
      "Epoch: 978. Loss: 0.0016074922168627381\n",
      "Epoch: 979. Loss: 0.0016041056951507926\n",
      "Epoch: 980. Loss: 0.0016006085788831115\n",
      "Epoch: 981. Loss: 0.0015971835236996412\n",
      "Epoch: 982. Loss: 0.0015935864066705108\n",
      "Epoch: 983. Loss: 0.00159016577526927\n",
      "Epoch: 984. Loss: 0.0015868922928348184\n",
      "Epoch: 985. Loss: 0.0015833884244784713\n",
      "Epoch: 986. Loss: 0.0015800355467945337\n",
      "Epoch: 987. Loss: 0.001576499897055328\n",
      "Epoch: 988. Loss: 0.0015731222229078412\n",
      "Epoch: 989. Loss: 0.0015701466472819448\n",
      "Epoch: 990. Loss: 0.0015667673433199525\n",
      "Epoch: 991. Loss: 0.001563300029374659\n",
      "Epoch: 992. Loss: 0.0015600296901538968\n",
      "Epoch: 993. Loss: 0.0015566542278975248\n",
      "Epoch: 994. Loss: 0.0015535721322521567\n",
      "Epoch: 995. Loss: 0.0015503030735999346\n",
      "Epoch: 996. Loss: 0.0015470759244635701\n",
      "Epoch: 997. Loss: 0.0015438401605933905\n",
      "Epoch: 998. Loss: 0.0015406818129122257\n",
      "Epoch: 999. Loss: 0.0015374650247395039\n"
     ]
    }
   ],
   "source": [
    "loss_cnn_train = []\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 120\n",
    "cnn_logits_lst = []\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    #for i in tqdm(range(0, len(X_train_CNN), BATCH_SIZE)):\n",
    "    batch_X = X_train_CNN.view(-1, 1, 33,33)\n",
    "    batch_y = Y_train_CNN\n",
    "\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net(batch_X.float())[0]\n",
    "    cnn_logits = net(batch_X.float())[1]\n",
    "    #cnn_logits_lst.extend(cnn_logits)\n",
    "    #print(outputs)\n",
    "    loss = loss_function(outputs,  batch_y.float().reshape((-1,1)))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_cnn_train.append(loss)\n",
    "    if EPOCHS % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d06e3923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4398"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_cnn_train)\n",
    "len(cnn_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f318f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64a0e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6f58325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('train_withlogits.csv')\n",
    "test = pd.read_csv('test_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09ff1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CNN_logits'] =cnn_logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef3e4d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "      <td>तल्ख़ी</td>\n",
       "      <td>تَوَجُّه</td>\n",
       "      <td>təlxiː</td>\n",
       "      <td>towæd͡ʒُّh</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.262500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492522</td>\n",
       "      <td>0.449687</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>-11.186650</td>\n",
       "      <td>-20.759451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7655</td>\n",
       "      <td>उदार</td>\n",
       "      <td>سخاوتمندانه</td>\n",
       "      <td>udaːr</td>\n",
       "      <td>sxɒvtmndɒnh</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>5.102273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340180</td>\n",
       "      <td>0.758899</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>-7.928271</td>\n",
       "      <td>-9.596358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12060</td>\n",
       "      <td>हम-</td>\n",
       "      <td>هرگز</td>\n",
       "      <td>ɦəmə-</td>\n",
       "      <td>hrɡz</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333505</td>\n",
       "      <td>0.659417</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>-20.914663</td>\n",
       "      <td>-19.077866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1962</td>\n",
       "      <td>कठोर</td>\n",
       "      <td>بي تفاوت</td>\n",
       "      <td>kəʈʰor</td>\n",
       "      <td>bي tfɒvt</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.247396</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369706</td>\n",
       "      <td>0.487233</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, ...</td>\n",
       "      <td>-9.150852</td>\n",
       "      <td>-8.425693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>2127</td>\n",
       "      <td>फ़िरंगी</td>\n",
       "      <td>فرصت</td>\n",
       "      <td>firəŋɡiː</td>\n",
       "      <td>frst</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.403646</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.498239</td>\n",
       "      <td>0.715128</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-11.355590</td>\n",
       "      <td>-9.454258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>501</td>\n",
       "      <td>8432</td>\n",
       "      <td>साज़</td>\n",
       "      <td>اتصال</td>\n",
       "      <td>saːz</td>\n",
       "      <td>ɒtsɒl</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.390779</td>\n",
       "      <td>0.225417</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-10.104707</td>\n",
       "      <td>-10.157057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>4370</td>\n",
       "      <td>टुकड़ा</td>\n",
       "      <td>باطله</td>\n",
       "      <td>ʈukɽaː</td>\n",
       "      <td>bɒtlh</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386734</td>\n",
       "      <td>0.621349</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-8.592606</td>\n",
       "      <td>-9.658706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1 loan_word original_word loan_word_epitran  \\\n",
       "0             0          1246      साया          سایه            saːjaː   \n",
       "1             1           524    तल्ख़ी      تَوَجُّه            təlxiː   \n",
       "2             2          7655      उदार   سخاوتمندانه             udaːr   \n",
       "3             3         12060       हम-          هرگز             ɦəmə-   \n",
       "4             4          1962      कठोर      بي تفاوت            kəʈʰor   \n",
       "..          ...           ...       ...           ...               ...   \n",
       "498         498          1047   रफ़्तार         رفتار           rəftaːr   \n",
       "499         499           412     ज़ब्त           ضبط             zəbtə   \n",
       "500         500          2127   फ़िरंगी          فرصت          firəŋɡiː   \n",
       "501         501          8432      साज़         اتصال              saːz   \n",
       "502         502          4370    टुकड़ा         باطله            ʈukɽaː   \n",
       "\n",
       "    original_word_epitran  Fast Levenshtein  Dolgo Prime Distance  \\\n",
       "0                    sɒjh          0.666667              0.166667   \n",
       "1              towæd͡ʒُّh          0.900000              0.200000   \n",
       "2             sxɒvtmndɒnh          0.909091              0.636364   \n",
       "3                    hrɡz          1.000000              0.600000   \n",
       "4                bي tfɒvt          1.000000              0.625000   \n",
       "..                    ...               ...                   ...   \n",
       "498                 rftɒr          0.428571              0.142857   \n",
       "499                   zbt          0.400000              0.400000   \n",
       "500                  frst          0.750000              0.625000   \n",
       "501                 ɒtsɒl          1.000000              0.600000   \n",
       "502                 bɒtlh          1.000000              0.500000   \n",
       "\n",
       "     Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                 0.062500                  0.069444   \n",
       "1                 0.177083                  0.200000   \n",
       "2                 0.615530                  0.681818   \n",
       "3                 0.166667                  0.208333   \n",
       "4                 0.247396                  0.281250   \n",
       "..                     ...                       ...   \n",
       "498               0.145833                  0.160714   \n",
       "499               0.358333                  0.400000   \n",
       "500               0.403646                  0.453125   \n",
       "501               0.404167                  0.441667   \n",
       "502               0.152778                  0.180556   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "1                     2.262500                              0.900000      0   \n",
       "2                     5.102273                              0.909091      0   \n",
       "3                     3.650000                              1.000000      0   \n",
       "4                     3.250000                              1.000000      0   \n",
       "..                         ...                                   ...    ...   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "500                   3.750000                              0.750000      0   \n",
       "501                   3.700000                              1.000000      0   \n",
       "502                   2.187500                              1.000000      0   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "1                0.492522            0.449687   \n",
       "2                0.340180            0.758899   \n",
       "3                0.333505            0.659417   \n",
       "4                0.369706            0.487233   \n",
       "..                    ...                 ...   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "500              0.498239            0.715128   \n",
       "501              0.390779            0.225417   \n",
       "502              0.386734            0.621349   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "1    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "2    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "3    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4    [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "..                                                 ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "500  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "501  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "502  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "1    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...  -11.186650  -20.759451  \n",
       "2    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   -7.928271   -9.596358  \n",
       "3    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...  -20.914663  -19.077866  \n",
       "4    [-1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, ...   -9.150852   -8.425693  \n",
       "..                                                 ...         ...         ...  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "500  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -11.355590   -9.454258  \n",
       "501  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -10.104707  -10.157057  \n",
       "502  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -8.592606   -9.658706  \n",
       "\n",
       "[503 rows x 19 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c331b",
   "metadata": {},
   "source": [
    "# Getting logits from CNN test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee318770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_test(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 2) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128,2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        x = torch.randn(30,30).view(-1,1,30,30)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.tanh(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "net = Net_test().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae65f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "323453d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib64/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.6659427881240845\n",
      "Epoch: 1. Loss: 0.5978161692619324\n",
      "Epoch: 2. Loss: 0.6174956560134888\n",
      "Epoch: 3. Loss: 0.5982579588890076\n",
      "Epoch: 4. Loss: 0.6039654016494751\n",
      "Epoch: 5. Loss: 0.6058598160743713\n",
      "Epoch: 6. Loss: 0.5957740545272827\n",
      "Epoch: 7. Loss: 0.5953888297080994\n",
      "Epoch: 8. Loss: 0.5996629595756531\n",
      "Epoch: 9. Loss: 0.59706711769104\n",
      "Epoch: 10. Loss: 0.5920834541320801\n",
      "Epoch: 11. Loss: 0.5911952257156372\n",
      "Epoch: 12. Loss: 0.5929105281829834\n",
      "Epoch: 13. Loss: 0.5925589799880981\n",
      "Epoch: 14. Loss: 0.5896614789962769\n",
      "Epoch: 15. Loss: 0.5872009992599487\n",
      "Epoch: 16. Loss: 0.5869786143302917\n",
      "Epoch: 17. Loss: 0.5873532295227051\n",
      "Epoch: 18. Loss: 0.5859675407409668\n",
      "Epoch: 19. Loss: 0.5833234190940857\n",
      "Epoch: 20. Loss: 0.581549882888794\n",
      "Epoch: 21. Loss: 0.5810942053794861\n",
      "Epoch: 22. Loss: 0.580150842666626\n",
      "Epoch: 23. Loss: 0.5776745080947876\n",
      "Epoch: 24. Loss: 0.5751029849052429\n",
      "Epoch: 25. Loss: 0.5737095475196838\n",
      "Epoch: 26. Loss: 0.5720832943916321\n",
      "Epoch: 27. Loss: 0.5689350366592407\n",
      "Epoch: 28. Loss: 0.5659328699111938\n",
      "Epoch: 29. Loss: 0.5638860464096069\n",
      "Epoch: 30. Loss: 0.5603866577148438\n",
      "Epoch: 31. Loss: 0.5562641620635986\n",
      "Epoch: 32. Loss: 0.5532025098800659\n",
      "Epoch: 33. Loss: 0.5484519004821777\n",
      "Epoch: 34. Loss: 0.5438640117645264\n",
      "Epoch: 35. Loss: 0.5392314791679382\n",
      "Epoch: 36. Loss: 0.5332193374633789\n",
      "Epoch: 37. Loss: 0.5279709696769714\n",
      "Epoch: 38. Loss: 0.5211919546127319\n",
      "Epoch: 39. Loss: 0.5146918296813965\n",
      "Epoch: 40. Loss: 0.5078135132789612\n",
      "Epoch: 41. Loss: 0.4992051422595978\n",
      "Epoch: 42. Loss: 0.4912663996219635\n",
      "Epoch: 43. Loss: 0.48378995060920715\n",
      "Epoch: 44. Loss: 0.47536948323249817\n",
      "Epoch: 45. Loss: 0.4661058187484741\n",
      "Epoch: 46. Loss: 0.4539138078689575\n",
      "Epoch: 47. Loss: 0.4416190981864929\n",
      "Epoch: 48. Loss: 0.4296482801437378\n",
      "Epoch: 49. Loss: 0.41807717084884644\n",
      "Epoch: 50. Loss: 0.411738783121109\n",
      "Epoch: 51. Loss: 0.42996710538864136\n",
      "Epoch: 52. Loss: 0.42742639780044556\n",
      "Epoch: 53. Loss: 0.3801381587982178\n",
      "Epoch: 54. Loss: 0.4159143567085266\n",
      "Epoch: 55. Loss: 0.35253265500068665\n",
      "Epoch: 56. Loss: 0.3829740583896637\n",
      "Epoch: 57. Loss: 0.3371771275997162\n",
      "Epoch: 58. Loss: 0.3590925931930542\n",
      "Epoch: 59. Loss: 0.31737327575683594\n",
      "Epoch: 60. Loss: 0.3303844928741455\n",
      "Epoch: 61. Loss: 0.30394771695137024\n",
      "Epoch: 62. Loss: 0.3034481406211853\n",
      "Epoch: 63. Loss: 0.2844410538673401\n",
      "Epoch: 64. Loss: 0.2798522412776947\n",
      "Epoch: 65. Loss: 0.26711833477020264\n",
      "Epoch: 66. Loss: 0.2568872570991516\n",
      "Epoch: 67. Loss: 0.2460758090019226\n",
      "Epoch: 68. Loss: 0.2376655787229538\n",
      "Epoch: 69. Loss: 0.22563688457012177\n",
      "Epoch: 70. Loss: 0.219034805893898\n",
      "Epoch: 71. Loss: 0.20343933999538422\n",
      "Epoch: 72. Loss: 0.20168857276439667\n",
      "Epoch: 73. Loss: 0.1837223619222641\n",
      "Epoch: 74. Loss: 0.18457424640655518\n",
      "Epoch: 75. Loss: 0.16750818490982056\n",
      "Epoch: 76. Loss: 0.16366448998451233\n",
      "Epoch: 77. Loss: 0.15494726598262787\n",
      "Epoch: 78. Loss: 0.143253356218338\n",
      "Epoch: 79. Loss: 0.14155666530132294\n",
      "Epoch: 80. Loss: 0.12921670079231262\n",
      "Epoch: 81. Loss: 0.12443500757217407\n",
      "Epoch: 82. Loss: 0.12006520479917526\n",
      "Epoch: 83. Loss: 0.1093350425362587\n",
      "Epoch: 84. Loss: 0.1068853884935379\n",
      "Epoch: 85. Loss: 0.10186318308115005\n",
      "Epoch: 86. Loss: 0.09292260557413101\n",
      "Epoch: 87. Loss: 0.09101428836584091\n",
      "Epoch: 88. Loss: 0.08690663427114487\n",
      "Epoch: 89. Loss: 0.07925208657979965\n",
      "Epoch: 90. Loss: 0.0773550271987915\n",
      "Epoch: 91. Loss: 0.07473476231098175\n",
      "Epoch: 92. Loss: 0.06821358948945999\n",
      "Epoch: 93. Loss: 0.06595367938280106\n",
      "Epoch: 94. Loss: 0.06447586417198181\n",
      "Epoch: 95. Loss: 0.05943655967712402\n",
      "Epoch: 96. Loss: 0.056856803596019745\n",
      "Epoch: 97. Loss: 0.056030403822660446\n",
      "Epoch: 98. Loss: 0.05253087356686592\n",
      "Epoch: 99. Loss: 0.049894507974386215\n",
      "Epoch: 100. Loss: 0.0492221899330616\n",
      "Epoch: 101. Loss: 0.04697367176413536\n",
      "Epoch: 102. Loss: 0.04459219425916672\n",
      "Epoch: 103. Loss: 0.04387882724404335\n",
      "Epoch: 104. Loss: 0.042513519525527954\n",
      "Epoch: 105. Loss: 0.040554411709308624\n",
      "Epoch: 106. Loss: 0.03977256640791893\n",
      "Epoch: 107. Loss: 0.03891923278570175\n",
      "Epoch: 108. Loss: 0.03740387037396431\n",
      "Epoch: 109. Loss: 0.03660792484879494\n",
      "Epoch: 110. Loss: 0.036053020507097244\n",
      "Epoch: 111. Loss: 0.03495270013809204\n",
      "Epoch: 112. Loss: 0.034183647483587265\n",
      "Epoch: 113. Loss: 0.03378730267286301\n",
      "Epoch: 114. Loss: 0.033014386892318726\n",
      "Epoch: 115. Loss: 0.032299939543008804\n",
      "Epoch: 116. Loss: 0.031974393874406815\n",
      "Epoch: 117. Loss: 0.03147752210497856\n",
      "Epoch: 118. Loss: 0.030850369483232498\n",
      "Epoch: 119. Loss: 0.030513187870383263\n",
      "Epoch: 120. Loss: 0.030206382274627686\n",
      "Epoch: 121. Loss: 0.029718732461333275\n",
      "Epoch: 122. Loss: 0.02935078553855419\n",
      "Epoch: 123. Loss: 0.029136553406715393\n",
      "Epoch: 124. Loss: 0.02881847694516182\n",
      "Epoch: 125. Loss: 0.028461618348956108\n",
      "Epoch: 126. Loss: 0.02823481895029545\n",
      "Epoch: 127. Loss: 0.028045454993844032\n",
      "Epoch: 128. Loss: 0.027775438502430916\n",
      "Epoch: 129. Loss: 0.027519138529896736\n",
      "Epoch: 130. Loss: 0.02735690213739872\n",
      "Epoch: 131. Loss: 0.02719871886074543\n",
      "Epoch: 132. Loss: 0.026989338919520378\n",
      "Epoch: 133. Loss: 0.02678997628390789\n",
      "Epoch: 134. Loss: 0.026653887704014778\n",
      "Epoch: 135. Loss: 0.02653348259627819\n",
      "Epoch: 136. Loss: 0.026382073760032654\n",
      "Epoch: 137. Loss: 0.02621903270483017\n",
      "Epoch: 138. Loss: 0.026086710393428802\n",
      "Epoch: 139. Loss: 0.025985844433307648\n",
      "Epoch: 140. Loss: 0.025890663266181946\n",
      "Epoch: 141. Loss: 0.02577834203839302\n",
      "Epoch: 142. Loss: 0.02565772831439972\n",
      "Epoch: 143. Loss: 0.025546176359057426\n",
      "Epoch: 144. Loss: 0.02545284293591976\n",
      "Epoch: 145. Loss: 0.025373550131917\n",
      "Epoch: 146. Loss: 0.025299666449427605\n",
      "Epoch: 147. Loss: 0.025223029777407646\n",
      "Epoch: 148. Loss: 0.025144588202238083\n",
      "Epoch: 149. Loss: 0.025065846741199493\n",
      "Epoch: 150. Loss: 0.02498771622776985\n",
      "Epoch: 151. Loss: 0.024913841858506203\n",
      "Epoch: 152. Loss: 0.02484428882598877\n",
      "Epoch: 153. Loss: 0.024779513478279114\n",
      "Epoch: 154. Loss: 0.024718062952160835\n",
      "Epoch: 155. Loss: 0.024659939110279083\n",
      "Epoch: 156. Loss: 0.02460431680083275\n",
      "Epoch: 157. Loss: 0.024550504982471466\n",
      "Epoch: 158. Loss: 0.024499686434864998\n",
      "Epoch: 159. Loss: 0.024449989199638367\n",
      "Epoch: 160. Loss: 0.02440357767045498\n",
      "Epoch: 161. Loss: 0.024361100047826767\n",
      "Epoch: 162. Loss: 0.02432723343372345\n",
      "Epoch: 163. Loss: 0.0243125781416893\n",
      "Epoch: 164. Loss: 0.02435041032731533\n",
      "Epoch: 165. Loss: 0.024544477462768555\n",
      "Epoch: 166. Loss: 0.02516007050871849\n",
      "Epoch: 167. Loss: 0.026808639988303185\n",
      "Epoch: 168. Loss: 0.029874268919229507\n",
      "Epoch: 169. Loss: 0.03195309638977051\n",
      "Epoch: 170. Loss: 0.02971259504556656\n",
      "Epoch: 171. Loss: 0.025407716631889343\n",
      "Epoch: 172. Loss: 0.02403055503964424\n",
      "Epoch: 173. Loss: 0.02574285678565502\n",
      "Epoch: 174. Loss: 0.027982240542769432\n",
      "Epoch: 175. Loss: 0.027310887351632118\n",
      "Epoch: 176. Loss: 0.024729285389184952\n",
      "Epoch: 177. Loss: 0.023950058966875076\n",
      "Epoch: 178. Loss: 0.02515316568315029\n",
      "Epoch: 179. Loss: 0.026294918730854988\n",
      "Epoch: 180. Loss: 0.02533678337931633\n",
      "Epoch: 181. Loss: 0.023979956284165382\n",
      "Epoch: 182. Loss: 0.023961879312992096\n",
      "Epoch: 183. Loss: 0.024911966174840927\n",
      "Epoch: 184. Loss: 0.025192875415086746\n",
      "Epoch: 185. Loss: 0.024355310946702957\n",
      "Epoch: 186. Loss: 0.02369835413992405\n",
      "Epoch: 187. Loss: 0.023918040096759796\n",
      "Epoch: 188. Loss: 0.024505186825990677\n",
      "Epoch: 189. Loss: 0.02450355514883995\n",
      "Epoch: 190. Loss: 0.023968027904629707\n",
      "Epoch: 191. Loss: 0.023575006052851677\n",
      "Epoch: 192. Loss: 0.023716429248452187\n",
      "Epoch: 193. Loss: 0.024040915071964264\n",
      "Epoch: 194. Loss: 0.024087032303214073\n",
      "Epoch: 195. Loss: 0.023793134838342667\n",
      "Epoch: 196. Loss: 0.023514797911047935\n",
      "Epoch: 197. Loss: 0.0234906617552042\n",
      "Epoch: 198. Loss: 0.023672429844737053\n",
      "Epoch: 199. Loss: 0.023803509771823883\n",
      "Epoch: 200. Loss: 0.023752568289637566\n",
      "Epoch: 201. Loss: 0.023558933287858963\n",
      "Epoch: 202. Loss: 0.023401906713843346\n",
      "Epoch: 203. Loss: 0.023365359753370285\n",
      "Epoch: 204. Loss: 0.023440983146429062\n",
      "Epoch: 205. Loss: 0.02353416010737419\n",
      "Epoch: 206. Loss: 0.023573460057377815\n",
      "Epoch: 207. Loss: 0.023530855774879456\n",
      "Epoch: 208. Loss: 0.02344023808836937\n",
      "Epoch: 209. Loss: 0.023341108113527298\n",
      "Epoch: 210. Loss: 0.023272458463907242\n",
      "Epoch: 211. Loss: 0.02324030175805092\n",
      "Epoch: 212. Loss: 0.023245766758918762\n",
      "Epoch: 213. Loss: 0.023272544145584106\n",
      "Epoch: 214. Loss: 0.023310821503400803\n",
      "Epoch: 215. Loss: 0.02335038036108017\n",
      "Epoch: 216. Loss: 0.02339240349829197\n",
      "Epoch: 217. Loss: 0.02343379519879818\n",
      "Epoch: 218. Loss: 0.02348233014345169\n",
      "Epoch: 219. Loss: 0.023542705923318863\n",
      "Epoch: 220. Loss: 0.023623937740921974\n",
      "Epoch: 221. Loss: 0.023730648681521416\n",
      "Epoch: 222. Loss: 0.02387385070323944\n",
      "Epoch: 223. Loss: 0.024046875536441803\n",
      "Epoch: 224. Loss: 0.02424713969230652\n",
      "Epoch: 225. Loss: 0.02441919967532158\n",
      "Epoch: 226. Loss: 0.024542100727558136\n",
      "Epoch: 227. Loss: 0.024570895358920097\n",
      "Epoch: 228. Loss: 0.024469563737511635\n",
      "Epoch: 229. Loss: 0.02424410730600357\n",
      "Epoch: 230. Loss: 0.023983683437108994\n",
      "Epoch: 231. Loss: 0.023726891726255417\n",
      "Epoch: 232. Loss: 0.023519359529018402\n",
      "Epoch: 233. Loss: 0.02336394600570202\n",
      "Epoch: 234. Loss: 0.023259539157152176\n",
      "Epoch: 235. Loss: 0.02319267950952053\n",
      "Epoch: 236. Loss: 0.023156430572271347\n",
      "Epoch: 237. Loss: 0.023144850507378578\n",
      "Epoch: 238. Loss: 0.023159105330705643\n",
      "Epoch: 239. Loss: 0.023205261677503586\n",
      "Epoch: 240. Loss: 0.023301057517528534\n",
      "Epoch: 241. Loss: 0.023469483479857445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242. Loss: 0.023757802322506905\n",
      "Epoch: 243. Loss: 0.02420501969754696\n",
      "Epoch: 244. Loss: 0.024792956188321114\n",
      "Epoch: 245. Loss: 0.025325169786810875\n",
      "Epoch: 246. Loss: 0.025550346821546555\n",
      "Epoch: 247. Loss: 0.02520829625427723\n",
      "Epoch: 248. Loss: 0.024534642696380615\n",
      "Epoch: 249. Loss: 0.023822985589504242\n",
      "Epoch: 250. Loss: 0.023314081132411957\n",
      "Epoch: 251. Loss: 0.023020150139927864\n",
      "Epoch: 252. Loss: 0.022891227155923843\n",
      "Epoch: 253. Loss: 0.02287665195763111\n",
      "Epoch: 254. Loss: 0.02294742502272129\n",
      "Epoch: 255. Loss: 0.023093320429325104\n",
      "Epoch: 256. Loss: 0.023322902619838715\n",
      "Epoch: 257. Loss: 0.023649021983146667\n",
      "Epoch: 258. Loss: 0.024042177945375443\n",
      "Epoch: 259. Loss: 0.024435119703412056\n",
      "Epoch: 260. Loss: 0.02465064451098442\n",
      "Epoch: 261. Loss: 0.02461429126560688\n",
      "Epoch: 262. Loss: 0.024289075285196304\n",
      "Epoch: 263. Loss: 0.023866044357419014\n",
      "Epoch: 264. Loss: 0.0234562698751688\n",
      "Epoch: 265. Loss: 0.02315373718738556\n",
      "Epoch: 266. Loss: 0.02295735850930214\n",
      "Epoch: 267. Loss: 0.022847216576337814\n",
      "Epoch: 268. Loss: 0.022796208038926125\n",
      "Epoch: 269. Loss: 0.022783342748880386\n",
      "Epoch: 270. Loss: 0.0227972399443388\n",
      "Epoch: 271. Loss: 0.022835392504930496\n",
      "Epoch: 272. Loss: 0.02290620654821396\n",
      "Epoch: 273. Loss: 0.023027898743748665\n",
      "Epoch: 274. Loss: 0.023231126368045807\n",
      "Epoch: 275. Loss: 0.02355066128075123\n",
      "Epoch: 276. Loss: 0.024012770503759384\n",
      "Epoch: 277. Loss: 0.024557441473007202\n",
      "Epoch: 278. Loss: 0.025023888796567917\n",
      "Epoch: 279. Loss: 0.025113491341471672\n",
      "Epoch: 280. Loss: 0.02480289898812771\n",
      "Epoch: 281. Loss: 0.024230899289250374\n",
      "Epoch: 282. Loss: 0.023668663576245308\n",
      "Epoch: 283. Loss: 0.023238547146320343\n",
      "Epoch: 284. Loss: 0.022970134392380714\n",
      "Epoch: 285. Loss: 0.022817859426140785\n",
      "Epoch: 286. Loss: 0.0227408055216074\n",
      "Epoch: 287. Loss: 0.022709166631102562\n",
      "Epoch: 288. Loss: 0.02270604856312275\n",
      "Epoch: 289. Loss: 0.022724280133843422\n",
      "Epoch: 290. Loss: 0.022765247151255608\n",
      "Epoch: 291. Loss: 0.022837435826659203\n",
      "Epoch: 292. Loss: 0.022958844900131226\n",
      "Epoch: 293. Loss: 0.02316177636384964\n",
      "Epoch: 294. Loss: 0.02349328063428402\n",
      "Epoch: 295. Loss: 0.023983294144272804\n",
      "Epoch: 296. Loss: 0.024568412452936172\n",
      "Epoch: 297. Loss: 0.025061246007680893\n",
      "Epoch: 298. Loss: 0.025136299431324005\n",
      "Epoch: 299. Loss: 0.024769499897956848\n",
      "Epoch: 300. Loss: 0.02413123846054077\n",
      "Epoch: 301. Loss: 0.023536385968327522\n",
      "Epoch: 302. Loss: 0.023099279031157494\n",
      "Epoch: 303. Loss: 0.02283807285130024\n",
      "Epoch: 304. Loss: 0.02270277962088585\n",
      "Epoch: 305. Loss: 0.022647814825177193\n",
      "Epoch: 306. Loss: 0.022642912343144417\n",
      "Epoch: 307. Loss: 0.022672010585665703\n",
      "Epoch: 308. Loss: 0.022731294855475426\n",
      "Epoch: 309. Loss: 0.02283070795238018\n",
      "Epoch: 310. Loss: 0.022993510589003563\n",
      "Epoch: 311. Loss: 0.02324545755982399\n",
      "Epoch: 312. Loss: 0.023614119738340378\n",
      "Epoch: 313. Loss: 0.024083198979496956\n",
      "Epoch: 314. Loss: 0.024561544880270958\n",
      "Epoch: 315. Loss: 0.02480287104845047\n",
      "Epoch: 316. Loss: 0.024707116186618805\n",
      "Epoch: 317. Loss: 0.024279549717903137\n",
      "Epoch: 318. Loss: 0.02376960590481758\n",
      "Epoch: 319. Loss: 0.023314939811825752\n",
      "Epoch: 320. Loss: 0.02299416810274124\n",
      "Epoch: 321. Loss: 0.02279024012386799\n",
      "Epoch: 322. Loss: 0.02267521619796753\n",
      "Epoch: 323. Loss: 0.022617308422923088\n",
      "Epoch: 324. Loss: 0.02259199321269989\n",
      "Epoch: 325. Loss: 0.02258474938571453\n",
      "Epoch: 326. Loss: 0.02258930914103985\n",
      "Epoch: 327. Loss: 0.02260550484061241\n",
      "Epoch: 328. Loss: 0.022638488560914993\n",
      "Epoch: 329. Loss: 0.022696947678923607\n",
      "Epoch: 330. Loss: 0.0227976031601429\n",
      "Epoch: 331. Loss: 0.022975951433181763\n",
      "Epoch: 332. Loss: 0.023278603330254555\n",
      "Epoch: 333. Loss: 0.023753901943564415\n",
      "Epoch: 334. Loss: 0.024396363645792007\n",
      "Epoch: 335. Loss: 0.025037050247192383\n",
      "Epoch: 336. Loss: 0.02527349442243576\n",
      "Epoch: 337. Loss: 0.0249794889241457\n",
      "Epoch: 338. Loss: 0.02428053505718708\n",
      "Epoch: 339. Loss: 0.023584887385368347\n",
      "Epoch: 340. Loss: 0.023079562932252884\n",
      "Epoch: 341. Loss: 0.02278127521276474\n",
      "Epoch: 342. Loss: 0.02262485772371292\n",
      "Epoch: 343. Loss: 0.022555753588676453\n",
      "Epoch: 344. Loss: 0.02254013530910015\n",
      "Epoch: 345. Loss: 0.022563913837075233\n",
      "Epoch: 346. Loss: 0.022625062614679337\n",
      "Epoch: 347. Loss: 0.022731278091669083\n",
      "Epoch: 348. Loss: 0.022903606295585632\n",
      "Epoch: 349. Loss: 0.023167580366134644\n",
      "Epoch: 350. Loss: 0.02354193665087223\n",
      "Epoch: 351. Loss: 0.02399117313325405\n",
      "Epoch: 352. Loss: 0.02441287413239479\n",
      "Epoch: 353. Loss: 0.02459276095032692\n",
      "Epoch: 354. Loss: 0.024432694539427757\n",
      "Epoch: 355. Loss: 0.0239887572824955\n",
      "Epoch: 356. Loss: 0.023499051108956337\n",
      "Epoch: 357. Loss: 0.02309184893965721\n",
      "Epoch: 358. Loss: 0.022816624492406845\n",
      "Epoch: 359. Loss: 0.02265261858701706\n",
      "Epoch: 360. Loss: 0.022564241662621498\n",
      "Epoch: 361. Loss: 0.022521737962961197\n",
      "Epoch: 362. Loss: 0.022505253553390503\n",
      "Epoch: 363. Loss: 0.02250429429113865\n",
      "Epoch: 364. Loss: 0.02251511998474598\n",
      "Epoch: 365. Loss: 0.022537866607308388\n",
      "Epoch: 366. Loss: 0.022578194737434387\n",
      "Epoch: 367. Loss: 0.022649012506008148\n",
      "Epoch: 368. Loss: 0.02277333289384842\n",
      "Epoch: 369. Loss: 0.022980522364377975\n",
      "Epoch: 370. Loss: 0.023305337876081467\n",
      "Epoch: 371. Loss: 0.023777291178703308\n",
      "Epoch: 372. Loss: 0.02433181367814541\n",
      "Epoch: 373. Loss: 0.02479756623506546\n",
      "Epoch: 374. Loss: 0.024895284324884415\n",
      "Epoch: 375. Loss: 0.02459404431283474\n",
      "Epoch: 376. Loss: 0.02402198128402233\n",
      "Epoch: 377. Loss: 0.023466454818844795\n",
      "Epoch: 378. Loss: 0.023042436689138412\n",
      "Epoch: 379. Loss: 0.02277296781539917\n",
      "Epoch: 380. Loss: 0.022615822032094002\n",
      "Epoch: 381. Loss: 0.022531509399414062\n",
      "Epoch: 382. Loss: 0.022489231079816818\n",
      "Epoch: 383. Loss: 0.022470539435744286\n",
      "Epoch: 384. Loss: 0.02246476337313652\n",
      "Epoch: 385. Loss: 0.02246726118028164\n",
      "Epoch: 386. Loss: 0.022478250786662102\n",
      "Epoch: 387. Loss: 0.0225004144012928\n",
      "Epoch: 388. Loss: 0.02254054881632328\n",
      "Epoch: 389. Loss: 0.02261439338326454\n",
      "Epoch: 390. Loss: 0.0227458868175745\n",
      "Epoch: 391. Loss: 0.022972390055656433\n",
      "Epoch: 392. Loss: 0.023350590839982033\n",
      "Epoch: 393. Loss: 0.023898370563983917\n",
      "Epoch: 394. Loss: 0.02455497533082962\n",
      "Epoch: 395. Loss: 0.02503117173910141\n",
      "Epoch: 396. Loss: 0.02506665140390396\n",
      "Epoch: 397. Loss: 0.02458243817090988\n",
      "Epoch: 398. Loss: 0.023926272988319397\n",
      "Epoch: 399. Loss: 0.023340342566370964\n",
      "Epoch: 400. Loss: 0.022940468043088913\n",
      "Epoch: 401. Loss: 0.022696826606988907\n",
      "Epoch: 402. Loss: 0.02256297692656517\n",
      "Epoch: 403. Loss: 0.022491000592708588\n",
      "Epoch: 404. Loss: 0.02245578169822693\n",
      "Epoch: 405. Loss: 0.02243908680975437\n",
      "Epoch: 406. Loss: 0.022432610392570496\n",
      "Epoch: 407. Loss: 0.022433122619986534\n",
      "Epoch: 408. Loss: 0.022438306361436844\n",
      "Epoch: 409. Loss: 0.022449158132076263\n",
      "Epoch: 410. Loss: 0.02247019298374653\n",
      "Epoch: 411. Loss: 0.022509995847940445\n",
      "Epoch: 412. Loss: 0.022583208978176117\n",
      "Epoch: 413. Loss: 0.022715434432029724\n",
      "Epoch: 414. Loss: 0.02294349856674671\n",
      "Epoch: 415. Loss: 0.02332797273993492\n",
      "Epoch: 416. Loss: 0.02391860820353031\n",
      "Epoch: 417. Loss: 0.024643000215291977\n",
      "Epoch: 418. Loss: 0.02517569810152054\n",
      "Epoch: 419. Loss: 0.025203844532370567\n",
      "Epoch: 420. Loss: 0.024659479036927223\n",
      "Epoch: 421. Loss: 0.02393322065472603\n",
      "Epoch: 422. Loss: 0.023306695744395256\n",
      "Epoch: 423. Loss: 0.02288944087922573\n",
      "Epoch: 424. Loss: 0.022644324228167534\n",
      "Epoch: 425. Loss: 0.022514333948493004\n",
      "Epoch: 426. Loss: 0.0224477369338274\n",
      "Epoch: 427. Loss: 0.02241642214357853\n",
      "Epoch: 428. Loss: 0.022406931966543198\n",
      "Epoch: 429. Loss: 0.022410579025745392\n",
      "Epoch: 430. Loss: 0.02242632769048214\n",
      "Epoch: 431. Loss: 0.022455308586359024\n",
      "Epoch: 432. Loss: 0.022503163665533066\n",
      "Epoch: 433. Loss: 0.02258342318236828\n",
      "Epoch: 434. Loss: 0.022719094529747963\n",
      "Epoch: 435. Loss: 0.022939184680581093\n",
      "Epoch: 436. Loss: 0.02329116128385067\n",
      "Epoch: 437. Loss: 0.023789800703525543\n",
      "Epoch: 438. Loss: 0.024363070726394653\n",
      "Epoch: 439. Loss: 0.02476213499903679\n",
      "Epoch: 440. Loss: 0.024782976135611534\n",
      "Epoch: 441. Loss: 0.024366499856114388\n",
      "Epoch: 442. Loss: 0.023785756900906563\n",
      "Epoch: 443. Loss: 0.023246722295880318\n",
      "Epoch: 444. Loss: 0.022865820676088333\n",
      "Epoch: 445. Loss: 0.022630462422966957\n",
      "Epoch: 446. Loss: 0.022497713565826416\n",
      "Epoch: 447. Loss: 0.022428689524531364\n",
      "Epoch: 448. Loss: 0.02239604853093624\n",
      "Epoch: 449. Loss: 0.022384651005268097\n",
      "Epoch: 450. Loss: 0.022387443110346794\n",
      "Epoch: 451. Loss: 0.02240080013871193\n",
      "Epoch: 452. Loss: 0.02242649532854557\n",
      "Epoch: 453. Loss: 0.02247123420238495\n",
      "Epoch: 454. Loss: 0.022546807304024696\n",
      "Epoch: 455. Loss: 0.02267363853752613\n",
      "Epoch: 456. Loss: 0.02287762612104416\n",
      "Epoch: 457. Loss: 0.02318539097905159\n",
      "Epoch: 458. Loss: 0.023606641218066216\n",
      "Epoch: 459. Loss: 0.024110235273838043\n",
      "Epoch: 460. Loss: 0.024543052539229393\n",
      "Epoch: 461. Loss: 0.024666665121912956\n",
      "Epoch: 462. Loss: 0.024365901947021484\n",
      "Epoch: 463. Loss: 0.023851802572607994\n",
      "Epoch: 464. Loss: 0.023326285183429718\n",
      "Epoch: 465. Loss: 0.02293189987540245\n",
      "Epoch: 466. Loss: 0.022672368213534355\n",
      "Epoch: 467. Loss: 0.022517481818795204\n",
      "Epoch: 468. Loss: 0.02243349328637123\n",
      "Epoch: 469. Loss: 0.022391604259610176\n",
      "Epoch: 470. Loss: 0.022371511906385422\n",
      "Epoch: 471. Loss: 0.022365404292941093\n",
      "Epoch: 472. Loss: 0.022367719560861588\n",
      "Epoch: 473. Loss: 0.022377273067831993\n",
      "Epoch: 474. Loss: 0.022396128624677658\n",
      "Epoch: 475. Loss: 0.022428318858146667\n",
      "Epoch: 476. Loss: 0.022482553496956825\n",
      "Epoch: 477. Loss: 0.022575708106160164\n",
      "Epoch: 478. Loss: 0.022732872515916824\n",
      "Epoch: 479. Loss: 0.02298584021627903\n",
      "Epoch: 480. Loss: 0.023372408002614975\n",
      "Epoch: 481. Loss: 0.02387937717139721\n",
      "Epoch: 482. Loss: 0.024405917152762413\n",
      "Epoch: 483. Loss: 0.024694066494703293\n",
      "Epoch: 484. Loss: 0.024614060297608376\n",
      "Epoch: 485. Loss: 0.024170834571123123\n",
      "Epoch: 486. Loss: 0.023625053465366364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487. Loss: 0.023145703598856926\n",
      "Epoch: 488. Loss: 0.022812338545918465\n",
      "Epoch: 489. Loss: 0.022603899240493774\n",
      "Epoch: 490. Loss: 0.022481633350253105\n",
      "Epoch: 491. Loss: 0.022412804886698723\n",
      "Epoch: 492. Loss: 0.022376026958227158\n",
      "Epoch: 493. Loss: 0.02235713228583336\n",
      "Epoch: 494. Loss: 0.022347845137119293\n",
      "Epoch: 495. Loss: 0.022344360128045082\n",
      "Epoch: 496. Loss: 0.022345054894685745\n",
      "Epoch: 497. Loss: 0.022349387407302856\n",
      "Epoch: 498. Loss: 0.0223578829318285\n",
      "Epoch: 499. Loss: 0.0223736222833395\n",
      "Epoch: 500. Loss: 0.022402510046958923\n",
      "Epoch: 501. Loss: 0.022452177479863167\n",
      "Epoch: 502. Loss: 0.02253800444304943\n",
      "Epoch: 503. Loss: 0.022690054029226303\n",
      "Epoch: 504. Loss: 0.022953703999519348\n",
      "Epoch: 505. Loss: 0.023371538147330284\n",
      "Epoch: 506. Loss: 0.023950520902872086\n",
      "Epoch: 507. Loss: 0.024594413116574287\n",
      "Epoch: 508. Loss: 0.024988295510411263\n",
      "Epoch: 509. Loss: 0.02489127777516842\n",
      "Epoch: 510. Loss: 0.024321360513567924\n",
      "Epoch: 511. Loss: 0.02364921011030674\n",
      "Epoch: 512. Loss: 0.023088358342647552\n",
      "Epoch: 513. Loss: 0.02272256836295128\n",
      "Epoch: 514. Loss: 0.022507939487695694\n",
      "Epoch: 515. Loss: 0.022394903004169464\n",
      "Epoch: 516. Loss: 0.0223432257771492\n",
      "Epoch: 517. Loss: 0.022329729050397873\n",
      "Epoch: 518. Loss: 0.022343235090374947\n",
      "Epoch: 519. Loss: 0.02237984538078308\n",
      "Epoch: 520. Loss: 0.022442879155278206\n",
      "Epoch: 521. Loss: 0.02254222147166729\n",
      "Epoch: 522. Loss: 0.022695448249578476\n",
      "Epoch: 523. Loss: 0.02291492372751236\n",
      "Epoch: 524. Loss: 0.02321973256766796\n",
      "Epoch: 525. Loss: 0.023587629199028015\n",
      "Epoch: 526. Loss: 0.023956362158060074\n",
      "Epoch: 527. Loss: 0.024161752313375473\n",
      "Epoch: 528. Loss: 0.02411952242255211\n",
      "Epoch: 529. Loss: 0.02383115142583847\n",
      "Epoch: 530. Loss: 0.023455530405044556\n",
      "Epoch: 531. Loss: 0.02309511974453926\n",
      "Epoch: 532. Loss: 0.022815987467765808\n",
      "Epoch: 533. Loss: 0.02261948771774769\n",
      "Epoch: 534. Loss: 0.022493060678243637\n",
      "Epoch: 535. Loss: 0.02241520956158638\n",
      "Epoch: 536. Loss: 0.02236839570105076\n",
      "Epoch: 537. Loss: 0.02234029769897461\n",
      "Epoch: 538. Loss: 0.022325284779071808\n",
      "Epoch: 539. Loss: 0.022317346185445786\n",
      "Epoch: 540. Loss: 0.022313838824629784\n",
      "Epoch: 541. Loss: 0.022312721237540245\n",
      "Epoch: 542. Loss: 0.02231336571276188\n",
      "Epoch: 543. Loss: 0.022315092384815216\n",
      "Epoch: 544. Loss: 0.02231873758137226\n",
      "Epoch: 545. Loss: 0.02232527546584606\n",
      "Epoch: 546. Loss: 0.022338539361953735\n",
      "Epoch: 547. Loss: 0.02236381359398365\n",
      "Epoch: 548. Loss: 0.022410398349165916\n",
      "Epoch: 549. Loss: 0.022497298195958138\n",
      "Epoch: 550. Loss: 0.022656187415122986\n",
      "Epoch: 551. Loss: 0.022942988201975822\n",
      "Epoch: 552. Loss: 0.023414382711052895\n",
      "Epoch: 553. Loss: 0.024097399786114693\n",
      "Epoch: 554. Loss: 0.024821510538458824\n",
      "Epoch: 555. Loss: 0.025238949805498123\n",
      "Epoch: 556. Loss: 0.024989431723952293\n",
      "Epoch: 557. Loss: 0.024298258125782013\n",
      "Epoch: 558. Loss: 0.023541325703263283\n",
      "Epoch: 559. Loss: 0.022980941459536552\n",
      "Epoch: 560. Loss: 0.02262958697974682\n",
      "Epoch: 561. Loss: 0.02243719808757305\n",
      "Epoch: 562. Loss: 0.022343575954437256\n",
      "Epoch: 563. Loss: 0.022305523976683617\n",
      "Epoch: 564. Loss: 0.022299885749816895\n",
      "Epoch: 565. Loss: 0.022318510338664055\n",
      "Epoch: 566. Loss: 0.02236013486981392\n",
      "Epoch: 567. Loss: 0.02242875285446644\n",
      "Epoch: 568. Loss: 0.022535016760230064\n",
      "Epoch: 569. Loss: 0.022694610059261322\n",
      "Epoch: 570. Loss: 0.022923804819583893\n",
      "Epoch: 571. Loss: 0.02323099784553051\n",
      "Epoch: 572. Loss: 0.023592066019773483\n",
      "Epoch: 573. Loss: 0.02391865849494934\n",
      "Epoch: 574. Loss: 0.024090582504868507\n",
      "Epoch: 575. Loss: 0.024014819413423538\n",
      "Epoch: 576. Loss: 0.023740779608488083\n",
      "Epoch: 577. Loss: 0.023370373994112015\n",
      "Epoch: 578. Loss: 0.023031437769532204\n",
      "Epoch: 579. Loss: 0.02277553454041481\n",
      "Epoch: 580. Loss: 0.02260591834783554\n",
      "Epoch: 581. Loss: 0.022497810423374176\n",
      "Epoch: 582. Loss: 0.022428985685110092\n",
      "Epoch: 583. Loss: 0.02238631062209606\n",
      "Epoch: 584. Loss: 0.02236217074096203\n",
      "Epoch: 585. Loss: 0.022348320111632347\n",
      "Epoch: 586. Loss: 0.022342614829540253\n",
      "Epoch: 587. Loss: 0.022343363612890244\n",
      "Epoch: 588. Loss: 0.022350478917360306\n",
      "Epoch: 589. Loss: 0.022365683689713478\n",
      "Epoch: 590. Loss: 0.022393381223082542\n",
      "Epoch: 591. Loss: 0.02243848890066147\n",
      "Epoch: 592. Loss: 0.022511066868901253\n",
      "Epoch: 593. Loss: 0.022629575803875923\n",
      "Epoch: 594. Loss: 0.02281637117266655\n",
      "Epoch: 595. Loss: 0.02308899722993374\n",
      "Epoch: 596. Loss: 0.02345501445233822\n",
      "Epoch: 597. Loss: 0.023849068209528923\n",
      "Epoch: 598. Loss: 0.024145837873220444\n",
      "Epoch: 599. Loss: 0.02419396862387657\n",
      "Epoch: 600. Loss: 0.02398824878036976\n",
      "Epoch: 601. Loss: 0.023611601442098618\n",
      "Epoch: 602. Loss: 0.02322106435894966\n",
      "Epoch: 603. Loss: 0.022897660732269287\n",
      "Epoch: 604. Loss: 0.022670941427350044\n",
      "Epoch: 605. Loss: 0.022521818056702614\n",
      "Epoch: 606. Loss: 0.022427013143897057\n",
      "Epoch: 607. Loss: 0.022366827353835106\n",
      "Epoch: 608. Loss: 0.022329306229948997\n",
      "Epoch: 609. Loss: 0.022307541221380234\n",
      "Epoch: 610. Loss: 0.022295650094747543\n",
      "Epoch: 611. Loss: 0.022288046777248383\n",
      "Epoch: 612. Loss: 0.02228265441954136\n",
      "Epoch: 613. Loss: 0.02227884717285633\n",
      "Epoch: 614. Loss: 0.022276343777775764\n",
      "Epoch: 615. Loss: 0.02227417193353176\n",
      "Epoch: 616. Loss: 0.022272871807217598\n",
      "Epoch: 617. Loss: 0.022271620109677315\n",
      "Epoch: 618. Loss: 0.022270718589425087\n",
      "Epoch: 619. Loss: 0.02226981520652771\n",
      "Epoch: 620. Loss: 0.02226901613175869\n",
      "Epoch: 621. Loss: 0.0222683884203434\n",
      "Epoch: 622. Loss: 0.022268064320087433\n",
      "Epoch: 623. Loss: 0.022268468514084816\n",
      "Epoch: 624. Loss: 0.0222699586302042\n",
      "Epoch: 625. Loss: 0.022274350747466087\n",
      "Epoch: 626. Loss: 0.022284280508756638\n",
      "Epoch: 627. Loss: 0.022306282073259354\n",
      "Epoch: 628. Loss: 0.022352369502186775\n",
      "Epoch: 629. Loss: 0.022449227049946785\n",
      "Epoch: 630. Loss: 0.022653639316558838\n",
      "Epoch: 631. Loss: 0.023064861074090004\n",
      "Epoch: 632. Loss: 0.023831337690353394\n",
      "Epoch: 633. Loss: 0.024974312633275986\n",
      "Epoch: 634. Loss: 0.026074711233377457\n",
      "Epoch: 635. Loss: 0.02621391974389553\n",
      "Epoch: 636. Loss: 0.025273941457271576\n",
      "Epoch: 637. Loss: 0.024016404524445534\n",
      "Epoch: 638. Loss: 0.02311725728213787\n",
      "Epoch: 639. Loss: 0.022618664428591728\n",
      "Epoch: 640. Loss: 0.02238057367503643\n",
      "Epoch: 641. Loss: 0.022281229496002197\n",
      "Epoch: 642. Loss: 0.02226307988166809\n",
      "Epoch: 643. Loss: 0.022296836599707603\n",
      "Epoch: 644. Loss: 0.022369707003235817\n",
      "Epoch: 645. Loss: 0.02249458059668541\n",
      "Epoch: 646. Loss: 0.022697696462273598\n",
      "Epoch: 647. Loss: 0.023000633344054222\n",
      "Epoch: 648. Loss: 0.023388030007481575\n",
      "Epoch: 649. Loss: 0.023798897862434387\n",
      "Epoch: 650. Loss: 0.024071287363767624\n",
      "Epoch: 651. Loss: 0.024083297699689865\n",
      "Epoch: 652. Loss: 0.023808833211660385\n",
      "Epoch: 653. Loss: 0.023402776569128036\n",
      "Epoch: 654. Loss: 0.02299502119421959\n",
      "Epoch: 655. Loss: 0.02268480509519577\n",
      "Epoch: 656. Loss: 0.02247719280421734\n",
      "Epoch: 657. Loss: 0.02235240675508976\n",
      "Epoch: 658. Loss: 0.022287026047706604\n",
      "Epoch: 659. Loss: 0.022262614220380783\n",
      "Epoch: 660. Loss: 0.02226494625210762\n",
      "Epoch: 661. Loss: 0.02228567749261856\n",
      "Epoch: 662. Loss: 0.022323764860630035\n",
      "Epoch: 663. Loss: 0.022382518276572227\n",
      "Epoch: 664. Loss: 0.022463921457529068\n",
      "Epoch: 665. Loss: 0.022568505257368088\n",
      "Epoch: 666. Loss: 0.022705769166350365\n",
      "Epoch: 667. Loss: 0.022877605631947517\n",
      "Epoch: 668. Loss: 0.023075299337506294\n",
      "Epoch: 669. Loss: 0.023265177384018898\n",
      "Epoch: 670. Loss: 0.023419244214892387\n",
      "Epoch: 671. Loss: 0.023475151509046555\n",
      "Epoch: 672. Loss: 0.023427076637744904\n",
      "Epoch: 673. Loss: 0.023284995928406715\n",
      "Epoch: 674. Loss: 0.023100562393665314\n",
      "Epoch: 675. Loss: 0.022912614047527313\n",
      "Epoch: 676. Loss: 0.022752976045012474\n",
      "Epoch: 677. Loss: 0.022627167403697968\n",
      "Epoch: 678. Loss: 0.02253541722893715\n",
      "Epoch: 679. Loss: 0.022469153627753258\n",
      "Epoch: 680. Loss: 0.022426987066864967\n",
      "Epoch: 681. Loss: 0.022405046969652176\n",
      "Epoch: 682. Loss: 0.022400211542844772\n",
      "Epoch: 683. Loss: 0.02240777015686035\n",
      "Epoch: 684. Loss: 0.02242625132203102\n",
      "Epoch: 685. Loss: 0.02245875634253025\n",
      "Epoch: 686. Loss: 0.02250848524272442\n",
      "Epoch: 687. Loss: 0.022579623386263847\n",
      "Epoch: 688. Loss: 0.02268035151064396\n",
      "Epoch: 689. Loss: 0.022813724353909492\n",
      "Epoch: 690. Loss: 0.02298220433294773\n",
      "Epoch: 691. Loss: 0.023161979392170906\n",
      "Epoch: 692. Loss: 0.023327801376581192\n",
      "Epoch: 693. Loss: 0.0234421007335186\n",
      "Epoch: 694. Loss: 0.023481767624616623\n",
      "Epoch: 695. Loss: 0.023421531543135643\n",
      "Epoch: 696. Loss: 0.02328643575310707\n",
      "Epoch: 697. Loss: 0.0231060478836298\n",
      "Epoch: 698. Loss: 0.022929681465029716\n",
      "Epoch: 699. Loss: 0.022774234414100647\n",
      "Epoch: 700. Loss: 0.02265375293791294\n",
      "Epoch: 701. Loss: 0.022564293816685677\n",
      "Epoch: 702. Loss: 0.022503724321722984\n",
      "Epoch: 703. Loss: 0.02246592752635479\n",
      "Epoch: 704. Loss: 0.022448187693953514\n",
      "Epoch: 705. Loss: 0.02244710735976696\n",
      "Epoch: 706. Loss: 0.022460835054516792\n",
      "Epoch: 707. Loss: 0.022489631548523903\n",
      "Epoch: 708. Loss: 0.02254042774438858\n",
      "Epoch: 709. Loss: 0.022612694650888443\n",
      "Epoch: 710. Loss: 0.0227045938372612\n",
      "Epoch: 711. Loss: 0.02281109243631363\n",
      "Epoch: 712. Loss: 0.02293703332543373\n",
      "Epoch: 713. Loss: 0.023067306727170944\n",
      "Epoch: 714. Loss: 0.023189501836895943\n",
      "Epoch: 715. Loss: 0.023282621055841446\n",
      "Epoch: 716. Loss: 0.023325279355049133\n",
      "Epoch: 717. Loss: 0.02329111099243164\n",
      "Epoch: 718. Loss: 0.02319631353020668\n",
      "Epoch: 719. Loss: 0.02305639535188675\n",
      "Epoch: 720. Loss: 0.022906331345438957\n",
      "Epoch: 721. Loss: 0.022766202688217163\n",
      "Epoch: 722. Loss: 0.022654343396425247\n",
      "Epoch: 723. Loss: 0.02257400192320347\n",
      "Epoch: 724. Loss: 0.022519927471876144\n",
      "Epoch: 725. Loss: 0.022486932575702667\n",
      "Epoch: 726. Loss: 0.022469455376267433\n",
      "Epoch: 727. Loss: 0.02246590331196785\n",
      "Epoch: 728. Loss: 0.022475948557257652\n",
      "Epoch: 729. Loss: 0.022502146661281586\n",
      "Epoch: 730. Loss: 0.022544775158166885\n",
      "Epoch: 731. Loss: 0.022603198885917664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 732. Loss: 0.0226780716329813\n",
      "Epoch: 733. Loss: 0.02276698499917984\n",
      "Epoch: 734. Loss: 0.02286781556904316\n",
      "Epoch: 735. Loss: 0.02297261729836464\n",
      "Epoch: 736. Loss: 0.02307468093931675\n",
      "Epoch: 737. Loss: 0.02314705215394497\n",
      "Epoch: 738. Loss: 0.023176101967692375\n",
      "Epoch: 739. Loss: 0.023156171664595604\n",
      "Epoch: 740. Loss: 0.023102642968297005\n",
      "Epoch: 741. Loss: 0.02301480621099472\n",
      "Epoch: 742. Loss: 0.02291283756494522\n",
      "Epoch: 743. Loss: 0.022813552990555763\n",
      "Epoch: 744. Loss: 0.022727176547050476\n",
      "Epoch: 745. Loss: 0.02265132963657379\n",
      "Epoch: 746. Loss: 0.022587519139051437\n",
      "Epoch: 747. Loss: 0.022537289187312126\n",
      "Epoch: 748. Loss: 0.022503376007080078\n",
      "Epoch: 749. Loss: 0.022484909743070602\n",
      "Epoch: 750. Loss: 0.022484377026557922\n",
      "Epoch: 751. Loss: 0.02250019647181034\n",
      "Epoch: 752. Loss: 0.022534072399139404\n",
      "Epoch: 753. Loss: 0.02258383296430111\n",
      "Epoch: 754. Loss: 0.02265206351876259\n",
      "Epoch: 755. Loss: 0.022733459249138832\n",
      "Epoch: 756. Loss: 0.022829843685030937\n",
      "Epoch: 757. Loss: 0.02292938530445099\n",
      "Epoch: 758. Loss: 0.0230224821716547\n",
      "Epoch: 759. Loss: 0.023090705275535583\n",
      "Epoch: 760. Loss: 0.023127257823944092\n",
      "Epoch: 761. Loss: 0.02313067391514778\n",
      "Epoch: 762. Loss: 0.02310793660581112\n",
      "Epoch: 763. Loss: 0.023054959252476692\n",
      "Epoch: 764. Loss: 0.02297835424542427\n",
      "Epoch: 765. Loss: 0.02288760617375374\n",
      "Epoch: 766. Loss: 0.022797731682658195\n",
      "Epoch: 767. Loss: 0.02271289750933647\n",
      "Epoch: 768. Loss: 0.02264244854450226\n",
      "Epoch: 769. Loss: 0.022588355466723442\n",
      "Epoch: 770. Loss: 0.0225504282861948\n",
      "Epoch: 771. Loss: 0.02252626046538353\n",
      "Epoch: 772. Loss: 0.02251332253217697\n",
      "Epoch: 773. Loss: 0.022510532289743423\n",
      "Epoch: 774. Loss: 0.022521240636706352\n",
      "Epoch: 775. Loss: 0.02254348434507847\n",
      "Epoch: 776. Loss: 0.0225785281509161\n",
      "Epoch: 777. Loss: 0.022624127566814423\n",
      "Epoch: 778. Loss: 0.022679965943098068\n",
      "Epoch: 779. Loss: 0.02273768000304699\n",
      "Epoch: 780. Loss: 0.022799963131546974\n",
      "Epoch: 781. Loss: 0.0228594858199358\n",
      "Epoch: 782. Loss: 0.022915886715054512\n",
      "Epoch: 783. Loss: 0.022956253960728645\n",
      "Epoch: 784. Loss: 0.0229810643941164\n",
      "Epoch: 785. Loss: 0.02298579551279545\n",
      "Epoch: 786. Loss: 0.022967198863625526\n",
      "Epoch: 787. Loss: 0.02292693220078945\n",
      "Epoch: 788. Loss: 0.022879749536514282\n",
      "Epoch: 789. Loss: 0.022827621549367905\n",
      "Epoch: 790. Loss: 0.022776950150728226\n",
      "Epoch: 791. Loss: 0.02272745966911316\n",
      "Epoch: 792. Loss: 0.02268187887966633\n",
      "Epoch: 793. Loss: 0.022641995921730995\n",
      "Epoch: 794. Loss: 0.022613493725657463\n",
      "Epoch: 795. Loss: 0.022591132670640945\n",
      "Epoch: 796. Loss: 0.022575225681066513\n",
      "Epoch: 797. Loss: 0.02256912738084793\n",
      "Epoch: 798. Loss: 0.02257460542023182\n",
      "Epoch: 799. Loss: 0.022588631138205528\n",
      "Epoch: 800. Loss: 0.022610656917095184\n",
      "Epoch: 801. Loss: 0.022635983303189278\n",
      "Epoch: 802. Loss: 0.022667521610856056\n",
      "Epoch: 803. Loss: 0.0227065347135067\n",
      "Epoch: 804. Loss: 0.022752735763788223\n",
      "Epoch: 805. Loss: 0.022801639512181282\n",
      "Epoch: 806. Loss: 0.02284904569387436\n",
      "Epoch: 807. Loss: 0.02288997173309326\n",
      "Epoch: 808. Loss: 0.02291734144091606\n",
      "Epoch: 809. Loss: 0.022922568023204803\n",
      "Epoch: 810. Loss: 0.022910024970769882\n",
      "Epoch: 811. Loss: 0.022877898067235947\n",
      "Epoch: 812. Loss: 0.02283482812345028\n",
      "Epoch: 813. Loss: 0.02278587594628334\n",
      "Epoch: 814. Loss: 0.022740373387932777\n",
      "Epoch: 815. Loss: 0.02269826829433441\n",
      "Epoch: 816. Loss: 0.022665120661258698\n",
      "Epoch: 817. Loss: 0.02264023944735527\n",
      "Epoch: 818. Loss: 0.022623451426625252\n",
      "Epoch: 819. Loss: 0.022611722350120544\n",
      "Epoch: 820. Loss: 0.022608183324337006\n",
      "Epoch: 821. Loss: 0.022612232714891434\n",
      "Epoch: 822. Loss: 0.022622931748628616\n",
      "Epoch: 823. Loss: 0.022638661786913872\n",
      "Epoch: 824. Loss: 0.02266216278076172\n",
      "Epoch: 825. Loss: 0.022689711302518845\n",
      "Epoch: 826. Loss: 0.02272447757422924\n",
      "Epoch: 827. Loss: 0.022763682529330254\n",
      "Epoch: 828. Loss: 0.022795947268605232\n",
      "Epoch: 829. Loss: 0.02282026782631874\n",
      "Epoch: 830. Loss: 0.022838721051812172\n",
      "Epoch: 831. Loss: 0.022853119298815727\n",
      "Epoch: 832. Loss: 0.022866010665893555\n",
      "Epoch: 833. Loss: 0.022868573665618896\n",
      "Epoch: 834. Loss: 0.02286134660243988\n",
      "Epoch: 835. Loss: 0.02284291759133339\n",
      "Epoch: 836. Loss: 0.022819802165031433\n",
      "Epoch: 837. Loss: 0.022796574980020523\n",
      "Epoch: 838. Loss: 0.022774461656808853\n",
      "Epoch: 839. Loss: 0.022748520597815514\n",
      "Epoch: 840. Loss: 0.022725891321897507\n",
      "Epoch: 841. Loss: 0.022705860435962677\n",
      "Epoch: 842. Loss: 0.022687245160341263\n",
      "Epoch: 843. Loss: 0.022674456238746643\n",
      "Epoch: 844. Loss: 0.022664014250040054\n",
      "Epoch: 845. Loss: 0.02265375293791294\n",
      "Epoch: 846. Loss: 0.0226454995572567\n",
      "Epoch: 847. Loss: 0.022636160254478455\n",
      "Epoch: 848. Loss: 0.022631371393799782\n",
      "Epoch: 849. Loss: 0.022623568773269653\n",
      "Epoch: 850. Loss: 0.022616997361183167\n",
      "Epoch: 851. Loss: 0.022611362859606743\n",
      "Epoch: 852. Loss: 0.02261187881231308\n",
      "Epoch: 853. Loss: 0.022610411047935486\n",
      "Epoch: 854. Loss: 0.022612273693084717\n",
      "Epoch: 855. Loss: 0.022621890529990196\n",
      "Epoch: 856. Loss: 0.0226341113448143\n",
      "Epoch: 857. Loss: 0.02264741063117981\n",
      "Epoch: 858. Loss: 0.022662915289402008\n",
      "Epoch: 859. Loss: 0.022679205983877182\n",
      "Epoch: 860. Loss: 0.022700369358062744\n",
      "Epoch: 861. Loss: 0.022723304107785225\n",
      "Epoch: 862. Loss: 0.022748379036784172\n",
      "Epoch: 863. Loss: 0.022768311202526093\n",
      "Epoch: 864. Loss: 0.022785888984799385\n",
      "Epoch: 865. Loss: 0.022799693048000336\n",
      "Epoch: 866. Loss: 0.022813944146037102\n",
      "Epoch: 867. Loss: 0.02281871996819973\n",
      "Epoch: 868. Loss: 0.022813480347394943\n",
      "Epoch: 869. Loss: 0.022798560559749603\n",
      "Epoch: 870. Loss: 0.022779062390327454\n",
      "Epoch: 871. Loss: 0.02275783382356167\n",
      "Epoch: 872. Loss: 0.02273615635931492\n",
      "Epoch: 873. Loss: 0.022708289325237274\n",
      "Epoch: 874. Loss: 0.022681551054120064\n",
      "Epoch: 875. Loss: 0.022655902430415154\n",
      "Epoch: 876. Loss: 0.022631673142313957\n",
      "Epoch: 877. Loss: 0.022608188912272453\n",
      "Epoch: 878. Loss: 0.022592881694436073\n",
      "Epoch: 879. Loss: 0.022588232532143593\n",
      "Epoch: 880. Loss: 0.02259415201842785\n",
      "Epoch: 881. Loss: 0.022608662024140358\n",
      "Epoch: 882. Loss: 0.02262810617685318\n",
      "Epoch: 883. Loss: 0.022647975012660027\n",
      "Epoch: 884. Loss: 0.02266899310052395\n",
      "Epoch: 885. Loss: 0.022695673629641533\n",
      "Epoch: 886. Loss: 0.022723738104104996\n",
      "Epoch: 887. Loss: 0.022747956216335297\n",
      "Epoch: 888. Loss: 0.022766435518860817\n",
      "Epoch: 889. Loss: 0.02277740091085434\n",
      "Epoch: 890. Loss: 0.022783532738685608\n",
      "Epoch: 891. Loss: 0.022777901962399483\n",
      "Epoch: 892. Loss: 0.022762995213270187\n",
      "Epoch: 893. Loss: 0.022741561755537987\n",
      "Epoch: 894. Loss: 0.022719213739037514\n",
      "Epoch: 895. Loss: 0.02269195392727852\n",
      "Epoch: 896. Loss: 0.02266600914299488\n",
      "Epoch: 897. Loss: 0.02264222502708435\n",
      "Epoch: 898. Loss: 0.02262732945382595\n",
      "Epoch: 899. Loss: 0.022618049755692482\n",
      "Epoch: 900. Loss: 0.022609680891036987\n",
      "Epoch: 901. Loss: 0.022601716220378876\n",
      "Epoch: 902. Loss: 0.02259586937725544\n",
      "Epoch: 903. Loss: 0.022590674459934235\n",
      "Epoch: 904. Loss: 0.022587910294532776\n",
      "Epoch: 905. Loss: 0.022588303312659264\n",
      "Epoch: 906. Loss: 0.022594083100557327\n",
      "Epoch: 907. Loss: 0.02260732091963291\n",
      "Epoch: 908. Loss: 0.022626357153058052\n",
      "Epoch: 909. Loss: 0.022646741941571236\n",
      "Epoch: 910. Loss: 0.022669365629553795\n",
      "Epoch: 911. Loss: 0.022690344601869583\n",
      "Epoch: 912. Loss: 0.02271396294236183\n",
      "Epoch: 913. Loss: 0.022732825949788094\n",
      "Epoch: 914. Loss: 0.02274179458618164\n",
      "Epoch: 915. Loss: 0.02274066023528576\n",
      "Epoch: 916. Loss: 0.0227329321205616\n",
      "Epoch: 917. Loss: 0.02272055298089981\n",
      "Epoch: 918. Loss: 0.022704562172293663\n",
      "Epoch: 919. Loss: 0.022684847936034203\n",
      "Epoch: 920. Loss: 0.022662531584501266\n",
      "Epoch: 921. Loss: 0.02263818494975567\n",
      "Epoch: 922. Loss: 0.02261515147984028\n",
      "Epoch: 923. Loss: 0.02259424701333046\n",
      "Epoch: 924. Loss: 0.02257947437465191\n",
      "Epoch: 925. Loss: 0.02257620170712471\n",
      "Epoch: 926. Loss: 0.02258230373263359\n",
      "Epoch: 927. Loss: 0.022597748786211014\n",
      "Epoch: 928. Loss: 0.022623280063271523\n",
      "Epoch: 929. Loss: 0.022659437730908394\n",
      "Epoch: 930. Loss: 0.022703757509589195\n",
      "Epoch: 931. Loss: 0.022754721343517303\n",
      "Epoch: 932. Loss: 0.02280000038444996\n",
      "Epoch: 933. Loss: 0.02283395826816559\n",
      "Epoch: 934. Loss: 0.0228551235049963\n",
      "Epoch: 935. Loss: 0.022862408310174942\n",
      "Epoch: 936. Loss: 0.02285519242286682\n",
      "Epoch: 937. Loss: 0.022829566150903702\n",
      "Epoch: 938. Loss: 0.02279469557106495\n",
      "Epoch: 939. Loss: 0.022748669609427452\n",
      "Epoch: 940. Loss: 0.022701293230056763\n",
      "Epoch: 941. Loss: 0.022652897983789444\n",
      "Epoch: 942. Loss: 0.022609438747167587\n",
      "Epoch: 943. Loss: 0.022574028000235558\n",
      "Epoch: 944. Loss: 0.022549515590071678\n",
      "Epoch: 945. Loss: 0.02253163978457451\n",
      "Epoch: 946. Loss: 0.022519124671816826\n",
      "Epoch: 947. Loss: 0.022512296214699745\n",
      "Epoch: 948. Loss: 0.022513827309012413\n",
      "Epoch: 949. Loss: 0.022524995729327202\n",
      "Epoch: 950. Loss: 0.022549083456397057\n",
      "Epoch: 951. Loss: 0.022585812956094742\n",
      "Epoch: 952. Loss: 0.022626906633377075\n",
      "Epoch: 953. Loss: 0.022668054327368736\n",
      "Epoch: 954. Loss: 0.02271060273051262\n",
      "Epoch: 955. Loss: 0.02275259792804718\n",
      "Epoch: 956. Loss: 0.022792141884565353\n",
      "Epoch: 957. Loss: 0.02281835302710533\n",
      "Epoch: 958. Loss: 0.022835679352283478\n",
      "Epoch: 959. Loss: 0.022836698219180107\n",
      "Epoch: 960. Loss: 0.02282264269888401\n",
      "Epoch: 961. Loss: 0.02279025875031948\n",
      "Epoch: 962. Loss: 0.022749120369553566\n",
      "Epoch: 963. Loss: 0.022707054391503334\n",
      "Epoch: 964. Loss: 0.02266499027609825\n",
      "Epoch: 965. Loss: 0.022626662626862526\n",
      "Epoch: 966. Loss: 0.022587448358535767\n",
      "Epoch: 967. Loss: 0.02255422994494438\n",
      "Epoch: 968. Loss: 0.022526513785123825\n",
      "Epoch: 969. Loss: 0.022505896165966988\n",
      "Epoch: 970. Loss: 0.022495949640870094\n",
      "Epoch: 971. Loss: 0.02249467559158802\n",
      "Epoch: 972. Loss: 0.022495754063129425\n",
      "Epoch: 973. Loss: 0.022504042834043503\n",
      "Epoch: 974. Loss: 0.0225196722894907\n",
      "Epoch: 975. Loss: 0.022544311359524727\n",
      "Epoch: 976. Loss: 0.02257770299911499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 977. Loss: 0.022616669535636902\n",
      "Epoch: 978. Loss: 0.02265785075724125\n",
      "Epoch: 979. Loss: 0.02269297093153\n",
      "Epoch: 980. Loss: 0.02272936888039112\n",
      "Epoch: 981. Loss: 0.022761695086956024\n",
      "Epoch: 982. Loss: 0.02279050275683403\n",
      "Epoch: 983. Loss: 0.022807393223047256\n",
      "Epoch: 984. Loss: 0.02280762977898121\n",
      "Epoch: 985. Loss: 0.022795386612415314\n",
      "Epoch: 986. Loss: 0.022774552926421165\n",
      "Epoch: 987. Loss: 0.022744195535779\n",
      "Epoch: 988. Loss: 0.02270529605448246\n",
      "Epoch: 989. Loss: 0.02266089804470539\n",
      "Epoch: 990. Loss: 0.022621817886829376\n",
      "Epoch: 991. Loss: 0.022593408823013306\n",
      "Epoch: 992. Loss: 0.02257687970995903\n",
      "Epoch: 993. Loss: 0.022569691762328148\n",
      "Epoch: 994. Loss: 0.022565634921193123\n",
      "Epoch: 995. Loss: 0.022563159465789795\n",
      "Epoch: 996. Loss: 0.02256385050714016\n",
      "Epoch: 997. Loss: 0.02256523072719574\n",
      "Epoch: 998. Loss: 0.02257007360458374\n",
      "Epoch: 999. Loss: 0.022583208978176117\n"
     ]
    }
   ],
   "source": [
    "loss_cnn_test = []\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 120\n",
    "cnn_logits_lst_test = []\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    #for i in tqdm(range(0, len(X_train_CNN), BATCH_SIZE)):\n",
    "    batch_X = X_test_CNN.view(-1, 1, 30,30)\n",
    "    batch_y = Y_test_CNN\n",
    "\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net(batch_X.float())[0]\n",
    "    cnn_logits_test = net(batch_X.float())[1]\n",
    "    #cnn_logits_lst.extend(cnn_logits)\n",
    "    #print(outputs)\n",
    "    loss = loss_function(outputs,  batch_y.float().reshape((-1,1)))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_cnn_test.append(loss)\n",
    "    if EPOCHS % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ad2464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_cnn_test)\n",
    "len(cnn_logits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64563d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['CNN_logits'] =cnn_logits_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3c11ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "      <td>तल्ख़ी</td>\n",
       "      <td>تَوَجُّه</td>\n",
       "      <td>təlxiː</td>\n",
       "      <td>towæd͡ʒُّh</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.262500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492522</td>\n",
       "      <td>0.449687</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>-11.186650</td>\n",
       "      <td>-20.759451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7655</td>\n",
       "      <td>उदार</td>\n",
       "      <td>سخاوتمندانه</td>\n",
       "      <td>udaːr</td>\n",
       "      <td>sxɒvtmndɒnh</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>5.102273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340180</td>\n",
       "      <td>0.758899</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>-7.928271</td>\n",
       "      <td>-9.596358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12060</td>\n",
       "      <td>हम-</td>\n",
       "      <td>هرگز</td>\n",
       "      <td>ɦəmə-</td>\n",
       "      <td>hrɡz</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333505</td>\n",
       "      <td>0.659417</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>-20.914663</td>\n",
       "      <td>-19.077866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1962</td>\n",
       "      <td>कठोर</td>\n",
       "      <td>بي تفاوت</td>\n",
       "      <td>kəʈʰor</td>\n",
       "      <td>bي tfɒvt</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.247396</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369706</td>\n",
       "      <td>0.487233</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, ...</td>\n",
       "      <td>-9.150852</td>\n",
       "      <td>-8.425693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>2127</td>\n",
       "      <td>फ़िरंगी</td>\n",
       "      <td>فرصت</td>\n",
       "      <td>firəŋɡiː</td>\n",
       "      <td>frst</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.403646</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.498239</td>\n",
       "      <td>0.715128</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-11.355590</td>\n",
       "      <td>-9.454258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>501</td>\n",
       "      <td>8432</td>\n",
       "      <td>साज़</td>\n",
       "      <td>اتصال</td>\n",
       "      <td>saːz</td>\n",
       "      <td>ɒtsɒl</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.390779</td>\n",
       "      <td>0.225417</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-10.104707</td>\n",
       "      <td>-10.157057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>4370</td>\n",
       "      <td>टुकड़ा</td>\n",
       "      <td>باطله</td>\n",
       "      <td>ʈukɽaː</td>\n",
       "      <td>bɒtlh</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386734</td>\n",
       "      <td>0.621349</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-8.592606</td>\n",
       "      <td>-9.658706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1 loan_word original_word loan_word_epitran  \\\n",
       "0             0          1246      साया          سایه            saːjaː   \n",
       "1             1           524    तल्ख़ी      تَوَجُّه            təlxiː   \n",
       "2             2          7655      उदार   سخاوتمندانه             udaːr   \n",
       "3             3         12060       हम-          هرگز             ɦəmə-   \n",
       "4             4          1962      कठोर      بي تفاوت            kəʈʰor   \n",
       "..          ...           ...       ...           ...               ...   \n",
       "498         498          1047   रफ़्तार         رفتار           rəftaːr   \n",
       "499         499           412     ज़ब्त           ضبط             zəbtə   \n",
       "500         500          2127   फ़िरंगी          فرصت          firəŋɡiː   \n",
       "501         501          8432      साज़         اتصال              saːz   \n",
       "502         502          4370    टुकड़ा         باطله            ʈukɽaː   \n",
       "\n",
       "    original_word_epitran  Fast Levenshtein  Dolgo Prime Distance  \\\n",
       "0                    sɒjh          0.666667              0.166667   \n",
       "1              towæd͡ʒُّh          0.900000              0.200000   \n",
       "2             sxɒvtmndɒnh          0.909091              0.636364   \n",
       "3                    hrɡz          1.000000              0.600000   \n",
       "4                bي tfɒvt          1.000000              0.625000   \n",
       "..                    ...               ...                   ...   \n",
       "498                 rftɒr          0.428571              0.142857   \n",
       "499                   zbt          0.400000              0.400000   \n",
       "500                  frst          0.750000              0.625000   \n",
       "501                 ɒtsɒl          1.000000              0.600000   \n",
       "502                 bɒtlh          1.000000              0.500000   \n",
       "\n",
       "     Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                 0.062500                  0.069444   \n",
       "1                 0.177083                  0.200000   \n",
       "2                 0.615530                  0.681818   \n",
       "3                 0.166667                  0.208333   \n",
       "4                 0.247396                  0.281250   \n",
       "..                     ...                       ...   \n",
       "498               0.145833                  0.160714   \n",
       "499               0.358333                  0.400000   \n",
       "500               0.403646                  0.453125   \n",
       "501               0.404167                  0.441667   \n",
       "502               0.152778                  0.180556   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "1                     2.262500                              0.900000      0   \n",
       "2                     5.102273                              0.909091      0   \n",
       "3                     3.650000                              1.000000      0   \n",
       "4                     3.250000                              1.000000      0   \n",
       "..                         ...                                   ...    ...   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "500                   3.750000                              0.750000      0   \n",
       "501                   3.700000                              1.000000      0   \n",
       "502                   2.187500                              1.000000      0   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "1                0.492522            0.449687   \n",
       "2                0.340180            0.758899   \n",
       "3                0.333505            0.659417   \n",
       "4                0.369706            0.487233   \n",
       "..                    ...                 ...   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "500              0.498239            0.715128   \n",
       "501              0.390779            0.225417   \n",
       "502              0.386734            0.621349   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "1    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "2    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "3    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4    [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "..                                                 ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "500  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "501  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "502  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "1    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...  -11.186650  -20.759451  \n",
       "2    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   -7.928271   -9.596358  \n",
       "3    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...  -20.914663  -19.077866  \n",
       "4    [-1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, ...   -9.150852   -8.425693  \n",
       "..                                                 ...         ...         ...  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "500  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -11.355590   -9.454258  \n",
       "501  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -10.104707  -10.157057  \n",
       "502  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -8.592606   -9.658706  \n",
       "\n",
       "[503 rows x 19 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247f5bc",
   "metadata": {},
   "source": [
    "# Classifiers on final train/test set: logistic reg, SVM(rbf), Random Forest etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "168af8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7590b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc21d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_withlogits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70415ff",
   "metadata": {},
   "source": [
    "# Try both a miniature train-like test set and a class-balanced test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "9bcd33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]\n",
    "\n",
    "# features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "#        'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "#        'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen', \n",
    "#         ]\n",
    "\n",
    "labels = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "0e7844c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = test['label']==0\n",
    "testneg = test.loc[idx][:140] \n",
    "testpos = test.loc[test['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "7f1e328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 20)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testneg.shape, testpos.shape\n",
    "test_balanced = pd.concat([testpos, testneg])\n",
    "test_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d958b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "dde3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()\n",
    "\n",
    "# x_test = test_balanced[features].values\n",
    "# y_test = test_balanced[labels].values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "81c9e5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4398, 10), (4398,), (503, 10), (503,))"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "51b989d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "7c191892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.5       ,   0.16666667,   0.17013889, ...,   0.56327808,\n",
       "          6.492431  ,  10.236605  ],\n",
       "       [  0.71428571,   0.42857143,   0.30059524, ...,   0.71721417,\n",
       "        -10.887565  ,  -6.9346147 ],\n",
       "       [  0.78947368,   0.52631579,   0.33881579, ...,   0.28867748,\n",
       "        -55.091427  , -48.7657    ],\n",
       "       ...,\n",
       "       [  1.        ,   0.77777778,   0.37962963, ...,   0.31559917,\n",
       "        -30.561073  , -19.542866  ],\n",
       "       [  0.83333333,   0.66666667,   0.27083333, ...,   0.68230748,\n",
       "        -23.322008  , -12.497076  ],\n",
       "       [  0.57142857,   0.28571429,   0.18154762, ...,   0.22730209,\n",
       "         16.455185  ,   8.119109  ]])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04c282",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with logits plus edit dist and cosine sims  as features: 10 features total: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "46f0299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "7d1f97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try after standardizing the data including the logits. \n",
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "547b998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "7b8c6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "4fbeee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9727891156462585\n",
      "precision :  0.9470198675496688\n",
      "recall :  1.0\n",
      "accuracy :  0.9840954274353877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       360\n",
      "           1       0.95      1.00      0.97       143\n",
      "\n",
      "    accuracy                           0.98       503\n",
      "   macro avg       0.97      0.99      0.98       503\n",
      "weighted avg       0.98      0.98      0.98       503\n",
      "\n",
      "[[352   8]\n",
      " [  0 143]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "2a1ee399",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "82e8526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9e25a",
   "metadata": {},
   "source": [
    "# for train-like test set, recall is very high with a slightly lower precision, 8 FP and no FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "c8964e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>5962</td>\n",
       "      <td>रहस्यमय</td>\n",
       "      <td>پنهان شده است</td>\n",
       "      <td>rəɦsjəməj</td>\n",
       "      <td>پnhɒn ʃdh ɒst</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.219551</td>\n",
       "      <td>0.246795</td>\n",
       "      <td>3.423077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253508</td>\n",
       "      <td>0.493680</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>-0.042522</td>\n",
       "      <td>0.281467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>12493</td>\n",
       "      <td>जन्नत</td>\n",
       "      <td>جواب</td>\n",
       "      <td>d͡ʒənnət</td>\n",
       "      <td>d͡ʒvɒb</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>2.734375</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459989</td>\n",
       "      <td>0.700303</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>0.389173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>7532</td>\n",
       "      <td>बदतरीन</td>\n",
       "      <td>بدولت</td>\n",
       "      <td>bədtəriːn</td>\n",
       "      <td>bdvlt</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.349537</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.530160</td>\n",
       "      <td>0.622041</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>-0.038698</td>\n",
       "      <td>0.163530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>4113</td>\n",
       "      <td>अशुद्ध</td>\n",
       "      <td>نجس</td>\n",
       "      <td>aʃudd̤ə</td>\n",
       "      <td>nd͡ʒs</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.516118</td>\n",
       "      <td>0.581282</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-0.068214</td>\n",
       "      <td>0.332891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>12290</td>\n",
       "      <td>क़ैंची</td>\n",
       "      <td>کامیاب</td>\n",
       "      <td>qæːɲt͡ʃiː</td>\n",
       "      <td>kɒmjɒb</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>2.236111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438396</td>\n",
       "      <td>0.638663</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-0.098507</td>\n",
       "      <td>0.356058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>1144</td>\n",
       "      <td>शबनम</td>\n",
       "      <td>شمشیر</td>\n",
       "      <td>ʃəbnəm</td>\n",
       "      <td>ʃmʃjr</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>3.541667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687149</td>\n",
       "      <td>0.806896</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-0.089616</td>\n",
       "      <td>0.381455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>11818</td>\n",
       "      <td>राज़ी</td>\n",
       "      <td>رشوه</td>\n",
       "      <td>raːziː</td>\n",
       "      <td>rʃvh</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>2.520833</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663253</td>\n",
       "      <td>0.647947</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>-0.048472</td>\n",
       "      <td>0.210873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>1817</td>\n",
       "      <td>जुमा</td>\n",
       "      <td>جستجو</td>\n",
       "      <td>d͡ʒumaː</td>\n",
       "      <td>d͡ʒstd͡ʒv</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.210648</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>3.152778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.455464</td>\n",
       "      <td>0.624441</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-0.080514</td>\n",
       "      <td>0.349072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word  original_word  \\\n",
       "40           40            40            5962   रहस्यमय  پنهان شده است   \n",
       "62           62            62           12493     जन्नत           جواب   \n",
       "129         129           129            7532    बदतरीन          بدولت   \n",
       "142         142           142            4113    अशुद्ध            نجس   \n",
       "190         190           190           12290    क़ैंची         کامیاب   \n",
       "209         209           209            1144      शबनम          شمشیر   \n",
       "326         326           326           11818     राज़ी           رشوه   \n",
       "386         386           386            1817      जुमा          جستجو   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "40          rəɦsjəməj         پnhɒn ʃdh ɒst          1.000000   \n",
       "62           d͡ʒənnət                d͡ʒvɒb          0.625000   \n",
       "129         bədtəriːn                 bdvlt          0.777778   \n",
       "142           aʃudd̤ə                 nd͡ʒs          0.857143   \n",
       "190         qæːɲt͡ʃiː                kɒmjɒb          1.000000   \n",
       "209            ʃəbnəm                 ʃmʃjr          0.833333   \n",
       "326            raːziː                  rʃvh          0.833333   \n",
       "386           d͡ʒumaː             d͡ʒstd͡ʒv          0.666667   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "40               0.461538               0.219551                  0.246795   \n",
       "62               0.500000               0.291667                  0.322917   \n",
       "129              0.555556               0.349537                  0.398148   \n",
       "142              0.714286               0.455357                  0.500000   \n",
       "190              0.333333               0.189815                  0.203704   \n",
       "209              0.833333               0.277778                  0.319444   \n",
       "326              0.500000               0.121528                  0.145833   \n",
       "386              0.444444               0.210648                  0.236111   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "40                    3.423077                              1.000000      0   \n",
       "62                    2.734375                              0.625000      0   \n",
       "129                   3.166667                              0.777778      0   \n",
       "142                   4.142857                              0.857143      0   \n",
       "190                   2.236111                              1.000000      0   \n",
       "209                   3.541667                              0.833333      0   \n",
       "326                   2.520833                              0.833333      0   \n",
       "386                   3.152778                              0.666667      0   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "40               0.253508            0.493680   \n",
       "62               0.459989            0.700303   \n",
       "129              0.530160            0.622041   \n",
       "142              0.516118            0.581282   \n",
       "190              0.438396            0.638663   \n",
       "209              0.687149            0.806896   \n",
       "326              0.663253            0.647947   \n",
       "386              0.455464            0.624441   \n",
       "\n",
       "                                         features_loan  \\\n",
       "40   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "62   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "129  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "142  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "190  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "209  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "326  [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "386  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "40   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   -0.042522    0.281467  \n",
       "62   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -0.089799    0.389173  \n",
       "129  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   -0.038698    0.163530  \n",
       "142  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   -0.068214    0.332891  \n",
       "190  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -0.098507    0.356058  \n",
       "209  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   -0.089616    0.381455  \n",
       "326  [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   -0.048472    0.210873  \n",
       "386  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   -0.080514    0.349072  "
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "48802555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Unnamed: 0.1, Unnamed: 0.1.1, loan_word, original_word, loan_word_epitran, original_word_epitran, Fast Levenshtein, Dolgo Prime Distance, Feature Edit Distance, Hamming Feature Distance, Weighted Feature Distance, Fast Levenshtein Distance Div Maxlen, label, mbert_cos_similarity, xlm_cos_similarity, features_loan, features_orig, DNN_logits, CNN_logits]\n",
       "Index: []"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "a455405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try after standardizing the data including the logits. \n",
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba95c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bea33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "9a628371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "c5924eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "9da85259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9225092250922509\n",
      "precision :  0.9765625\n",
      "recall :  0.8741258741258742\n",
      "accuracy :  0.9257950530035336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       140\n",
      "           1       0.98      0.87      0.92       143\n",
      "\n",
      "    accuracy                           0.93       283\n",
      "   macro avg       0.93      0.93      0.93       283\n",
      "weighted avg       0.93      0.93      0.93       283\n",
      "\n",
      "[[137   3]\n",
      " [ 18 125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "53dc3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_pred, y_test, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "597bf5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore: [0.92881356 0.92250923]\n",
      "precision: [0.97857143 0.87412587]\n",
      "recall: [0.88387097 0.9765625 ]\n",
      "support: [155 128]\n"
     ]
    }
   ],
   "source": [
    "print('fscore: {}'.format(fscore))\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "b4f13bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3198"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "2730ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "10b8251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "8bd54528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>11390</td>\n",
       "      <td>दुश्वार</td>\n",
       "      <td>دور</td>\n",
       "      <td>duʃvaːr</td>\n",
       "      <td>dvr</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3.107143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428112</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>-8.963047</td>\n",
       "      <td>-11.555681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>8530</td>\n",
       "      <td>जामा</td>\n",
       "      <td>جگر</td>\n",
       "      <td>d͡ʒaːmaː</td>\n",
       "      <td>d͡ʒɡr</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>0.213542</td>\n",
       "      <td>2.281250</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.612502</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-13.170180</td>\n",
       "      <td>-7.685933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>9841</td>\n",
       "      <td>ज़र्रा</td>\n",
       "      <td>زار زار</td>\n",
       "      <td>zərraː</td>\n",
       "      <td>zɒr zɒr</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.172619</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.462647</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-7.707266</td>\n",
       "      <td>-11.462407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "78           78            78           11390   दुश्वार           دور   \n",
       "94           94            94            8530      जामा           جگر   \n",
       "182         182           182            9841    ज़र्रा       زار زار   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "78            duʃvaːr                   dvr          0.571429   \n",
       "94           d͡ʒaːmaː                 d͡ʒɡr          0.625000   \n",
       "182            zərraː               zɒr zɒr          0.714286   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "78               0.428571               0.386905                  0.428571   \n",
       "94               0.375000               0.182292                  0.213542   \n",
       "182              0.285714               0.172619                  0.208333   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "78                    3.107143                              0.571429      0   \n",
       "94                    2.281250                              0.625000      0   \n",
       "182                   1.857143                              0.714286      0   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "78               0.428112            0.321477   \n",
       "94               0.630003            0.612502   \n",
       "182              0.462647            0.596593   \n",
       "\n",
       "                                         features_loan  \\\n",
       "78   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "94   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "182  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "78   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   -8.963047  -11.555681  \n",
       "94   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -13.170180   -7.685933  \n",
       "182  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   -7.707266  -11.462407  "
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "d5829784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>अंदरुनी</td>\n",
       "      <td>اندرونی</td>\n",
       "      <td>ndəruniː</td>\n",
       "      <td>ɒndrvnj</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153646</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>3.171875</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.484583</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.576007</td>\n",
       "      <td>8.895311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>604</td>\n",
       "      <td>-दान</td>\n",
       "      <td>دانستن</td>\n",
       "      <td>-daːn</td>\n",
       "      <td>dɒnstn</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293757</td>\n",
       "      <td>0.621542</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>6.620969</td>\n",
       "      <td>7.180840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>601</td>\n",
       "      <td>दहेज़</td>\n",
       "      <td>جهیز</td>\n",
       "      <td>dəɦez</td>\n",
       "      <td>d͡ʒhjz</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466845</td>\n",
       "      <td>0.620716</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>29.674936</td>\n",
       "      <td>10.942152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>526</td>\n",
       "      <td>तश्तरी</td>\n",
       "      <td>تشت</td>\n",
       "      <td>təʃtəriː</td>\n",
       "      <td>tʃt</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.440104</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.547848</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>14.743056</td>\n",
       "      <td>10.560655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>1236</td>\n",
       "      <td>सही</td>\n",
       "      <td>صحیح</td>\n",
       "      <td>səɦiː</td>\n",
       "      <td>shjh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488294</td>\n",
       "      <td>0.608805</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>8.049563</td>\n",
       "      <td>8.439472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>272</td>\n",
       "      <td>ख़ुशनवीसी</td>\n",
       "      <td>خُوشنَوِیسی</td>\n",
       "      <td>xuʃnəviːsiː</td>\n",
       "      <td>xُvʃnowejsj</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.160985</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560712</td>\n",
       "      <td>0.560799</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>7.862397</td>\n",
       "      <td>8.812596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>727</td>\n",
       "      <td>पहल</td>\n",
       "      <td>پهلو</td>\n",
       "      <td>pəɦəl</td>\n",
       "      <td>پhlv</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>4.775000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708759</td>\n",
       "      <td>0.752857</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>8.313698</td>\n",
       "      <td>9.281388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>76</td>\n",
       "      <td>आलू</td>\n",
       "      <td>آلو</td>\n",
       "      <td>aːluː</td>\n",
       "      <td>ɒlv</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625537</td>\n",
       "      <td>0.419534</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>8.036224</td>\n",
       "      <td>11.560460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>781</td>\n",
       "      <td>फ़िरंगी</td>\n",
       "      <td>فرنگی</td>\n",
       "      <td>firəŋɡiː</td>\n",
       "      <td>frŋj</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>3.078125</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574454</td>\n",
       "      <td>0.696136</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>7.528652</td>\n",
       "      <td>8.187250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>725</td>\n",
       "      <td>पहरा</td>\n",
       "      <td>پهره</td>\n",
       "      <td>pəɦraː</td>\n",
       "      <td>پhrh</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>3.395833</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700302</td>\n",
       "      <td>0.729959</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>6.872724</td>\n",
       "      <td>6.540695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>अंगूरी</td>\n",
       "      <td>انگوری</td>\n",
       "      <td>ŋɡuːriː</td>\n",
       "      <td>ɒŋvrj</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.127976</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>2.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.457744</td>\n",
       "      <td>0.781127</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.712349</td>\n",
       "      <td>8.891312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>482</td>\n",
       "      <td>तंबू</td>\n",
       "      <td>تنبو</td>\n",
       "      <td>təmbuː</td>\n",
       "      <td>tnbv</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609910</td>\n",
       "      <td>0.777764</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>9.243767</td>\n",
       "      <td>7.644005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>403</td>\n",
       "      <td>ज़ंग</td>\n",
       "      <td>زنگ</td>\n",
       "      <td>zəŋɡə</td>\n",
       "      <td>zŋ</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598645</td>\n",
       "      <td>0.770941</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>9.147930</td>\n",
       "      <td>9.274412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>423</td>\n",
       "      <td>423</td>\n",
       "      <td>158</td>\n",
       "      <td>क़बूल</td>\n",
       "      <td>قبول</td>\n",
       "      <td>qəbuːl</td>\n",
       "      <td>ɣbvl</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323891</td>\n",
       "      <td>0.571688</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>9.164741</td>\n",
       "      <td>15.450974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>445</td>\n",
       "      <td>445</td>\n",
       "      <td>176</td>\n",
       "      <td>क़िला</td>\n",
       "      <td>قلعه</td>\n",
       "      <td>qilaː</td>\n",
       "      <td>ɣlʔh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465095</td>\n",
       "      <td>0.758047</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>6.803464</td>\n",
       "      <td>8.363299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "      <td>109</td>\n",
       "      <td>इर्द-गिर्द</td>\n",
       "      <td>گرد</td>\n",
       "      <td>irdə-ɡirdə</td>\n",
       "      <td>ɡrd</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352625</td>\n",
       "      <td>0.543652</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>10.556375</td>\n",
       "      <td>19.998860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>714</td>\n",
       "      <td>परवानगी</td>\n",
       "      <td>پروانگی</td>\n",
       "      <td>pərvaːnɡiː</td>\n",
       "      <td>پrvɒŋj</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706648</td>\n",
       "      <td>0.784062</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>7.333721</td>\n",
       "      <td>9.131607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1   loan_word original_word  \\\n",
       "66           66            66               4     अंदरुनी       اندرونی   \n",
       "71           71            71             604        -दान        دانستن   \n",
       "76           76            76             601       दहेज़          جهیز   \n",
       "81           81            81             526      तश्तरी           تشت   \n",
       "92           92            92            1236         सही          صحیح   \n",
       "107         107           107             272   ख़ुशनवीसी   خُوشنَوِیسی   \n",
       "112         112           112             727         पहल          پهلو   \n",
       "136         136           136              76         आलू           آلو   \n",
       "167         167           167             781     फ़िरंगी         فرنگی   \n",
       "220         220           220             725        पहरा          پهره   \n",
       "248         248           248               1      अंगूरी        انگوری   \n",
       "290         290           290             482        तंबू          تنبو   \n",
       "370         370           370             403        ज़ंग           زنگ   \n",
       "423         423           423             158       क़बूल          قبول   \n",
       "445         445           445             176       क़िला          قلعه   \n",
       "466         466           466             109  इर्द-गिर्द           گرد   \n",
       "480         480           480             714     परवानगी       پروانگی   \n",
       "494         494           494             137        ओहदा          عهده   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "66           ndəruniː               ɒndrvnj          0.625000   \n",
       "71              -daːn                dɒnstn          0.833333   \n",
       "76              dəɦez                d͡ʒhjz          0.666667   \n",
       "81           təʃtəriː                   tʃt          0.625000   \n",
       "92              səɦiː                  shjh          0.800000   \n",
       "107       xuʃnəviːsiː           xُvʃnowejsj          0.727273   \n",
       "112             pəɦəl                  پhlv          1.000000   \n",
       "136             aːluː                   ɒlv          0.800000   \n",
       "167          firəŋɡiː                  frŋj          0.625000   \n",
       "220            pəɦraː                  پhrh          0.833333   \n",
       "248           ŋɡuːriː                 ɒŋvrj          0.857143   \n",
       "290            təmbuː                  tnbv          0.666667   \n",
       "370             zəŋɡə                    zŋ          0.600000   \n",
       "423            qəbuːl                  ɣbvl          0.666667   \n",
       "445             qilaː                  ɣlʔh          0.800000   \n",
       "466        irdə-ɡirdə                   ɡrd          0.700000   \n",
       "480        pərvaːnɡiː                پrvɒŋj          0.800000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "66               0.500000               0.153646                  0.182292   \n",
       "71               0.500000               0.479167                  0.520833   \n",
       "76               0.500000               0.197917                  0.222222   \n",
       "81               0.500000               0.440104                  0.500000   \n",
       "92               0.400000               0.108333                  0.125000   \n",
       "107              0.363636               0.160985                  0.181818   \n",
       "112              0.800000               0.462500                  0.516667   \n",
       "136              0.200000               0.095833                  0.108333   \n",
       "167              0.500000               0.351562                  0.395833   \n",
       "220              0.500000               0.347222                  0.388889   \n",
       "248              0.571429               0.127976                  0.148810   \n",
       "290              0.500000               0.225694                  0.256944   \n",
       "370              0.600000               0.537500                  0.600000   \n",
       "423              0.333333               0.229167                  0.256944   \n",
       "445              0.400000               0.191667                  0.216667   \n",
       "466              0.600000               0.533333                  0.600000   \n",
       "480              0.400000               0.304167                  0.337500   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "66                    3.171875                              0.625000      1   \n",
       "71                    3.875000                              0.833333      1   \n",
       "76                    1.937500                              0.666667      1   \n",
       "81                    3.625000                              0.625000      1   \n",
       "92                    2.650000                              0.800000      1   \n",
       "107                   2.363636                              0.727273      1   \n",
       "112                   4.775000                              1.000000      1   \n",
       "136                   2.000000                              0.800000      1   \n",
       "167                   3.078125                              0.625000      1   \n",
       "220                   3.395833                              0.833333      1   \n",
       "248                   2.928571                              0.857143      1   \n",
       "290                   2.812500                              0.666667      1   \n",
       "370                   4.350000                              0.600000      1   \n",
       "423                   2.916667                              0.666667      1   \n",
       "445                   3.700000                              0.800000      1   \n",
       "466                   4.350000                              0.700000      1   \n",
       "480                   2.825000                              0.800000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "66               0.484583            0.788032   \n",
       "71               0.293757            0.621542   \n",
       "76               0.466845            0.620716   \n",
       "81               0.590817            0.547848   \n",
       "92               0.488294            0.608805   \n",
       "107              0.560712            0.560799   \n",
       "112              0.708759            0.752857   \n",
       "136              0.625537            0.419534   \n",
       "167              0.574454            0.696136   \n",
       "220              0.700302            0.729959   \n",
       "248              0.457744            0.781127   \n",
       "290              0.609910            0.777764   \n",
       "370              0.598645            0.770941   \n",
       "423              0.323891            0.571688   \n",
       "445              0.465095            0.758047   \n",
       "466              0.352625            0.543652   \n",
       "480              0.706648            0.784062   \n",
       "494              0.249290            0.475189   \n",
       "\n",
       "                                         features_loan  \\\n",
       "66   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "71   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "76   [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "81   [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "92   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "107  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "112  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "136  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...   \n",
       "167  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "220  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "248  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "290  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "370  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "423  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "445  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "466  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "480  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "66   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.576007    8.895311  \n",
       "71   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    6.620969    7.180840  \n",
       "76   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...   29.674936   10.942152  \n",
       "81   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   14.743056   10.560655  \n",
       "92   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...    8.049563    8.439472  \n",
       "107  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    7.862397    8.812596  \n",
       "112  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...    8.313698    9.281388  \n",
       "136  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...    8.036224   11.560460  \n",
       "167  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    7.528652    8.187250  \n",
       "220  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    6.872724    6.540695  \n",
       "248  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.712349    8.891312  \n",
       "290  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...    9.243767    7.644005  \n",
       "370  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    9.147930    9.274412  \n",
       "423  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    9.164741   15.450974  \n",
       "445  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...    6.803464    8.363299  \n",
       "466  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   10.556375   19.998860  \n",
       "480  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    7.333721    9.131607  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  "
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3acf4",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with   edit dist as features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "6890585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen', \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "10a4edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "81167305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "12df41af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4398, 6), (4398,), (503, 6), (503,))"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "90158ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "5ad8211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "486a9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8873239436619719\n",
      "precision :  0.8936170212765957\n",
      "recall :  0.8811188811188811\n",
      "accuracy :  0.9363817097415507\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "f1a249e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       360\n",
      "           1       0.89      0.88      0.89       143\n",
      "\n",
      "    accuracy                           0.94       503\n",
      "   macro avg       0.92      0.92      0.92       503\n",
      "weighted avg       0.94      0.94      0.94       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249560b3",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with   edit dist and cosine sims as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "fc6c15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "03e87711",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "5a3a279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "8ed62e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4398, 8), (4398,), (503, 8), (503,))"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "f0434366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "42b07ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7d8a5ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.881118881118881\n",
      "precision :  0.8811188811188811\n",
      "recall :  0.8811188811188811\n",
      "accuracy :  0.9324055666003976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       360\n",
      "           1       0.88      0.88      0.88       143\n",
      "\n",
      "    accuracy                           0.93       503\n",
      "   macro avg       0.92      0.92      0.92       503\n",
      "weighted avg       0.93      0.93      0.93       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9dfae",
   "metadata": {},
   "source": [
    "# Try with SVM with various combinations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "932823a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "c7537f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "13a6f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "48eaa064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8842105263157896\n",
      "precision :  0.8873239436619719\n",
      "recall :  0.8811188811188811\n",
      "accuracy :  0.9343936381709742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       360\n",
      "           1       0.89      0.88      0.88       143\n",
      "\n",
      "    accuracy                           0.93       503\n",
      "   macro avg       0.92      0.92      0.92       503\n",
      "weighted avg       0.93      0.93      0.93       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204700e4",
   "metadata": {},
   "source": [
    "# with all 10 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "aa4b5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "4563c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "5f565d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "865c1327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "4bbe9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "bc13a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9727891156462585\n",
      "precision :  0.9470198675496688\n",
      "recall :  1.0\n",
      "accuracy :  0.9840954274353877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       360\n",
      "           1       0.95      1.00      0.97       143\n",
      "\n",
      "    accuracy                           0.98       503\n",
      "   macro avg       0.97      0.99      0.98       503\n",
      "weighted avg       0.98      0.98      0.98       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047c0d8",
   "metadata": {},
   "source": [
    "# with edit distances and cosine sim: 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "7fb9ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "8e2e4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "7d77ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "7fd06392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4398, 8), (4398,), (503, 8), (503,))"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "dd6e54c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "ec4ae395",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "febb241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8936170212765958\n",
      "precision :  0.9064748201438849\n",
      "recall :  0.8811188811188811\n",
      "accuracy :  0.9403578528827038\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       360\n",
      "           1       0.91      0.88      0.89       143\n",
      "\n",
      "    accuracy                           0.94       503\n",
      "   macro avg       0.93      0.92      0.93       503\n",
      "weighted avg       0.94      0.94      0.94       503\n",
      "\n",
      "[[347  13]\n",
      " [ 17 126]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87d762",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "eb95cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "ea75b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ff91c",
   "metadata": {},
   "source": [
    "# with 8 features, edits and cosine w/o logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "938bd7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4398, 8), (4398,), (503, 8), (503,))"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "1bc4dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "bcf4f5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "30858579",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "2a51ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8936170212765958\n",
      "precision :  0.9064748201438849\n",
      "recall :  0.8811188811188811\n",
      "accuracy :  0.9403578528827038\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       360\n",
      "           1       0.91      0.88      0.89       143\n",
      "\n",
      "    accuracy                           0.94       503\n",
      "   macro avg       0.93      0.92      0.93       503\n",
      "weighted avg       0.94      0.94      0.94       503\n",
      "\n",
      "[[347  13]\n",
      " [ 17 126]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486608f1",
   "metadata": {},
   "source": [
    "# 10 features, all with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "b990cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "fa60bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "553798ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "b77c24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "bd2a8e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "8ab5b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "20c5bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9750889679715302\n",
      "precision :  0.9927536231884058\n",
      "recall :  0.958041958041958\n",
      "accuracy :  0.9860834990059643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       360\n",
      "           1       0.99      0.96      0.98       143\n",
      "\n",
      "    accuracy                           0.99       503\n",
      "   macro avg       0.99      0.98      0.98       503\n",
      "weighted avg       0.99      0.99      0.99       503\n",
      "\n",
      "[[359   1]\n",
      " [  6 137]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "f7307ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "024adbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "0a43df8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 203, 363, 399, 458, 492]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "f6b3c686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>589</td>\n",
       "      <td>दर्ज़ी</td>\n",
       "      <td>درزی</td>\n",
       "      <td>dərziː</td>\n",
       "      <td>drzj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548778</td>\n",
       "      <td>0.457338</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>-0.048472</td>\n",
       "      <td>0.210873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>804</td>\n",
       "      <td>बजाय</td>\n",
       "      <td>بجای</td>\n",
       "      <td>bəd͡ʒaːj</td>\n",
       "      <td>bd͡ʒɒj</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.127604</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554636</td>\n",
       "      <td>0.697365</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>-0.038698</td>\n",
       "      <td>0.163530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>775</td>\n",
       "      <td>फ़ारसी</td>\n",
       "      <td>فارسی</td>\n",
       "      <td>faːrsiː</td>\n",
       "      <td>fɒrsj</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1</td>\n",
       "      <td>0.461557</td>\n",
       "      <td>0.759727</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-0.068214</td>\n",
       "      <td>0.332891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>1291</td>\n",
       "      <td>हकीम</td>\n",
       "      <td>حکیم</td>\n",
       "      <td>ɦəkiːm</td>\n",
       "      <td>hkjm</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.201389</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517663</td>\n",
       "      <td>0.650957</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-0.098507</td>\n",
       "      <td>0.356058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "      <td>554</td>\n",
       "      <td>तुर्की</td>\n",
       "      <td>ترکی</td>\n",
       "      <td>turkiː</td>\n",
       "      <td>trkj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700281</td>\n",
       "      <td>0.403561</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-0.089616</td>\n",
       "      <td>0.381455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>749</td>\n",
       "      <td>पैग़ाम</td>\n",
       "      <td>پیغام</td>\n",
       "      <td>pæːɣaːm</td>\n",
       "      <td>پjɣɒm</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.175595</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>1.803571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577389</td>\n",
       "      <td>0.793600</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>0.389173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "59           59            59             589    दर्ज़ी          درزی   \n",
       "203         203           203             804      बजाय          بجای   \n",
       "363         363           363             775    फ़ारसी         فارسی   \n",
       "399         399           399            1291      हकीम          حکیم   \n",
       "458         458           458             554    तुर्की          ترکی   \n",
       "492         492           492             749    पैग़ाम         پیغام   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "59             dərziː                  drzj          0.500000   \n",
       "203          bəd͡ʒaːj                bd͡ʒɒj          0.375000   \n",
       "363           faːrsiː                 fɒrsj          0.571429   \n",
       "399            ɦəkiːm                  hkjm          0.666667   \n",
       "458            turkiː                  trkj          0.500000   \n",
       "492           pæːɣaːm                 پjɣɒm          0.714286   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "59               0.333333               0.170139                  0.194444   \n",
       "203              0.125000               0.127604                  0.140625   \n",
       "363              0.142857               0.035714                  0.041667   \n",
       "399              0.333333               0.177083                  0.201389   \n",
       "458              0.333333               0.170139                  0.194444   \n",
       "492              0.285714               0.175595                  0.196429   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "59                    1.687500                              0.500000      1   \n",
       "203                   1.093750                              0.375000      1   \n",
       "363                   0.625000                              0.571429      1   \n",
       "399                   1.729167                              0.666667      1   \n",
       "458                   1.687500                              0.500000      1   \n",
       "492                   1.803571                              0.714286      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "59               0.548778            0.457338   \n",
       "203              0.554636            0.697365   \n",
       "363              0.461557            0.759727   \n",
       "399              0.517663            0.650957   \n",
       "458              0.700281            0.403561   \n",
       "492              0.577389            0.793600   \n",
       "\n",
       "                                         features_loan  \\\n",
       "59   [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "203  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "363  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "399  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "458  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "492  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "59   [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   -0.048472    0.210873  \n",
       "203  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   -0.038698    0.163530  \n",
       "363  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   -0.068214    0.332891  \n",
       "399  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -0.098507    0.356058  \n",
       "458  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   -0.089616    0.381455  \n",
       "492  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -0.089799    0.389173  "
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "e68d517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>12493</td>\n",
       "      <td>जन्नत</td>\n",
       "      <td>جواب</td>\n",
       "      <td>d͡ʒənnət</td>\n",
       "      <td>d͡ʒvɒb</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>2.734375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459989</td>\n",
       "      <td>0.700303</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>0.389173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "62          62            62           12493     जन्नत          جواب   \n",
       "\n",
       "   loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "62          d͡ʒənnət                d͡ʒvɒb             0.625   \n",
       "\n",
       "    Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "62                   0.5               0.291667                  0.322917   \n",
       "\n",
       "    Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "62                   2.734375                                 0.625      0   \n",
       "\n",
       "    mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "62              0.459989            0.700303   \n",
       "\n",
       "                                        features_loan  \\\n",
       "62  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                        features_orig  DNN_logits  CNN_logits  \n",
       "62  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -0.089799    0.389173  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "67132a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "      <td>पेचकस</td>\n",
       "      <td>پیچ کش</td>\n",
       "      <td>pet͡ʃkəs</td>\n",
       "      <td>پjt͡ʃ kʃ</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524065</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>7.977158</td>\n",
       "      <td>9.532843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>154</td>\n",
       "      <td>क़तार</td>\n",
       "      <td>قطار</td>\n",
       "      <td>qətaːr</td>\n",
       "      <td>ɣtɒr</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496417</td>\n",
       "      <td>0.828683</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>8.294688</td>\n",
       "      <td>8.501902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>702</td>\n",
       "      <td>पंजा</td>\n",
       "      <td>پنجه</td>\n",
       "      <td>pəɲd͡ʒaː</td>\n",
       "      <td>پnd͡ʒh</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566448</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>7.139163</td>\n",
       "      <td>10.420475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>410</td>\n",
       "      <td>ज़बान</td>\n",
       "      <td>زبان</td>\n",
       "      <td>zəbaːn</td>\n",
       "      <td>zbɒn</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.327706</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>6.765188</td>\n",
       "      <td>7.704823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>482</td>\n",
       "      <td>1261</td>\n",
       "      <td>सिरका</td>\n",
       "      <td>سرکه</td>\n",
       "      <td>sirkaː</td>\n",
       "      <td>srkh</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.641863</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>6.061551</td>\n",
       "      <td>6.145308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>1059</td>\n",
       "      <td>राय</td>\n",
       "      <td>رأی</td>\n",
       "      <td>raːj</td>\n",
       "      <td>rɒʔj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.678451</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.641813</td>\n",
       "      <td>9.061541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "0             0             0            1246      साया          سایه   \n",
       "5             5             5             743     पेचकस        پیچ کش   \n",
       "7             7             7             154     क़तार          قطار   \n",
       "8             8             8             702      पंजा          پنجه   \n",
       "9             9             9             410     ज़बान          زبان   \n",
       "..          ...           ...             ...       ...           ...   \n",
       "482         482           482            1261     सिरका          سرکه   \n",
       "485         485           485            1059       राय           رأی   \n",
       "494         494           494             137      ओहदा          عهده   \n",
       "498         498           498            1047   रफ़्तार         رفتار   \n",
       "499         499           499             412     ज़ब्त           ضبط   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "0              saːjaː                  sɒjh          0.666667   \n",
       "5            pet͡ʃkəs              پjt͡ʃ kʃ          0.625000   \n",
       "7              qətaːr                  ɣtɒr          0.666667   \n",
       "8            pəɲd͡ʒaː                پnd͡ʒh          0.625000   \n",
       "9              zəbaːn                  zbɒn          0.500000   \n",
       "..                ...                   ...               ...   \n",
       "482            sirkaː                  srkh          0.500000   \n",
       "485              raːj                  rɒʔj          0.500000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "498           rəftaːr                 rftɒr          0.428571   \n",
       "499             zəbtə                   zbt          0.400000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                0.166667               0.062500                  0.069444   \n",
       "5                0.375000               0.250000                  0.281250   \n",
       "7                0.166667               0.190972                  0.208333   \n",
       "8                0.375000               0.273438                  0.307292   \n",
       "9                0.166667               0.170139                  0.187500   \n",
       "..                    ...                    ...                       ...   \n",
       "482              0.333333               0.190972                  0.215278   \n",
       "485              0.250000               0.255208                  0.281250   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "498              0.142857               0.145833                  0.160714   \n",
       "499              0.400000               0.358333                  0.400000   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "5                     2.265625                              0.625000      1   \n",
       "7                     1.750000                              0.666667      1   \n",
       "8                     2.718750                              0.625000      1   \n",
       "9                     1.458333                              0.500000      1   \n",
       "..                         ...                                   ...    ...   \n",
       "482                   2.145833                              0.500000      1   \n",
       "485                   2.187500                              0.500000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "5                0.524065            0.851965   \n",
       "7                0.496417            0.828683   \n",
       "8                0.566448            0.512996   \n",
       "9                0.407726            0.327706   \n",
       "..                    ...                 ...   \n",
       "482              0.607168            0.641863   \n",
       "485              0.490000            0.678451   \n",
       "494              0.249290            0.475189   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "5    [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "7    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "..                                                 ...   \n",
       "482  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "485  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "5    [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...    7.977158    9.532843  \n",
       "7    [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    8.294688    8.501902  \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    7.139163   10.420475  \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...    6.765188    7.704823  \n",
       "..                                                 ...         ...         ...  \n",
       "482  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    6.061551    6.145308  \n",
       "485  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.641813    9.061541  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "\n",
       "[137 rows x 20 columns]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[tp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "a23a3961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "      <td>पेचकस</td>\n",
       "      <td>پیچ کش</td>\n",
       "      <td>pet͡ʃkəs</td>\n",
       "      <td>پjt͡ʃ kʃ</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524065</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>7.977158</td>\n",
       "      <td>9.532843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>154</td>\n",
       "      <td>क़तार</td>\n",
       "      <td>قطار</td>\n",
       "      <td>qətaːr</td>\n",
       "      <td>ɣtɒr</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496417</td>\n",
       "      <td>0.828683</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>8.294688</td>\n",
       "      <td>8.501902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>702</td>\n",
       "      <td>पंजा</td>\n",
       "      <td>پنجه</td>\n",
       "      <td>pəɲd͡ʒaː</td>\n",
       "      <td>پnd͡ʒh</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566448</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>7.139163</td>\n",
       "      <td>10.420475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>410</td>\n",
       "      <td>ज़बान</td>\n",
       "      <td>زبان</td>\n",
       "      <td>zəbaːn</td>\n",
       "      <td>zbɒn</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.327706</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>6.765188</td>\n",
       "      <td>7.704823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>482</td>\n",
       "      <td>1261</td>\n",
       "      <td>सिरका</td>\n",
       "      <td>سرکه</td>\n",
       "      <td>sirkaː</td>\n",
       "      <td>srkh</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.641863</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>6.061551</td>\n",
       "      <td>6.145308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>1059</td>\n",
       "      <td>राय</td>\n",
       "      <td>رأی</td>\n",
       "      <td>raːj</td>\n",
       "      <td>rɒʔj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.678451</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.641813</td>\n",
       "      <td>9.061541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "0             0             0            1246      साया          سایه   \n",
       "5             5             5             743     पेचकस        پیچ کش   \n",
       "7             7             7             154     क़तार          قطار   \n",
       "8             8             8             702      पंजा          پنجه   \n",
       "9             9             9             410     ज़बान          زبان   \n",
       "..          ...           ...             ...       ...           ...   \n",
       "482         482           482            1261     सिरका          سرکه   \n",
       "485         485           485            1059       राय           رأی   \n",
       "494         494           494             137      ओहदा          عهده   \n",
       "498         498           498            1047   रफ़्तार         رفتار   \n",
       "499         499           499             412     ज़ब्त           ضبط   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "0              saːjaː                  sɒjh          0.666667   \n",
       "5            pet͡ʃkəs              پjt͡ʃ kʃ          0.625000   \n",
       "7              qətaːr                  ɣtɒr          0.666667   \n",
       "8            pəɲd͡ʒaː                پnd͡ʒh          0.625000   \n",
       "9              zəbaːn                  zbɒn          0.500000   \n",
       "..                ...                   ...               ...   \n",
       "482            sirkaː                  srkh          0.500000   \n",
       "485              raːj                  rɒʔj          0.500000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "498           rəftaːr                 rftɒr          0.428571   \n",
       "499             zəbtə                   zbt          0.400000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                0.166667               0.062500                  0.069444   \n",
       "5                0.375000               0.250000                  0.281250   \n",
       "7                0.166667               0.190972                  0.208333   \n",
       "8                0.375000               0.273438                  0.307292   \n",
       "9                0.166667               0.170139                  0.187500   \n",
       "..                    ...                    ...                       ...   \n",
       "482              0.333333               0.190972                  0.215278   \n",
       "485              0.250000               0.255208                  0.281250   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "498              0.142857               0.145833                  0.160714   \n",
       "499              0.400000               0.358333                  0.400000   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "5                     2.265625                              0.625000      1   \n",
       "7                     1.750000                              0.666667      1   \n",
       "8                     2.718750                              0.625000      1   \n",
       "9                     1.458333                              0.500000      1   \n",
       "..                         ...                                   ...    ...   \n",
       "482                   2.145833                              0.500000      1   \n",
       "485                   2.187500                              0.500000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "5                0.524065            0.851965   \n",
       "7                0.496417            0.828683   \n",
       "8                0.566448            0.512996   \n",
       "9                0.407726            0.327706   \n",
       "..                    ...                 ...   \n",
       "482              0.607168            0.641863   \n",
       "485              0.490000            0.678451   \n",
       "494              0.249290            0.475189   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "5    [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "7    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "..                                                 ...   \n",
       "482  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "485  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "5    [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...    7.977158    9.532843  \n",
       "7    [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    8.294688    8.501902  \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    7.139163   10.420475  \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...    6.765188    7.704823  \n",
       "..                                                 ...         ...         ...  \n",
       "482  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    6.061551    6.145308  \n",
       "485  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.641813    9.061541  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "\n",
       "[137 rows x 20 columns]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[tp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d8959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
