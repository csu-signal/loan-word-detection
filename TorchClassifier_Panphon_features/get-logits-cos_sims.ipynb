{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d900aa",
   "metadata": {},
   "source": [
    "Assumes you have run `Train_Testset.ipynb` first to make the `alldata`, `realdist`, and `balanced` train/test splits for the chosen language pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a3a5e",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "091671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4977c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed2ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import io\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "726f5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer specific imports \n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,\\\n",
    "    BertForSequenceClassification, BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "78b4fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "#device = torch.device(\"cuda:0:3\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "80da282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e42f7",
   "metadata": {},
   "source": [
    "# DNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e92610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            \n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits_new = self.linear_relu_stack(x)\n",
    "        logits  = self.dropout(logits_new)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits_new\n",
    "    \n",
    "    def fit(self, X_train, Y_train, X_val, Y_val, criterion, optimizer, n_epochs=5000):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accur = []\n",
    "        val_accur = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            y_pred, logits = self(X_train.float())\n",
    "\n",
    "            train_loss = criterion(y_pred, Y_train.float())\n",
    "\n",
    "            if epoch % (n_epochs // 50) == 0:\n",
    "                train_acc,_ = self.calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "                y_val_pred = self(X_val.float())[0]\n",
    "\n",
    "                val_loss = criterion(y_val_pred, Y_val.float())\n",
    "\n",
    "                val_acc, total_corr = self.calculate_accuracy(Y_val, y_val_pred)\n",
    "\n",
    "                print(f'''epoch {epoch}\n",
    "                    Train set - loss: {self.round_tensor(train_loss)}, accuracy: {self.round_tensor(train_acc)} \n",
    "                    Val set - loss: {self.round_tensor(val_loss)}, accuracy: {self.round_tensor(val_acc)}''')\n",
    "                \n",
    "                train_losses.append(train_loss.detach().numpy())\n",
    "                val_losses.append(val_loss.detach().numpy())\n",
    "\n",
    "                val_accur.append(val_acc.detach().numpy())\n",
    "                train_accur.append(train_acc.detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        return train_losses,val_losses,train_accur,val_accur\n",
    "    \n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        predicted = y_pred.ge(.5) \n",
    "        return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())\n",
    "    \n",
    "    def round_tensor(self, t, decimal_places=3):\n",
    "        return round(t.item(), decimal_places)\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses, train_accur, val_accur):\n",
    "        epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "        plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "        plt.plot(epochs, val_accur, 'b', label='Vaidation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_losses, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36154569",
   "metadata": {},
   "source": [
    "# MyDataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8b14ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a47bf8",
   "metadata": {},
   "source": [
    "# Download LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b243a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "xlm_tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "xlm_model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55834b9",
   "metadata": {},
   "source": [
    "# Pipeline function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4051b",
   "metadata": {},
   "source": [
    "## Get Panphon phonetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4354d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_panphon_features(train_set, test_set):\n",
    "    #get phonetic features using PanPhon\n",
    "    ft = panphon.FeatureTable()   \n",
    "    \n",
    "    train_set['features_loan'] = train_set.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "    train_set['features_orig'] = train_set.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "    test_set['features_loan'] = test_set.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "    test_set['features_orig'] = test_set.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "    train_set['features_loan'] = train_set['features_loan'].apply(lambda x:sum(x, []))\n",
    "    train_set['features_orig'] = train_set['features_orig'].apply(lambda x:sum(x, []))\n",
    "    test_set['features_orig'] = test_set['features_orig'].apply(lambda x:sum(x, []))\n",
    "    test_set['features_loan'] = test_set['features_loan'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "413d0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_panphon_features(train_set, test_set, maxlen, verbose=False):\n",
    "    # Pad the phonetic features of the loan word and original word out to the maxlen \n",
    "    # of the features appearing in the training set (format: `<loan><pad 0s><orig><pad 0s>`).\n",
    "    train_set['features_loan'] = train_set['features_loan'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[0]-len(x)), 'constant'))\n",
    "    train_set['features_orig'] = train_set['features_orig'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[1]-len(x)), 'constant'))\n",
    "    test_set['features_loan'] = test_set['features_loan'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[0]-len(x)), 'constant'))\n",
    "    test_set['features_orig'] = test_set['features_orig'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[1]-len(x)), 'constant'))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Sample train features:\\n\",\\\n",
    "                train_set['features_loan'][np.random.randint(len(train_set['features_loan']))],\\\n",
    "                train_set['features_orig'][np.random.randint(len(train_set['features_loan']))])\n",
    "\n",
    "        print(\"Sample test features:\\n\",\\\n",
    "                test_set['features_loan'][np.random.randint(len(test_set['features_loan']))],\\\n",
    "                test_set['features_orig'][np.random.randint(len(test_set['features_orig']))])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a031511",
   "metadata": {},
   "source": [
    "## Add target labels and make train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "900f4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_labels(train_set, test_set):\n",
    "    Y_train = np.array([y for y in train_set['label_bin']])\n",
    "    Y_test = np.array([y for y in test_set['label_bin']])\n",
    "    return Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93f3aa",
   "metadata": {},
   "source": [
    "Make a validation split for training the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16fd11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_set(train_set, test_set, Y_train):\n",
    "    X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                np.array([x for x in train_alldata['features_orig']])])\n",
    "    X_test = np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                np.array([x for x in test_alldata['features_orig']])])\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2,\\\n",
    "                                                      random_state=1, stratify=Y_train)\n",
    "    return X_train, X_val, X_test, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee312b",
   "metadata": {},
   "source": [
    "Make tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44851fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensors(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    X_train = torch.tensor(X_train).to(device)\n",
    "    Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "    X_val = torch.tensor(X_val).to(device)\n",
    "    Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n",
    "    \n",
    "    X_test = torch.tensor(X_test).to(device)\n",
    "    Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40b267",
   "metadata": {},
   "source": [
    "## Get cosine similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683ed59",
   "metadata": {},
   "source": [
    "MBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48e84130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbert_cos_sims(l1_data,l2_data):\n",
    "    with torch.no_grad():\n",
    "        tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "        tokenizer.model_max_length = MAXTOKENS\n",
    "        l1_encodings = tokenizer(l1_data, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "        l2_encodings = tokenizer(l2_data, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "        \n",
    "        dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "        \n",
    "        base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(device)\n",
    "        base_model.eval()\n",
    "        cos_s = torch.nn.CosineSimilarity()\n",
    "        \n",
    "        sim_lst = []\n",
    "        \n",
    "        #loop through dataset \n",
    "        for step, batch in enumerate(data_loader):\n",
    "            l1_vector = base_model(batch['l1_input_ids'].to(device),\n",
    "                                          attention_mask=batch['l1_attention_mask'].to(device),\n",
    "                                          return_dict=True).last_hidden_state[:, 0, :]\n",
    "            l2_vector = base_model(batch['l2_input_ids'].to(device),\n",
    "                                          attention_mask=batch['l2_attention_mask'].to(device),\n",
    "                                          return_dict=True).last_hidden_state[:, 0, :]\n",
    "            sims = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "            sim_lst.extend(list(sims))\n",
    "            if (step * BS) % 100 < BS:\n",
    "                print(\"Got {}\".format(len(sim_lst)))\n",
    "        print()\n",
    "                \n",
    "    return sim_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643675a",
   "metadata": {},
   "source": [
    "XLM-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6ecafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xlm_cos_sims(l1_data,l2_data):\n",
    "    with torch.no_grad():\n",
    "        tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "        tokenizer.model_max_length = MAXTOKENS\n",
    "        l1_encodings = tokenizer(l1_data, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "        l2_encodings = tokenizer(l2_data, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "\n",
    "        dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "\n",
    "        data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "\n",
    "        base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(device)\n",
    "        base_model.eval()\n",
    "        cos_s = torch.nn.CosineSimilarity()\n",
    "        \n",
    "        sim_lst = []\n",
    "\n",
    "        #loop through dataset \n",
    "        for step, batch in enumerate(data_loader):\n",
    "            l1_vector = base_model(batch['l1_input_ids'].to(device), output_hidden_states=True)[0] \n",
    "            l2_vector = base_model(batch['l2_input_ids'].to(device), output_hidden_states=True)[0]\n",
    "            sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "            sim_lst.extend(list(sims))\n",
    "            if (step * BS) % 100 < BS:\n",
    "                print(\"Got {}\".format(len(sim_lst)))\n",
    "        print()\n",
    "                \n",
    "    return sim_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcffe48",
   "metadata": {},
   "source": [
    "# Load `language-pairs.json` list and run pipeline for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b372a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian-Dutch\n",
      "\n",
      "Using cpu device\n",
      "\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") \n",
      "\n",
      "epoch 0\n",
      "                    Train set - loss: 0.691, accuracy: 0.501 \n",
      "                    Val set - loss: 0.691, accuracy: 0.512\n",
      "epoch 100\n",
      "                    Train set - loss: 0.487, accuracy: 0.793 \n",
      "                    Val set - loss: 0.488, accuracy: 0.797\n",
      "epoch 200\n",
      "                    Train set - loss: 0.427, accuracy: 0.791 \n",
      "                    Val set - loss: 0.43, accuracy: 0.792\n",
      "epoch 300\n",
      "                    Train set - loss: 0.417, accuracy: 0.794 \n",
      "                    Val set - loss: 0.419, accuracy: 0.794\n",
      "epoch 400\n",
      "                    Train set - loss: 0.408, accuracy: 0.792 \n",
      "                    Val set - loss: 0.414, accuracy: 0.789\n",
      "epoch 500\n",
      "                    Train set - loss: 0.397, accuracy: 0.794 \n",
      "                    Val set - loss: 0.404, accuracy: 0.791\n",
      "epoch 600\n",
      "                    Train set - loss: 0.389, accuracy: 0.787 \n",
      "                    Val set - loss: 0.395, accuracy: 0.791\n",
      "epoch 700\n",
      "                    Train set - loss: 0.377, accuracy: 0.786 \n",
      "                    Val set - loss: 0.385, accuracy: 0.789\n",
      "epoch 800\n",
      "                    Train set - loss: 0.362, accuracy: 0.792 \n",
      "                    Val set - loss: 0.371, accuracy: 0.788\n",
      "epoch 900\n",
      "                    Train set - loss: 0.346, accuracy: 0.795 \n",
      "                    Val set - loss: 0.352, accuracy: 0.799\n",
      "epoch 1000\n",
      "                    Train set - loss: 0.328, accuracy: 0.803 \n",
      "                    Val set - loss: 0.348, accuracy: 0.792\n",
      "epoch 1100\n",
      "                    Train set - loss: 0.315, accuracy: 0.805 \n",
      "                    Val set - loss: 0.331, accuracy: 0.805\n",
      "epoch 1200\n",
      "                    Train set - loss: 0.299, accuracy: 0.812 \n",
      "                    Val set - loss: 0.313, accuracy: 0.819\n",
      "epoch 1300\n",
      "                    Train set - loss: 0.285, accuracy: 0.819 \n",
      "                    Val set - loss: 0.304, accuracy: 0.82\n",
      "epoch 1400\n",
      "                    Train set - loss: 0.271, accuracy: 0.826 \n",
      "                    Val set - loss: 0.304, accuracy: 0.811\n",
      "epoch 1500\n",
      "                    Train set - loss: 0.257, accuracy: 0.832 \n",
      "                    Val set - loss: 0.295, accuracy: 0.815\n",
      "epoch 1600\n",
      "                    Train set - loss: 0.245, accuracy: 0.838 \n",
      "                    Val set - loss: 0.292, accuracy: 0.812\n",
      "epoch 1700\n",
      "                    Train set - loss: 0.231, accuracy: 0.849 \n",
      "                    Val set - loss: 0.285, accuracy: 0.813\n",
      "epoch 1800\n",
      "                    Train set - loss: 0.222, accuracy: 0.852 \n",
      "                    Val set - loss: 0.278, accuracy: 0.816\n",
      "epoch 1900\n",
      "                    Train set - loss: 0.207, accuracy: 0.863 \n",
      "                    Val set - loss: 0.27, accuracy: 0.822\n",
      "epoch 2000\n",
      "                    Train set - loss: 0.198, accuracy: 0.867 \n",
      "                    Val set - loss: 0.264, accuracy: 0.83\n",
      "epoch 2100\n",
      "                    Train set - loss: 0.187, accuracy: 0.875 \n",
      "                    Val set - loss: 0.254, accuracy: 0.838\n",
      "epoch 2200\n",
      "                    Train set - loss: 0.177, accuracy: 0.882 \n",
      "                    Val set - loss: 0.259, accuracy: 0.831\n",
      "epoch 2300\n",
      "                    Train set - loss: 0.168, accuracy: 0.887 \n",
      "                    Val set - loss: 0.248, accuracy: 0.84\n",
      "epoch 2400\n",
      "                    Train set - loss: 0.16, accuracy: 0.89 \n",
      "                    Val set - loss: 0.247, accuracy: 0.84\n",
      "epoch 2500\n",
      "                    Train set - loss: 0.151, accuracy: 0.895 \n",
      "                    Val set - loss: 0.249, accuracy: 0.838\n",
      "epoch 2600\n",
      "                    Train set - loss: 0.145, accuracy: 0.897 \n",
      "                    Val set - loss: 0.248, accuracy: 0.835\n",
      "epoch 2700\n",
      "                    Train set - loss: 0.136, accuracy: 0.902 \n",
      "                    Val set - loss: 0.242, accuracy: 0.845\n",
      "epoch 2800\n",
      "                    Train set - loss: 0.132, accuracy: 0.901 \n",
      "                    Val set - loss: 0.24, accuracy: 0.84\n",
      "epoch 2900\n",
      "                    Train set - loss: 0.125, accuracy: 0.905 \n",
      "                    Val set - loss: 0.243, accuracy: 0.842\n",
      "epoch 3000\n",
      "                    Train set - loss: 0.123, accuracy: 0.901 \n",
      "                    Val set - loss: 0.243, accuracy: 0.845\n",
      "epoch 3100\n",
      "                    Train set - loss: 0.119, accuracy: 0.902 \n",
      "                    Val set - loss: 0.247, accuracy: 0.838\n",
      "epoch 3200\n",
      "                    Train set - loss: 0.115, accuracy: 0.905 \n",
      "                    Val set - loss: 0.252, accuracy: 0.838\n",
      "epoch 3300\n",
      "                    Train set - loss: 0.11, accuracy: 0.905 \n",
      "                    Val set - loss: 0.24, accuracy: 0.854\n",
      "epoch 3400\n",
      "                    Train set - loss: 0.106, accuracy: 0.909 \n",
      "                    Val set - loss: 0.24, accuracy: 0.856\n",
      "epoch 3500\n",
      "                    Train set - loss: 0.102, accuracy: 0.91 \n",
      "                    Val set - loss: 0.246, accuracy: 0.848\n",
      "epoch 3600\n",
      "                    Train set - loss: 0.1, accuracy: 0.91 \n",
      "                    Val set - loss: 0.25, accuracy: 0.844\n",
      "epoch 3700\n",
      "                    Train set - loss: 0.098, accuracy: 0.91 \n",
      "                    Val set - loss: 0.252, accuracy: 0.848\n",
      "epoch 3800\n",
      "                    Train set - loss: 0.096, accuracy: 0.91 \n",
      "                    Val set - loss: 0.252, accuracy: 0.844\n",
      "epoch 3900\n",
      "                    Train set - loss: 0.089, accuracy: 0.915 \n",
      "                    Val set - loss: 0.258, accuracy: 0.841\n",
      "epoch 4000\n",
      "                    Train set - loss: 0.093, accuracy: 0.907 \n",
      "                    Val set - loss: 0.253, accuracy: 0.859\n",
      "epoch 4100\n",
      "                    Train set - loss: 0.089, accuracy: 0.912 \n",
      "                    Val set - loss: 0.257, accuracy: 0.856\n",
      "epoch 4200\n",
      "                    Train set - loss: 0.089, accuracy: 0.91 \n",
      "                    Val set - loss: 0.26, accuracy: 0.852\n",
      "epoch 4300\n",
      "                    Train set - loss: 0.085, accuracy: 0.913 \n",
      "                    Val set - loss: 0.276, accuracy: 0.841\n",
      "epoch 4400\n",
      "                    Train set - loss: 0.081, accuracy: 0.917 \n",
      "                    Val set - loss: 0.264, accuracy: 0.843\n",
      "epoch 4500\n",
      "                    Train set - loss: 0.086, accuracy: 0.91 \n",
      "                    Val set - loss: 0.265, accuracy: 0.843\n",
      "epoch 4600\n",
      "                    Train set - loss: 0.084, accuracy: 0.912 \n",
      "                    Val set - loss: 0.272, accuracy: 0.845\n",
      "epoch 4700\n",
      "                    Train set - loss: 0.082, accuracy: 0.91 \n",
      "                    Val set - loss: 0.276, accuracy: 0.844\n",
      "epoch 4800\n",
      "                    Train set - loss: 0.081, accuracy: 0.913 \n",
      "                    Val set - loss: 0.271, accuracy: 0.853\n",
      "epoch 4900\n",
      "                    Train set - loss: 0.08, accuracy: 0.911 \n",
      "                    Val set - loss: 0.285, accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logits from DNN\n",
      "Getting MBERT similarities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/mt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2279: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8\n",
      "Got 112\n",
      "Got 208\n",
      "Got 312\n",
      "Got 408\n",
      "Got 512\n",
      "Got 608\n",
      "Got 712\n",
      "Got 808\n",
      "Got 912\n",
      "Got 1008\n",
      "Got 1112\n",
      "Got 1208\n",
      "Got 1312\n",
      "Got 1408\n",
      "Got 1512\n",
      "Got 1608\n",
      "Got 1712\n",
      "Got 1808\n",
      "Got 1912\n",
      "Got 2008\n",
      "Got 2112\n",
      "Got 2208\n",
      "Got 2312\n",
      "Got 2408\n",
      "Got 2512\n",
      "Got 2608\n",
      "Got 2712\n",
      "Got 2808\n",
      "Got 2912\n",
      "Got 3008\n",
      "Got 3112\n",
      "Got 3208\n",
      "Got 3312\n",
      "Got 3408\n",
      "Got 3512\n",
      "Got 3608\n",
      "Got 3712\n",
      "Got 3808\n",
      "Got 3912\n",
      "Got 4008\n",
      "Got 4112\n",
      "Got 4208\n",
      "Got 4312\n",
      "Got 4408\n",
      "Got 4512\n",
      "Got 4608\n",
      "Got 4712\n",
      "Got 4808\n",
      "Got 4912\n",
      "Got 5008\n",
      "Got 5112\n",
      "Got 5208\n",
      "Got 5312\n",
      "Got 5408\n",
      "Got 5512\n",
      "Got 5608\n",
      "Got 5712\n",
      "Got 5808\n",
      "Got 5912\n",
      "Got 6008\n",
      "Got 6112\n",
      "Got 6208\n",
      "Got 6312\n",
      "Got 6408\n",
      "Got 6512\n",
      "Got 6608\n",
      "Got 6712\n",
      "Got 6808\n",
      "Got 6912\n",
      "Got 7008\n",
      "Got 7112\n",
      "Got 7208\n",
      "Got 7312\n",
      "Got 7408\n",
      "Got 7512\n",
      "Got 7608\n",
      "Got 7712\n",
      "Got 7808\n",
      "Got 7912\n",
      "Got 8008\n",
      "Got 8112\n",
      "Got 8208\n",
      "Got 8312\n",
      "Got 8408\n",
      "Got 8512\n",
      "Got 8608\n",
      "Got 8712\n",
      "Got 8808\n",
      "Got 8912\n",
      "Got 9008\n",
      "Got 9112\n",
      "Got 9208\n",
      "Got 9312\n",
      "Got 9408\n",
      "Got 9512\n",
      "Got 9608\n",
      "Got 9712\n",
      "Got 9808\n",
      "Got 9912\n",
      "Got 10008\n",
      "Got 10112\n",
      "Got 10208\n",
      "Got 10312\n",
      "Got 10408\n",
      "Got 10512\n",
      "Got 10608\n",
      "Got 10712\n",
      "Got 10808\n",
      "Got 10912\n",
      "Got 11008\n",
      "Got 11112\n",
      "Got 11208\n",
      "Got 11312\n",
      "Got 11408\n",
      "Got 11512\n",
      "Got 11608\n",
      "Got 11712\n",
      "Got 11808\n",
      "Got 11912\n",
      "Got 12008\n",
      "Got 12112\n",
      "Got 12208\n",
      "Got 12312\n",
      "Got 12408\n",
      "Got 12512\n",
      "Got 12608\n",
      "Got 12712\n",
      "Got 12808\n",
      "Got 12912\n",
      "Got 13008\n",
      "Got 13112\n",
      "Got 13208\n",
      "Got 13312\n",
      "Got 13408\n",
      "Got 13512\n",
      "Got 13608\n",
      "Got 13712\n",
      "Got 13808\n",
      "Got 13912\n",
      "Got 14008\n",
      "Got 14112\n",
      "Got 14208\n",
      "Got 14312\n",
      "Got 14408\n",
      "Got 14512\n",
      "Got 14608\n",
      "Got 14712\n",
      "Got 14808\n",
      "Got 14912\n",
      "Got 15008\n",
      "Got 15112\n",
      "Got 15208\n",
      "Got 15312\n",
      "Got 15408\n",
      "Got 15512\n",
      "Got 15608\n",
      "Got 15712\n",
      "Got 15808\n",
      "Got 15912\n",
      "Got 16008\n",
      "Got 16112\n",
      "Got 16208\n",
      "Got 16312\n",
      "Got 16408\n",
      "Got 16512\n",
      "Got 16608\n",
      "Got 16712\n",
      "Got 16808\n",
      "Got 16912\n",
      "Got 17008\n",
      "Got 17112\n",
      "Got 17208\n",
      "Got 17312\n",
      "Got 17408\n",
      "Got 17512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/mt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2279: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8\n",
      "Got 112\n",
      "Got 208\n",
      "Got 312\n",
      "Got 408\n",
      "Got 512\n",
      "Got 608\n",
      "Got 712\n",
      "Got 808\n",
      "Got 912\n",
      "Got 1008\n",
      "Got 1112\n",
      "Got 1208\n",
      "Got 1312\n",
      "Got 1408\n",
      "Got 1512\n",
      "Got 1608\n",
      "Got 1712\n",
      "Got 1808\n",
      "Got 1912\n",
      "\n",
      "\n",
      "Getting XLM similarities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/mt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2279: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/xr/f9q8dphs5vq5mhqy843mnkv40000gn/T/ipykernel_7761/1127116239.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
      "/var/folders/xr/f9q8dphs5vq5mhqy843mnkv40000gn/T/ipykernel_7761/1127116239.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8\n",
      "Got 112\n",
      "Got 208\n",
      "Got 312\n",
      "Got 408\n",
      "Got 512\n",
      "Got 608\n",
      "Got 712\n",
      "Got 808\n",
      "Got 912\n",
      "Got 1008\n",
      "Got 1112\n",
      "Got 1208\n",
      "Got 1312\n",
      "Got 1408\n",
      "Got 1512\n",
      "Got 1608\n",
      "Got 1712\n",
      "Got 1808\n",
      "Got 1912\n",
      "Got 2008\n",
      "Got 2112\n",
      "Got 2208\n",
      "Got 2312\n",
      "Got 2408\n",
      "Got 2512\n",
      "Got 2608\n",
      "Got 2712\n",
      "Got 2808\n",
      "Got 2912\n",
      "Got 3008\n",
      "Got 3112\n",
      "Got 3208\n",
      "Got 3312\n",
      "Got 3408\n",
      "Got 3512\n",
      "Got 3608\n",
      "Got 3712\n",
      "Got 3808\n",
      "Got 3912\n",
      "Got 4008\n",
      "Got 4112\n",
      "Got 4208\n",
      "Got 4312\n",
      "Got 4408\n",
      "Got 4512\n",
      "Got 4608\n",
      "Got 4712\n",
      "Got 4808\n",
      "Got 4912\n",
      "Got 5008\n",
      "Got 5112\n",
      "Got 5208\n",
      "Got 5312\n",
      "Got 5408\n",
      "Got 5512\n",
      "Got 5608\n",
      "Got 5712\n",
      "Got 5808\n",
      "Got 5912\n",
      "Got 6008\n",
      "Got 6112\n",
      "Got 6208\n",
      "Got 6312\n",
      "Got 6408\n",
      "Got 6512\n",
      "Got 6608\n",
      "Got 6712\n",
      "Got 6808\n",
      "Got 6912\n",
      "Got 7008\n",
      "Got 7112\n",
      "Got 7208\n",
      "Got 7312\n",
      "Got 7408\n",
      "Got 7512\n",
      "Got 7608\n",
      "Got 7712\n",
      "Got 7808\n",
      "Got 7912\n",
      "Got 8008\n",
      "Got 8112\n",
      "Got 8208\n",
      "Got 8312\n",
      "Got 8408\n",
      "Got 8512\n",
      "Got 8608\n",
      "Got 8712\n",
      "Got 8808\n",
      "Got 8912\n",
      "Got 9008\n",
      "Got 9112\n",
      "Got 9208\n",
      "Got 9312\n",
      "Got 9408\n",
      "Got 9512\n",
      "Got 9608\n",
      "Got 9712\n",
      "Got 9808\n",
      "Got 9912\n",
      "Got 10008\n",
      "Got 10112\n",
      "Got 10208\n",
      "Got 10312\n",
      "Got 10408\n",
      "Got 10512\n",
      "Got 10608\n",
      "Got 10712\n",
      "Got 10808\n",
      "Got 10912\n",
      "Got 11008\n",
      "Got 11112\n",
      "Got 11208\n",
      "Got 11312\n",
      "Got 11408\n",
      "Got 11512\n",
      "Got 11608\n",
      "Got 11712\n",
      "Got 11808\n",
      "Got 11912\n",
      "Got 12008\n",
      "Got 12112\n",
      "Got 12208\n",
      "Got 12312\n",
      "Got 12408\n",
      "Got 12512\n",
      "Got 12608\n",
      "Got 12712\n",
      "Got 12808\n",
      "Got 12912\n",
      "Got 13008\n",
      "Got 13112\n",
      "Got 13208\n",
      "Got 13312\n",
      "Got 13408\n",
      "Got 13512\n",
      "Got 13608\n",
      "Got 13712\n",
      "Got 13808\n",
      "Got 13912\n",
      "Got 14008\n",
      "Got 14112\n",
      "Got 14208\n",
      "Got 14312\n",
      "Got 14408\n",
      "Got 14512\n",
      "Got 14608\n",
      "Got 14712\n",
      "Got 14808\n",
      "Got 14912\n",
      "Got 15008\n",
      "Got 15112\n",
      "Got 15208\n",
      "Got 15312\n",
      "Got 15408\n",
      "Got 15512\n",
      "Got 15608\n",
      "Got 15712\n",
      "Got 15808\n",
      "Got 15912\n",
      "Got 16008\n",
      "Got 16112\n",
      "Got 16208\n",
      "Got 16312\n",
      "Got 16408\n",
      "Got 16512\n",
      "Got 16608\n",
      "Got 16712\n",
      "Got 16808\n",
      "Got 16912\n",
      "Got 17008\n",
      "Got 17112\n",
      "Got 17208\n",
      "Got 17312\n",
      "Got 17408\n",
      "Got 17512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/mt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2279: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/xr/f9q8dphs5vq5mhqy843mnkv40000gn/T/ipykernel_7761/1127116239.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
      "/var/folders/xr/f9q8dphs5vq5mhqy843mnkv40000gn/T/ipykernel_7761/1127116239.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8\n",
      "Got 112\n",
      "Got 208\n",
      "Got 312\n",
      "Got 408\n",
      "Got 512\n",
      "Got 608\n",
      "Got 712\n",
      "Got 808\n",
      "Got 912\n",
      "Got 1008\n",
      "Got 1112\n",
      "Got 1208\n",
      "Got 1312\n",
      "Got 1408\n",
      "Got 1512\n",
      "Got 1608\n",
      "Got 1712\n",
      "Got 1808\n",
      "Got 1912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = None\n",
    "\n",
    "with open('../language-pairs.json', 'r') as f:\n",
    "    pairs = json.loads(f.read())\n",
    "\n",
    "for pair in pairs:\n",
    "    print(pair)\n",
    "    L1 = pairs[pair]['target']['name']\n",
    "    L2 = pairs[pair]['source']['name']\n",
    "    \n",
    "    # load datasets\n",
    "    prefix = f'../Datasets/production_train_test/{L1}-{L2}'\n",
    "    \n",
    "    train_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')\n",
    "\n",
    "    # get and pad PanPhon features for alldata split\n",
    "    get_panphon_features(train_alldata, test_alldata)\n",
    "    train_alldata_maxlen = (np.max(train_alldata['features_loan'].str.len()),\\\n",
    "                               np.max(train_alldata['features_orig'].str.len()))\n",
    "    pad_panphon_features(train_alldata, test_alldata, train_alldata_maxlen)\n",
    "\n",
    "    # add target labels\n",
    "    Y_train, Y_test = add_target_labels(train_alldata, test_alldata)\n",
    "\n",
    "    # make train and val splits\n",
    "    X_train, X_val, X_test, Y_train, Y_val = make_train_val_set(train_alldata, test_alldata, Y_train)\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = make_tensors(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "\n",
    "    # instantiate network\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nUsing {device} device\\n\")\n",
    "    \n",
    "    # set random seeds for reproducibility\n",
    "    np.random.seed(666)\n",
    "\n",
    "    model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "    print(model,\"\\n\")\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # train and plot losses, accuracy\n",
    "    train_losses, val_losses, train_accur, val_accur = \\\n",
    "        model.fit(X_train, Y_train, X_val, Y_val, criterion, optimizer)\n",
    "    model.plot_losses(train_losses,val_losses,train_accur,val_accur)\n",
    "\n",
    "    # get and pad PanPhon features for realdist and balanced splits\n",
    "    get_panphon_features(train_realdist,test_realdist)\n",
    "    pad_panphon_features(train_realdist,test_realdist,train_alldata_maxlen)\n",
    "\n",
    "    get_panphon_features(train_balanced,test_balanced)\n",
    "    pad_panphon_features(train_balanced,test_balanced,train_alldata_maxlen)\n",
    "\n",
    "    # create data to get logits for\n",
    "    X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                         np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "    X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                        np.array([x for x in test_alldata['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                         np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "    X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                        np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                         np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "    X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                        np.array([x for x in test_balanced['features_orig']])])).to(device)\n",
    "\n",
    "    # place model in eval mode and get logits from DNN for all datasets/splits\n",
    "    print(\"Getting logits from DNN\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_logits_dnn_alldata = model(X_train_alldata.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_alldata = model(X_test_alldata.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_realdist = model(X_train_realdist.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_realdist = model(X_test_realdist.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_balanced = model(X_train_balanced.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_balanced = model(X_test_balanced.float())[1].detach().cpu().numpy()\n",
    "\n",
    "    # remove PanPhon features from dataframe and add logits column\n",
    "    train_alldata = train_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_alldata['DNN_logits'] = train_logits_dnn_alldata\n",
    "\n",
    "    test_alldata = test_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_alldata['DNN_logits'] = test_logits_dnn_alldata\n",
    "\n",
    "    train_realdist = train_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_realdist['DNN_logits'] = train_logits_dnn_realdist\n",
    "\n",
    "    test_realdist = test_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_realdist['DNN_logits'] = test_logits_dnn_realdist\n",
    "\n",
    "    train_balanced = train_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_balanced['DNN_logits'] = train_logits_dnn_balanced\n",
    "\n",
    "    test_balanced = test_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_balanced['DNN_logits'] = test_logits_dnn_balanced\n",
    "\n",
    "    #set the seeds for reproducibility even though we are not fine-tuning or training and the weights \n",
    "    #for both these models are effectively frozen for our purpose \n",
    "    torch.manual_seed(7)\n",
    "    random.seed(7)\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # Setting PyTorch's required configuration variables for reproducibility.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "    PRE_TRAINED_bert_MODEL = 'bert-base-multilingual-cased'\n",
    "    PRE_TRAINED_xlm_MODEL = 'xlm-mlm-100-1280'\n",
    "\n",
    "    MAXTOKENS = 5\n",
    "    BS = 8  # batch size\n",
    "\n",
    "    #list of loan-original words for train sets\n",
    "    l1_train_alldata = list(train_alldata[\"loan_word\"])\n",
    "    l2_train_alldata = list(train_alldata[\"original_word\"])\n",
    "\n",
    "    l1_train_realdist = list(train_realdist[\"loan_word\"])\n",
    "    l2_train_realdist = list(train_realdist[\"original_word\"])\n",
    "\n",
    "    l1_train_balanced = list(train_balanced[\"loan_word\"])\n",
    "    l2_train_balanced = list(train_balanced[\"original_word\"])\n",
    "\n",
    "    #list of loan-original words for test sets\n",
    "    l1_test_alldata = list(test_alldata[\"loan_word\"])\n",
    "    l2_test_alldata = list(test_alldata[\"original_word\"])\n",
    "\n",
    "    l1_test_realdist = list(test_realdist[\"loan_word\"])\n",
    "    l2_test_realdist = list(test_realdist[\"original_word\"])\n",
    "\n",
    "    l1_test_balanced = list(test_balanced[\"loan_word\"])\n",
    "    l2_test_balanced = list(test_balanced[\"original_word\"])\n",
    "\n",
    "    print(\"Getting MBERT similarities\")\n",
    "    train_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['MBERT_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    train_balanced['MBERT_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    test_realdist['MBERT_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    test_balanced['MBERT_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    print()\n",
    "    print(\"Getting XLM similarities\")\n",
    "    train_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['XLM_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    train_balanced['XLM_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "\n",
    "    test_realdist['XLM_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    test_balanced['XLM_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "        \n",
    "    train_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e0293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
