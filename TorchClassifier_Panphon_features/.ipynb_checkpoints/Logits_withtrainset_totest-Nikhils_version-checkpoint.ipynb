{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d900aa",
   "metadata": {},
   "source": [
    "Assumes you have run `Train_Testset.ipynb` first to make the `alldata`, `realdist`, and `balanced` train/test splits for the chosen language pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a3a5e",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4977c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import io\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b4fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "#device = torch.device(\"cuda:0:3\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963de5f8",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata = pd.read_csv('../Datasets/train_final_production_alldata.csv')\n",
    "test_alldata = pd.read_csv('../Datasets/test_final_production_alldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b808ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist = pd.read_csv('../Datasets/train_final_production_realdist.csv')\n",
    "test_realdist = pd.read_csv('../Datasets/test_final_production_realdist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced = pd.read_csv('../Datasets/train_final_production_balanced.csv')\n",
    "test_balanced = pd.read_csv('../Datasets/test_final_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4051b",
   "metadata": {},
   "source": [
    "## Get Panphon phonetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42985f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get phonetic features using panPhon\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e887590",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata['features_loan'] = train_alldata.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_alldata['features_orig'] = train_alldata.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_alldata['features_loan'] = test_alldata.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_alldata['features_orig'] = test_alldata.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_alldata['features_loan'] = train_alldata['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_alldata['features_orig'] = train_alldata['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x:sum(x, []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384941ff",
   "metadata": {},
   "source": [
    "Pad the phonetic features of the loan word and original word out to the maxlen of the features appearing in the training set (format: `<loan><pad 0s><orig><pad 0s>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24078d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_maxlen = (np.max(train_alldata['features_loan'].str.len()),\\\n",
    "                               np.max(train_alldata['features_orig'].str.len()))\n",
    "\n",
    "train_alldata['features_loan'] = train_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_alldata['features_orig'] = train_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "train_alldata['features_loan'][np.random.randint(len(train_alldata['features_loan']))],\\\n",
    "train_alldata['features_orig'][np.random.randint(len(train_alldata['features_loan']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf93371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_alldata['features_loan'][np.random.randint(len(test_alldata['features_loan']))],\\\n",
    "test_alldata['features_orig'][np.random.randint(len(test_alldata['features_orig']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a031511",
   "metadata": {},
   "source": [
    "## Add target labels and make train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array([y for y in train_alldata['label_bin']])\n",
    "Y_test = np.array([y for y in test_alldata['label_bin']])\n",
    "Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f08e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])\n",
    "X_test = np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93f3aa",
   "metadata": {},
   "source": [
    "Make a validation split for training the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44851fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and validation splits keeping the composition of labels balanced between them using a random state '1 '\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "X_test = torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "\n",
    "X_val = torch.tensor(X_val).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e42f7",
   "metadata": {},
   "source": [
    "## DNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            \n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        #logits = self.linear_relu_stack(x)\n",
    "        logits_new = self.linear_relu_stack(x)\n",
    "        logits  = self.dropout(logits_new)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits_new\n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "#model = NeuralNetwork(X_test.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5) \n",
    "    return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fa7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad1111",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ce499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 5000 epochs and get the logits \n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_accur = []\n",
    "train_accur = []\n",
    "logits = []\n",
    "for epoch in range(5000):\n",
    "\n",
    "    y_pred = model(X_train.float())[0]\n",
    "    logits = model(X_train.float())[1]\n",
    "    #getting logits for test set \n",
    "#     y_pred = model(X_test.float())[0]\n",
    "#     logits = model(X_test.float())[1]\n",
    "    #y_pred = model(X_train) \n",
    "    #print(y_pred)\n",
    "\n",
    "    #y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, Y_train.float())\n",
    "    \n",
    "    #test_loss = criterion(y_pred, Y_test.float())\n",
    "    #train_loss = criterion(y_pred, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        train_acc,_ = calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "        y_val_pred = model(X_val.float())[0]\n",
    "        #y_test_pred = torch.squeeze(y_test_pred)\n",
    "         \n",
    "\n",
    "        val_loss = criterion(y_val_pred, Y_val.float())\n",
    "\n",
    "        val_acc, total_corr = calculate_accuracy(Y_val, y_val_pred)\n",
    "        #print(total_corr)\n",
    "        \n",
    "        print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)} Val set - loss: {round_tensor(val_loss)}, Val accuracy: {round_tensor(val_acc)}\n",
    "''')\n",
    "        #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "        #print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "        train_losses.append(train_loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        \n",
    "        val_accur.append(val_acc.detach().numpy())\n",
    "        train_accur.append(train_acc.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss.backward()\n",
    "    #test_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df878f6a",
   "metadata": {},
   "source": [
    "Plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb71617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_accur, 'b', label='vaidation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a602a53",
   "metadata": {},
   "source": [
    "## Setup evaluation datasets\n",
    "\n",
    "Get Panphon features and pad (use `train_alldata` as already defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist['features_loan'] = train_realdist.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_realdist['features_orig'] = train_realdist.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_realdist['features_loan'] = test_realdist.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_realdist['features_orig'] = test_realdist.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x:sum(x, []))\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_balanced['features_orig'] = train_balanced.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_balanced['features_loan'] = test_balanced.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_balanced['features_orig'] = test_balanced.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x:sum(x, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d768958",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                     np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                    np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                     np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                    np.array([x for x in test_balanced['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadd528",
   "metadata": {},
   "source": [
    "Get logits from DNN for all datasets/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_logits_dnn_alldata = model(X_train_alldata.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_alldata = model(X_test_alldata.float())[1].detach().cpu().numpy()\n",
    "    train_logits_dnn_realdist = model(X_train_realdist.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_realdist = model(X_test_realdist.float())[1].detach().cpu().numpy()\n",
    "    train_logits_dnn_balanced = model(X_train_balanced.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_balanced = model(X_test_balanced.float())[1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f44c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits_dnn_alldata, test_logits_dnn_alldata,\\\n",
    "train_logits_dnn_realdist, test_logits_dnn_realdist,\\\n",
    "train_logits_dnn_balanced, test_logits_dnn_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b347",
   "metadata": {},
   "source": [
    "Add DNN logit column to production datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits = pd.read_csv('../Datasets/train_final_production_alldata.csv')\n",
    "test_alldata_dnnlogits = pd.read_csv('../Datasets/test_final_production_alldata.csv')\n",
    "train_realdist_dnnlogits = pd.read_csv('../Datasets/train_final_production_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../Datasets/test_final_production_realdist.csv')\n",
    "train_balanced_dnnlogits = pd.read_csv('../Datasets/train_final_production_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../Datasets/test_final_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_alldata\n",
    "test_alldata_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_alldata\n",
    "\n",
    "train_realdist_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_realdist\n",
    "test_realdist_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_realdist\n",
    "\n",
    "train_balanced_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_balanced\n",
    "test_balanced_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_balanced\n",
    "\n",
    "train_alldata_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_alldata.csv')\n",
    "test_alldata_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_alldata.csv')\n",
    "\n",
    "train_realdist_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_realdist.csv')\n",
    "\n",
    "train_balanced_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_balanced.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df5737",
   "metadata": {},
   "source": [
    "## Setup data for CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba8383",
   "metadata": {},
   "source": [
    "Current CNN approach doesn't really work.  Because the inputs are PanPhon features with padding dependent on the maxlen of the input data, and then reshaped into a square to fit into a 2D CNN, the filter is not able to capture the necessary dependency: that is, the relationship between an L1 feature and an L2 feature at the same approximate position in their respective words (cf., Persian /ɣ/ usually becomes Hindi /q/).  If we reshape the data this way for the network, because the ordering of PanPhon features are effectively conventionlized into a fixed order, a /q/ in an (e.g.) Hindi word is not guaranteed to fall in the same window as the equivalent /ɣ/ in the (e.g.) Persian source word, thus losing the dependency.  Therefore the CNN usually falls into a local minimum of predicting everything to be a non-loan word.  Loss remains relatively compared to the DNN and accuracy plateaus at about 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ad51e",
   "metadata": {},
   "source": [
    "Use `train_alldata` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df564fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])\n",
    "Y_train = np.array([y for y in train_alldata['label_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation splits for proper model training while keeping the composition of labels balanced between them using a random state '1 '\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to torch tensors for padding\n",
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "X_test = torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "\n",
    "X_val = torch.tensor(X_val).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_perfect_square = X_train.shape[1]\n",
    "while (True):\n",
    "    if np.sqrt(closest_perfect_square) - np.floor(np.sqrt(closest_perfect_square)) != 0:\n",
    "        closest_perfect_square += 1\n",
    "    else:\n",
    "        break\n",
    "view_shape = int(np.sqrt(closest_perfect_square))\n",
    "closest_perfect_square,view_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = F.pad(X_train, pad=(0, closest_perfect_square-X_train.shape[1]), value=0)\n",
    "X_val = F.pad(X_val, pad=(0, closest_perfect_square-X_val.shape[1]), value=0)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b700",
   "metadata": {},
   "source": [
    "## CNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 128, 8) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(128, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 32, 2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        #x = torch.randn(33,33).view(-1,1,33,33) #33 because its the square root of 1089\n",
    "        #x = torch.randn(30,30).view(-1,1,30,30) #30 because its the square root of 900 for real dist train set\n",
    "        #x = torch.randn(29,29).view(-1,1,29,29) #29 because its the square root of 841 for balanced train set\n",
    "        x = torch.randn(view_shape,view_shape).view(-1,1,view_shape,view_shape) # for trained model logit prediction, for all data, 1089 is sq of 33\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(torch.tanh(self.conv1(x)), (2, 2))\n",
    "        #x = F.max_pool2d(torch.tanh(self.conv2(x)), (1, 1))\n",
    "        #x = F.max_pool2d(torch.tanh(self.conv3(x)), (1, 1))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        \n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = self.dropout(x)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "CNN_Net = CCN_Net() \n",
    "CNN_Net = nn.DataParallel(CNN_Net)\n",
    "CNN_Net.to(device)\n",
    "print(CNN_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).view(-1,view_shape,view_shape).to(device)\n",
    "X_val = torch.tensor(X_val).view(-1,view_shape,view_shape).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device)\n",
    "X_train.shape,X_val.shape,Y_train.shape,Y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(CNN_Net.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(CNN_Net.parameters(),lr=0.001, momentum=0.0,  weight_decay=0.0, nesterov=False)\n",
    "#optimizer = torch.optim.RMSprop(CNN_Net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#loss_function = nn.MSELoss()\n",
    "scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.unsqueeze(1) #just do it once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the seed manually to 42\n",
    "torch.manual_seed(42)\n",
    "# a = X_val_CNN[torch.randint(len(X_val_CNN), (120,))]  \n",
    "# # b = Y_val_CNN[torch.randint(len(X_val_CNN), (120,))]  \n",
    "# # a.shape\n",
    "# b\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea985a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train for 10000 epochs and get the logits \n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_accur = []\n",
    "train_accur = []\n",
    "train_losses_batch = []\n",
    "logits = []\n",
    "BATCH_SIZE = 512\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for i in tqdm(range(0, len(X_train), BATCH_SIZE)):\n",
    "        batch_X = X_train[i:i+BATCH_SIZE].view(-1, 1, view_shape,view_shape)  \n",
    "        #batch_X = X_train_CNN.view(-1, 1, 29,29)  \n",
    "        batch_y = Y_train[i:i+BATCH_SIZE]\n",
    "        \n",
    "        \n",
    "\n",
    "        #X_train_CNN = X_train_CNN.view(-1, 1, 33,33) # for balanced train set\n",
    "        CNN_Net.zero_grad()\n",
    "        \n",
    "        y_pred = CNN_Net(batch_X .float())[0]\n",
    "        #print(y_pred)\n",
    "        logits = CNN_Net(batch_X.float())[1]\n",
    "        #getting logits for test set \n",
    "    #     y_pred = model(X_test.float())[0]\n",
    "    #     logits = model(X_test.float())[1]\n",
    "        #y_pred = model(X_train) \n",
    "        #print(y_pred)\n",
    "\n",
    "        #y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = loss_function(y_pred, batch_y.float())\n",
    "\n",
    "        #test_loss = criterion(y_pred, Y_test.float())\n",
    "        #train_loss = criterion(y_pred, Y_train)\n",
    "        train_losses.append(train_loss)\n",
    "        if epoch % (n_epochs // 20) == 0:\n",
    "            with torch.no_grad():\n",
    "                CNN_Net.eval()\n",
    "\n",
    "                val_batch_X = X_val[torch.randint(len(X_val), (BATCH_SIZE,))] \n",
    "                val_batch_Y = Y_val[torch.randint(len(X_val), (BATCH_SIZE,))] \n",
    "\n",
    "                train_acc,_ = calculate_accuracy(batch_y, y_pred)\n",
    "                #X_val_CNN= X_val_CNN.unsqueeze(1) don't do it here, it will keep adding a channel dimension every time the for loop operates\n",
    "                y_val_pred = CNN_Net(val_batch_X.float())[0]\n",
    "                #print(y_val_pred)\n",
    "                #y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "\n",
    "                val_loss = loss_function(y_val_pred, val_batch_Y.float())\n",
    "\n",
    "                val_acc, total_corr = calculate_accuracy(val_batch_Y, y_val_pred)\n",
    "                #print(total_corr)\n",
    "\n",
    "                print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)} Val  set - loss: {round_tensor(val_loss)}, Val accuracy: {round_tensor(val_acc)}\n",
    "        ''')\n",
    "                #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "                #print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "                train_losses_batch.append(train_loss.detach().numpy())\n",
    "                val_losses.append(val_loss.detach().numpy())\n",
    "\n",
    "                train_accur.append(train_acc.detach().numpy())\n",
    "                val_accur.append(val_acc.detach().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.backward()\n",
    "        #test_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    scheduler1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10738be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_accur, 'b', label='vaidation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses_batch, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda54c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])).to(device)\n",
    "\n",
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                     np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                    np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                     np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                    np.array([x for x in test_balanced['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae325ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alldata = F.pad(X_train_alldata, pad=(0, closest_perfect_square-X_train_alldata.shape[1]), value=0)\n",
    "X_test_alldata = F.pad(X_test_alldata, pad=(0, closest_perfect_square-X_test_alldata.shape[1]), value=0)\n",
    "X_train_realdist = F.pad(X_train_realdist, pad=(0, closest_perfect_square-X_train_realdist.shape[1]), value=0)\n",
    "X_test_realdist = F.pad(X_test_realdist, pad=(0, closest_perfect_square-X_test_realdist.shape[1]), value=0)\n",
    "X_train_balanced = F.pad(X_train_balanced, pad=(0, closest_perfect_square-X_train_balanced.shape[1]), value=0)\n",
    "X_test_balanced = F.pad(X_test_balanced, pad=(0, closest_perfect_square-X_test_balanced.shape[1]), value=0)\n",
    "X_train_alldata.shape,X_test_alldata.shape,X_train_realdist.shape,X_test_realdist.shape,X_train_balanced.shape,X_test_balanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alldata = torch.tensor(X_train_alldata).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_alldata = torch.tensor(X_test_alldata).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_realdist = torch.tensor(X_train_realdist).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_realdist = torch.tensor(X_test_realdist).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_balanced = torch.tensor(X_train_balanced).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_balanced = torch.tensor(X_test_balanced).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_alldata.shape,X_test_alldata.shape,X_train_realdist.shape,X_test_realdist.shape,X_train_balanced.shape,X_test_balanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained CNN model to get logits for the three different splits\n",
    "\n",
    "CNN_Net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_logits_cnn_alldata = CNN_Net(X_train_alldata.float())[1]\n",
    "    test_logits_cnn_alldata = CNN_Net(X_test_alldata.float())[1]\n",
    "    \n",
    "    train_logits_cnn_realdist = CNN_Net(X_train_realdist.float())[1]\n",
    "    test_logits_cnn_realdist = CNN_Net(X_test_realdist.float())[1]\n",
    "    \n",
    "    train_logits_cnn_balanced = CNN_Net(X_train_balanced.float())[1]\n",
    "    test_logits_cnn_balanced = CNN_Net(X_test_balanced.float())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3159efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CNN_Net(X_train_alldata.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits_cnn_alldata,\\\n",
    "test_logits_cnn_alldata,\\\n",
    "train_logits_cnn_realdist,\\\n",
    "test_logits_cnn_realdist,\\\n",
    "train_logits_cnn_balanced,\\\n",
    "test_logits_cnn_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb31ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_alldata.csv')\n",
    " \n",
    "test_alldata_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_alldata.csv')\n",
    "train_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_realdist.csv')\n",
    "train_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868c2c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11857, 18), (1320, 18), (4030, 18), (450, 18), (2417, 18), (271, 18))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_alldata_dnnlogits.shape, test_alldata_dnnlogits.shape, train_realdist_dnnlogits.shape, test_realdist_dnnlogits.shape,train_balanced_dnnlogits.shape,test_balanced_dnnlogits.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d9f8c",
   "metadata": {},
   "source": [
    "# this part includes steps to get the cosine similarities from the two multi-lingual transformer model we are using : M-bert multilingual cased and XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abce7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer specific imports \n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\\\n",
    "    , BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c8d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "xlm_tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "xlm_model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb1d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seeds for reproducibility even though we are not fine-tuning or training and the weights \n",
    "#for both these models are effectively frozen for our purpose \n",
    "\n",
    "torch.manual_seed(7)\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "# Setting PyTorch's required configuration variables for reproducibility.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724d8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_bert_MODEL = 'bert-base-multilingual-cased'\n",
    "PRE_TRAINED_xlm_MODEL = 'xlm-mlm-100-1280'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d5f779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 5\n",
    "NUM_EPOCHS = 2000  # default maximum number of epochs\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "BS = 8  # batch size\n",
    "INITIAL_LR = 1e-5  # initial learning rate\n",
    "save_epochs = [1, 2, 3, 4, 5, 6, 7]  # these are the epoch numbers (starting from 1) to test the model on the test set\n",
    "# and save the model checkpoint.\n",
    "EARLY_STOP_PATIENCE = 30  # If model does not improve for this number of epochs, training stops.\n",
    "\n",
    "# Setting GPU cards to use for training the model. Make sure you read our paper to figure out if you have enough GPU\n",
    "# memory. If not, you can change all of them to 'cpu' to use CPU instead of GPU. By the way, two 24 GB GPU cards are\n",
    "# enough for current configuration, but in case of developing based on this you may need more (that's why there are\n",
    "# three cards declared here)\n",
    "# CUDA_0 = 'cuda:1'\n",
    "# CUDA_1 = 'cuda:1'\n",
    "# CUDA_2 = 'cuda:1'\n",
    "args = sys.argv\n",
    "epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438ae647",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 512\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "#CUDA_0 = 'cuda:1'\n",
    "#CUDA_1 = 'cuda:1'\n",
    "#CUDA_2 = 'cuda:1'\n",
    "CUDA_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#CUDA_1 = 'cuda:0'\n",
    "#CUDA_2 = 'cuda:0'\n",
    "\n",
    "# The function for printing in both console and a given log file.\n",
    "def myprint(mystr, logfile):\n",
    "    print(mystr)\n",
    "    print(mystr, file=logfile)\n",
    "\n",
    "\n",
    "# The function for loading datasets from parallel tsv files and returning texts in lists.\n",
    "def load_data(file_name):\n",
    "    try:\n",
    "        # f = open(file_name)\n",
    "        f = pd.read_csv(file_name, sep='\\t', names=['l1_text', 'l2_text'])#, 'extra'])\n",
    "    except:\n",
    "        print('my log: could not read file')\n",
    "        exit()\n",
    "    print(\"This many number of rows were removed from \" + file_name.split(\"/\")[-1] + \" due to having missing values: \",\n",
    "          f.shape[0] - f.dropna().shape[0])\n",
    "    f.dropna(inplace=True)\n",
    "    l1_texts = f['l1_text'].values.tolist()\n",
    "    l2_texts = f['l2_text'].values.tolist()\n",
    "    print(len(l1_texts), len(l2_texts))\n",
    "    print(l1_texts[500])\n",
    "    print(\"\\n\")\n",
    "    print(l2_texts[500])\n",
    "    return l1_texts, l2_texts\n",
    "\n",
    "\n",
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyDataset1(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    # Each component other than the Transformer, are in a sequential layer (it is not required obviously, but it is\n",
    "    # possible to stack them with other layers if desired)\n",
    "    def __init__(self, base_model, n_classes, dropout=0.05):\n",
    "        super().__init__()\n",
    "        # self.base_model = base_model.to(CUDA_0)\n",
    "        self.transformation_learner = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU()\n",
    "        ).to(CUDA_0)\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        l1_pooler_output = input\n",
    "        # l2 = input2\n",
    "        # if 'l1_attention_mask' in kwargs:\n",
    "        #     l1_attention_mask = kwargs['l1_attention_mask']\n",
    "            # l2_attention_mask = kwargs['l2_attention_mask']\n",
    "        # else:\n",
    "        #     print(\"my err: attention mask is not set, error maybe\")\n",
    "        # here we use only the CLS token\n",
    "        # l1_pooler_output = self.base_model(l1.to(CUDA_0), attention_mask=l1_attention_mask.to(CUDA_0)).pooler_output\n",
    "        myoutput = self.transformation_learner(l1_pooler_output)\n",
    "        return myoutput\n",
    "\n",
    "\n",
    "# The function to compute and print the performance measure scores using sklearn implementations.\n",
    "def evaluate_model(labels, predictions, titlestr, logfile):\n",
    "    myprint(titlestr, logfile)\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    myprint(\"Confusion matrix- \\n\" + str(conf_matrix), logfile)\n",
    "    acc_score = accuracy_score(labels, predictions)\n",
    "    myprint('  Accuracy Score: {0:.2f}'.format(acc_score), logfile)\n",
    "    myprint('Report', logfile)\n",
    "    cls_rep = classification_report(labels, predictions)\n",
    "    myprint(cls_rep, logfile)\n",
    "    return f1_score(labels, predictions)  # return f-1 for positive class (sarcasm) as the early stopping measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341641f1",
   "metadata": {},
   "source": [
    "# get the cosine similarties for all three types of data from M-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d060e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of loan word and original word pairs that we are feeding inside the transformer models with tokenizers \n",
    "#to get their vector embedding of the CLS or classification token and \n",
    "#then calculating their cosine similarities between those embedding pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f1da24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11857, 4030, 2417, 1320, 450, 271)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of loan-original words for train sets\n",
    "\n",
    "l1_train_alldata = list(train_alldata_dnnlogits[\"loan_word\"])\n",
    "l2_train_alldata = list(train_alldata_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_train_realdist = list(train_realdist_dnnlogits[\"loan_word\"])\n",
    "l2_train_realdist = list(train_realdist_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_train_balanced = list(train_balanced_dnnlogits[\"loan_word\"])\n",
    "l2_train_balanced = list(train_balanced_dnnlogits[\"original_word\"])\n",
    "\n",
    "#list of loan-original words for test sets\n",
    "\n",
    "\n",
    "l1_test_alldata = list(test_alldata_dnnlogits[\"loan_word\"])\n",
    "l2_test_alldata = list(test_alldata_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_test_realdist = list(test_realdist_dnnlogits[\"loan_word\"])\n",
    "l2_test_realdist = list(test_realdist_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_test_balanced = list(test_balanced_dnnlogits[\"loan_word\"])\n",
    "l2_test_balanced = list(test_balanced_dnnlogits[\"original_word\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(l1_train_alldata), len(l1_train_realdist), len(l1_train_balanced), len(l1_test_alldata), len(l1_test_realdist), len(l1_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ffc2f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-multilingual-cased'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_TRAINED_bert_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587b5de",
   "metadata": {},
   "source": [
    "# get cosine similarities for train set for all three data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a49f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings_alldata = tokenizer(l1_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_alldata = tokenizer(l2_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_realdist = tokenizer(l1_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_realdist = tokenizer(l2_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_balanced = tokenizer(l1_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_balanced = tokenizer(l2_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    \n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_alldata = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_realdist = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_balanced = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['m-bert_cosim'] = sim_lst_alldata\n",
    "train_realdist_dnnlogits['m-bert_cosim'] = sim_lst_realdist\n",
    "train_balanced_dnnlogits['m-bert_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b040d9e",
   "metadata": {},
   "source": [
    "# Get cosim for test set for three types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings_alldata = tokenizer(l1_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_alldata = tokenizer(l2_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_realdist = tokenizer(l1_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_realdist = tokenizer(l2_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_balanced = tokenizer(l1_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_balanced = tokenizer(l2_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    \n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_alldata = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_realdist = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_balanced = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits['m-bert_cosim'] = sim_lst_alldata\n",
    "test_realdist_dnnlogits['m-bert_cosim'] = sim_lst_realdist\n",
    "test_balanced_dnnlogits['m-bert_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83017d",
   "metadata": {},
   "source": [
    "# get cosine sims from the XLM-100 model now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268cc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   # tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    \n",
    "    l1_encodings_alldata = tokenizer(l1_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_alldata = tokenizer(l2_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_realdist = tokenizer(l1_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_realdist = tokenizer(l2_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_balanced = tokenizer(l1_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_balanced = tokenizer(l2_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        \n",
    "        \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_alldata = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_realdist = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_balanced = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['xlm_cosim'] = sim_lst_alldata\n",
    "train_realdist_dnnlogits['xlm_cosim'] = sim_lst_realdist\n",
    "train_balanced_dnnlogits['xlm_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95678761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the new datafames as the notebook is running out of cuda memory \n",
    "\n",
    "train_alldata_dnnlogits.to_csv('modelpredictedlogits_trainDNN_cosims_alldata.csv')\n",
    "train_realdist_dnnlogits.to_csv('modelpredictedlogits_trainDNN_cosims_realdist.csv')\n",
    "train_balanced_dnnlogits.to_csv('modelpredictedlogits_trainDNN_cosims_balanced.csv')\n",
    "\n",
    "test_alldata_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "test_realdist_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "test_balanced_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_balanced.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656e829",
   "metadata": {},
   "source": [
    "# get XLM cosine sims for test splits now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e95a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2268: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n",
      "450\n",
      "271\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   # tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    \n",
    "    l1_encodings_alldata = tokenizer(l1_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_alldata = tokenizer(l2_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_realdist = tokenizer(l1_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_realdist = tokenizer(l2_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_balanced = tokenizer(l1_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_balanced = tokenizer(l2_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        \n",
    "        \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_alldata = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_realdist = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_balanced = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a33ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_alldata_dnnlogits = pd.read_csv('../TorchClassifier_Panphon_features/modelpredictedlogits_trainDNN_alldata.csv')\n",
    " \n",
    "test_alldata_dnnlogits = pd.read_csv('../TorchClassifier_Panphon_features/modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "#train_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../TorchClassifier_Panphon_features/modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "#train_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../TorchClassifier_Panphon_features/modelpredictedlogits_testDNN_cosims_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa776fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>Dolgo Prime Distance Div Maxlen</th>\n",
       "      <th>Feature Edit Distance Div Maxlen</th>\n",
       "      <th>Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>Weighted Feature Distance Div Maxlen</th>\n",
       "      <th>Partial Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>plain Levenshtein</th>\n",
       "      <th>label</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>DNNlogits_modelpredicted</th>\n",
       "      <th>m-bert_cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2180</td>\n",
       "      <td>नौकर</td>\n",
       "      <td>ناهار</td>\n",
       "      <td>nɔːkər</td>\n",
       "      <td>nɒhɒr</td>\n",
       "      <td>Servant</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>5</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.632645</td>\n",
       "      <td>0.651157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2185</td>\n",
       "      <td>प्रभाव</td>\n",
       "      <td>نتیجه</td>\n",
       "      <td>prəb̤aːv</td>\n",
       "      <td>ntjd͡ʒh</td>\n",
       "      <td>Effect</td>\n",
       "      <td>Result</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.336806</td>\n",
       "      <td>6</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.517828</td>\n",
       "      <td>0.388871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3050</td>\n",
       "      <td>माही</td>\n",
       "      <td>ماب</td>\n",
       "      <td>maːɦiː</td>\n",
       "      <td>mɒb</td>\n",
       "      <td>lover</td>\n",
       "      <td>map</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.893986</td>\n",
       "      <td>0.676401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>818</td>\n",
       "      <td>बरामद</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>bəraːməd</td>\n",
       "      <td>rhɒjʔj</td>\n",
       "      <td>found</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.315476</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>4.607143</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.171056</td>\n",
       "      <td>0.478386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2571</td>\n",
       "      <td>बराबर</td>\n",
       "      <td>بارآور</td>\n",
       "      <td>bəraːbər</td>\n",
       "      <td>bɒrɒvr</td>\n",
       "      <td>Equal</td>\n",
       "      <td>Fertile</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.172619</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1.589286</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>6</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.876501</td>\n",
       "      <td>0.472573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1315</td>\n",
       "      <td>1315</td>\n",
       "      <td>3170</td>\n",
       "      <td>वाहवाही</td>\n",
       "      <td>تشویق و تمجید</td>\n",
       "      <td>vaːɦvaːɦiː</td>\n",
       "      <td>tʃvjɣ v tmd͡ʒjd</td>\n",
       "      <td>Praise</td>\n",
       "      <td>Applause</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.422348</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>4.568182</td>\n",
       "      <td>0.452652</td>\n",
       "      <td>13</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-16.205833</td>\n",
       "      <td>0.523541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1316</td>\n",
       "      <td>1316</td>\n",
       "      <td>4560</td>\n",
       "      <td>नुकसान पहुचने वाला</td>\n",
       "      <td>خطرناک</td>\n",
       "      <td>nuksaːnə pəɦut͡ʃne vaːlaː</td>\n",
       "      <td>xtrnɒk</td>\n",
       "      <td>Harmful</td>\n",
       "      <td>Dangerous</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.702546</td>\n",
       "      <td>18</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-39.144264</td>\n",
       "      <td>0.398689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>1317</td>\n",
       "      <td>1317</td>\n",
       "      <td>140</td>\n",
       "      <td>कबाब</td>\n",
       "      <td>کباب</td>\n",
       "      <td>kəbaːb</td>\n",
       "      <td>kbɒb</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>4</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>4.328829</td>\n",
       "      <td>0.677797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1318</td>\n",
       "      <td>1318</td>\n",
       "      <td>4493</td>\n",
       "      <td>अटल</td>\n",
       "      <td>یک دنده</td>\n",
       "      <td>aʈəl</td>\n",
       "      <td>jk dndh</td>\n",
       "      <td>Firm</td>\n",
       "      <td>stubborn</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.413194</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>7</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.486457</td>\n",
       "      <td>0.437662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1319</td>\n",
       "      <td>1319</td>\n",
       "      <td>663</td>\n",
       "      <td>नवाज़</td>\n",
       "      <td>نواز</td>\n",
       "      <td>nəvaːz</td>\n",
       "      <td>nvɒz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>5</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>6.445583</td>\n",
       "      <td>0.555900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1           loan_word  \\\n",
       "0              0             0            2180                नौकर   \n",
       "1              1             1            2185              प्रभाव   \n",
       "2              2             2            3050                माही   \n",
       "3              3             3             818               बरामद   \n",
       "4              4             4            2571               बराबर   \n",
       "...          ...           ...             ...                 ...   \n",
       "1315        1315          1315            3170             वाहवाही   \n",
       "1316        1316          1316            4560  नुकसान पहुचने वाला   \n",
       "1317        1317          1317             140                कबाब   \n",
       "1318        1318          1318            4493                 अटल   \n",
       "1319        1319          1319             663               नवाज़   \n",
       "\n",
       "      original_word          loan_word_epitran original_word_epitran  \\\n",
       "0             ناهار                     nɔːkər                 nɒhɒr   \n",
       "1             نتیجه                   prəb̤aːv               ntjd͡ʒh   \n",
       "2               ماب                     maːɦiː                   mɒb   \n",
       "3             رهائی                   bəraːməd                rhɒjʔj   \n",
       "4            بارآور                   bəraːbər                bɒrɒvr   \n",
       "...             ...                        ...                   ...   \n",
       "1315  تشویق و تمجید                 vaːɦvaːɦiː       tʃvjɣ v tmd͡ʒjd   \n",
       "1316         خطرناک  nuksaːnə pəɦut͡ʃne vaːlaː                xtrnɒk   \n",
       "1317           کباب                     kəbaːb                  kbɒb   \n",
       "1318        یک دنده                       aʈəl               jk dndh   \n",
       "1319           نواز                     nəvaːz                  nvɒz   \n",
       "\n",
       "     loan_english original_english  Fast Levenshtein Distance Div Maxlen  \\\n",
       "0         Servant            Lunch                              0.666667   \n",
       "1          Effect           Result                              1.000000   \n",
       "2           lover              map                              0.833333   \n",
       "3           found            رهائی                              0.875000   \n",
       "4           Equal          Fertile                              0.625000   \n",
       "...           ...              ...                                   ...   \n",
       "1315       Praise         Applause                              0.866667   \n",
       "1316      Harmful        Dangerous                              0.920000   \n",
       "1317        Kebab            Kebab                              0.500000   \n",
       "1318         Firm         stubborn                              1.000000   \n",
       "1319        Nawaz            Nawaz                              0.500000   \n",
       "\n",
       "      Dolgo Prime Distance Div Maxlen  Feature Edit Distance Div Maxlen  \\\n",
       "0                            0.200000                          0.083333   \n",
       "1                            1.000000                          0.319444   \n",
       "2                            0.500000                          0.296875   \n",
       "3                            0.714286                          0.315476   \n",
       "4                            0.142857                          0.172619   \n",
       "...                               ...                               ...   \n",
       "1315                         0.727273                          0.422348   \n",
       "1316                         0.722222                          0.636574   \n",
       "1317                         0.200000                          0.204167   \n",
       "1318                         0.666667                          0.413194   \n",
       "1319                         0.200000                          0.204167   \n",
       "\n",
       "      Hamming Feature Distance Div Maxlen  \\\n",
       "0                                0.083333   \n",
       "1                                0.368056   \n",
       "2                                0.322917   \n",
       "3                                0.369048   \n",
       "4                                0.190476   \n",
       "...                                   ...   \n",
       "1315                             0.465909   \n",
       "1316                             0.708333   \n",
       "1317                             0.225000   \n",
       "1318                             0.465278   \n",
       "1319                             0.225000   \n",
       "\n",
       "      Weighted Feature Distance Div Maxlen  \\\n",
       "0                                 1.300000   \n",
       "1                                 4.083333   \n",
       "2                                 3.187500   \n",
       "3                                 4.607143   \n",
       "4                                 1.589286   \n",
       "...                                    ...   \n",
       "1315                              4.568182   \n",
       "1316                              5.333333   \n",
       "1317                              1.750000   \n",
       "1318                              4.583333   \n",
       "1319                              1.750000   \n",
       "\n",
       "      Partial Hamming Feature Distance Div Maxlen  plain Levenshtein  \\\n",
       "0                                        0.083333                  5   \n",
       "1                                        0.336806                  6   \n",
       "2                                        0.322917                  4   \n",
       "3                                        0.339286                  5   \n",
       "4                                        0.187500                  6   \n",
       "...                                           ...                ...   \n",
       "1315                                     0.452652                 13   \n",
       "1316                                     0.702546                 18   \n",
       "1317                                     0.225000                  4   \n",
       "1318                                     0.447917                  7   \n",
       "1319                                     0.225000                  5   \n",
       "\n",
       "              label  label_bin  DNNlogits_modelpredicted  m-bert_cosim  \n",
       "0     hard_negative          0                 -6.632645      0.651157  \n",
       "1           synonym          0                -12.517828      0.388871  \n",
       "2     hard_negative          0                 -4.893986      0.676401  \n",
       "3            random          0                 -4.171056      0.478386  \n",
       "4     hard_negative          0                 -8.876501      0.472573  \n",
       "...             ...        ...                       ...           ...  \n",
       "1315        synonym          0                -16.205833      0.523541  \n",
       "1316        synonym          0                -39.144264      0.398689  \n",
       "1317           loan          1                  4.328829      0.677797  \n",
       "1318        synonym          0                 -5.486457      0.437662  \n",
       "1319           loan          1                  6.445583      0.555900  \n",
       "\n",
       "[1320 rows x 20 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61edcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ed8a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits['xlm_cosim'] = sim_lst_alldata\n",
    "test_realdist_dnnlogits['xlm_cosim'] = sim_lst_realdist\n",
    "test_balanced_dnnlogits['xlm_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81838972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature Edit Distance Div Maxlen</th>\n",
       "      <th>Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>Weighted Feature Distance Div Maxlen</th>\n",
       "      <th>Partial Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>plain Levenshtein</th>\n",
       "      <th>label</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>DNNlogits_modelpredicted</th>\n",
       "      <th>m-bert_cosim</th>\n",
       "      <th>xlm_cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2180</td>\n",
       "      <td>नौकर</td>\n",
       "      <td>ناهار</td>\n",
       "      <td>nɔːkər</td>\n",
       "      <td>nɒhɒr</td>\n",
       "      <td>Servant</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>5</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.632645</td>\n",
       "      <td>0.651157</td>\n",
       "      <td>0.698117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2185</td>\n",
       "      <td>प्रभाव</td>\n",
       "      <td>نتیجه</td>\n",
       "      <td>prəb̤aːv</td>\n",
       "      <td>ntjd͡ʒh</td>\n",
       "      <td>Effect</td>\n",
       "      <td>Result</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.336806</td>\n",
       "      <td>6</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.517828</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>0.722407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3050</td>\n",
       "      <td>माही</td>\n",
       "      <td>ماب</td>\n",
       "      <td>maːɦiː</td>\n",
       "      <td>mɒb</td>\n",
       "      <td>lover</td>\n",
       "      <td>map</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.893986</td>\n",
       "      <td>0.676401</td>\n",
       "      <td>0.534651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>818</td>\n",
       "      <td>बरामद</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>bəraːməd</td>\n",
       "      <td>rhɒjʔj</td>\n",
       "      <td>found</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315476</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>4.607143</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.171056</td>\n",
       "      <td>0.478386</td>\n",
       "      <td>0.650366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2571</td>\n",
       "      <td>बराबर</td>\n",
       "      <td>بارآور</td>\n",
       "      <td>bəraːbər</td>\n",
       "      <td>bɒrɒvr</td>\n",
       "      <td>Equal</td>\n",
       "      <td>Fertile</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172619</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1.589286</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>6</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.876501</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>0.447921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1315</td>\n",
       "      <td>1315</td>\n",
       "      <td>3170</td>\n",
       "      <td>वाहवाही</td>\n",
       "      <td>تشویق و تمجید</td>\n",
       "      <td>vaːɦvaːɦiː</td>\n",
       "      <td>tʃvjɣ v tmd͡ʒjd</td>\n",
       "      <td>Praise</td>\n",
       "      <td>Applause</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422348</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>4.568182</td>\n",
       "      <td>0.452652</td>\n",
       "      <td>13</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-16.205833</td>\n",
       "      <td>0.523541</td>\n",
       "      <td>0.581920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1316</td>\n",
       "      <td>1316</td>\n",
       "      <td>4560</td>\n",
       "      <td>नुकसान पहुचने वाला</td>\n",
       "      <td>خطرناک</td>\n",
       "      <td>nuksaːnə pəɦut͡ʃne vaːlaː</td>\n",
       "      <td>xtrnɒk</td>\n",
       "      <td>Harmful</td>\n",
       "      <td>Dangerous</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.702546</td>\n",
       "      <td>18</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-39.144264</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.202958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>1317</td>\n",
       "      <td>1317</td>\n",
       "      <td>140</td>\n",
       "      <td>कबाब</td>\n",
       "      <td>کباب</td>\n",
       "      <td>kəbaːb</td>\n",
       "      <td>kbɒb</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>4</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>4.328829</td>\n",
       "      <td>0.677797</td>\n",
       "      <td>0.805250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1318</td>\n",
       "      <td>1318</td>\n",
       "      <td>4493</td>\n",
       "      <td>अटल</td>\n",
       "      <td>یک دنده</td>\n",
       "      <td>aʈəl</td>\n",
       "      <td>jk dndh</td>\n",
       "      <td>Firm</td>\n",
       "      <td>stubborn</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413194</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>7</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.486457</td>\n",
       "      <td>0.437662</td>\n",
       "      <td>0.476721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1319</td>\n",
       "      <td>1319</td>\n",
       "      <td>663</td>\n",
       "      <td>नवाज़</td>\n",
       "      <td>نواز</td>\n",
       "      <td>nəvaːz</td>\n",
       "      <td>nvɒz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>5</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>6.445583</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.467944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1           loan_word  \\\n",
       "0              0             0            2180                नौकर   \n",
       "1              1             1            2185              प्रभाव   \n",
       "2              2             2            3050                माही   \n",
       "3              3             3             818               बरामद   \n",
       "4              4             4            2571               बराबर   \n",
       "...          ...           ...             ...                 ...   \n",
       "1315        1315          1315            3170             वाहवाही   \n",
       "1316        1316          1316            4560  नुकसान पहुचने वाला   \n",
       "1317        1317          1317             140                कबाब   \n",
       "1318        1318          1318            4493                 अटल   \n",
       "1319        1319          1319             663               नवाज़   \n",
       "\n",
       "      original_word          loan_word_epitran original_word_epitran  \\\n",
       "0             ناهار                     nɔːkər                 nɒhɒr   \n",
       "1             نتیجه                   prəb̤aːv               ntjd͡ʒh   \n",
       "2               ماب                     maːɦiː                   mɒb   \n",
       "3             رهائی                   bəraːməd                rhɒjʔj   \n",
       "4            بارآور                   bəraːbər                bɒrɒvr   \n",
       "...             ...                        ...                   ...   \n",
       "1315  تشویق و تمجید                 vaːɦvaːɦiː       tʃvjɣ v tmd͡ʒjd   \n",
       "1316         خطرناک  nuksaːnə pəɦut͡ʃne vaːlaː                xtrnɒk   \n",
       "1317           کباب                     kəbaːb                  kbɒb   \n",
       "1318        یک دنده                       aʈəl               jk dndh   \n",
       "1319           نواز                     nəvaːz                  nvɒz   \n",
       "\n",
       "     loan_english original_english  Fast Levenshtein Distance Div Maxlen  ...  \\\n",
       "0         Servant            Lunch                              0.666667  ...   \n",
       "1          Effect           Result                              1.000000  ...   \n",
       "2           lover              map                              0.833333  ...   \n",
       "3           found            رهائی                              0.875000  ...   \n",
       "4           Equal          Fertile                              0.625000  ...   \n",
       "...           ...              ...                                   ...  ...   \n",
       "1315       Praise         Applause                              0.866667  ...   \n",
       "1316      Harmful        Dangerous                              0.920000  ...   \n",
       "1317        Kebab            Kebab                              0.500000  ...   \n",
       "1318         Firm         stubborn                              1.000000  ...   \n",
       "1319        Nawaz            Nawaz                              0.500000  ...   \n",
       "\n",
       "      Feature Edit Distance Div Maxlen  Hamming Feature Distance Div Maxlen  \\\n",
       "0                             0.083333                             0.083333   \n",
       "1                             0.319444                             0.368056   \n",
       "2                             0.296875                             0.322917   \n",
       "3                             0.315476                             0.369048   \n",
       "4                             0.172619                             0.190476   \n",
       "...                                ...                                  ...   \n",
       "1315                          0.422348                             0.465909   \n",
       "1316                          0.636574                             0.708333   \n",
       "1317                          0.204167                             0.225000   \n",
       "1318                          0.413194                             0.465278   \n",
       "1319                          0.204167                             0.225000   \n",
       "\n",
       "      Weighted Feature Distance Div Maxlen  \\\n",
       "0                                 1.300000   \n",
       "1                                 4.083333   \n",
       "2                                 3.187500   \n",
       "3                                 4.607143   \n",
       "4                                 1.589286   \n",
       "...                                    ...   \n",
       "1315                              4.568182   \n",
       "1316                              5.333333   \n",
       "1317                              1.750000   \n",
       "1318                              4.583333   \n",
       "1319                              1.750000   \n",
       "\n",
       "      Partial Hamming Feature Distance Div Maxlen  plain Levenshtein  \\\n",
       "0                                        0.083333                  5   \n",
       "1                                        0.336806                  6   \n",
       "2                                        0.322917                  4   \n",
       "3                                        0.339286                  5   \n",
       "4                                        0.187500                  6   \n",
       "...                                           ...                ...   \n",
       "1315                                     0.452652                 13   \n",
       "1316                                     0.702546                 18   \n",
       "1317                                     0.225000                  4   \n",
       "1318                                     0.447917                  7   \n",
       "1319                                     0.225000                  5   \n",
       "\n",
       "              label label_bin  DNNlogits_modelpredicted  m-bert_cosim  \\\n",
       "0     hard_negative         0                 -6.632645      0.651157   \n",
       "1           synonym         0                -12.517828      0.388871   \n",
       "2     hard_negative         0                 -4.893986      0.676401   \n",
       "3            random         0                 -4.171056      0.478386   \n",
       "4     hard_negative         0                 -8.876501      0.472573   \n",
       "...             ...       ...                       ...           ...   \n",
       "1315        synonym         0                -16.205833      0.523541   \n",
       "1316        synonym         0                -39.144264      0.398689   \n",
       "1317           loan         1                  4.328829      0.677797   \n",
       "1318        synonym         0                 -5.486457      0.437662   \n",
       "1319           loan         1                  6.445583      0.555900   \n",
       "\n",
       "      xlm_cosim  \n",
       "0      0.698117  \n",
       "1      0.722407  \n",
       "2      0.534651  \n",
       "3      0.650366  \n",
       "4      0.447921  \n",
       "...         ...  \n",
       "1315   0.581920  \n",
       "1316   0.202958  \n",
       "1317   0.805250  \n",
       "1318   0.476721  \n",
       "1319   0.467944  \n",
       "\n",
       "[1320 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b294627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes as csv files \n",
    "\n",
    "test_alldata_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "test_realdist_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "test_balanced_dnnlogits.to_csv('modelpredictedlogits_testDNN_cosims_balanced.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18819a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea1cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
