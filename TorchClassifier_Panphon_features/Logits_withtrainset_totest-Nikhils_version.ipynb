{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d900aa",
   "metadata": {},
   "source": [
    "Assumes you have run `Train_Testset.ipynb` first to make the `alldata`, `realdist`, and `balanced` train/test splits for the chosen language pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a3a5e",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4977c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import io\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b4fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "#device = torch.device(\"cuda:0:3\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963de5f8",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata = pd.read_csv('../Datasets/train_final_production_alldata.csv')\n",
    "test_alldata = pd.read_csv('../Datasets/test_final_production_alldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b808ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist = pd.read_csv('../Datasets/train_final_production_realdist.csv')\n",
    "test_realdist = pd.read_csv('../Datasets/test_final_production_realdist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced = pd.read_csv('../Datasets/train_final_production_balanced.csv')\n",
    "test_balanced = pd.read_csv('../Datasets/test_final_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4051b",
   "metadata": {},
   "source": [
    "## Get Panphon phonetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42985f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get phonetic features using panPhon\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e887590",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata['features_loan'] = train_alldata.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_alldata['features_orig'] = train_alldata.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_alldata['features_loan'] = test_alldata.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_alldata['features_orig'] = test_alldata.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_alldata['features_loan'] = train_alldata['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_alldata['features_orig'] = train_alldata['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x:sum(x, []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384941ff",
   "metadata": {},
   "source": [
    "Pad the phonetic features of the loan word and original word out to the maxlen of the features appearing in the training set (format: `<loan><pad 0s><orig><pad 0s>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24078d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_maxlen = (np.max(train_alldata['features_loan'].str.len()),\\\n",
    "                               np.max(train_alldata['features_orig'].str.len()))\n",
    "\n",
    "train_alldata['features_loan'] = train_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_alldata['features_orig'] = train_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "train_alldata['features_loan'][np.random.randint(len(train_alldata['features_loan']))],\\\n",
    "train_alldata['features_orig'][np.random.randint(len(train_alldata['features_loan']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf93371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_alldata['features_loan'][np.random.randint(len(test_alldata['features_loan']))],\\\n",
    "test_alldata['features_orig'][np.random.randint(len(test_alldata['features_orig']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a031511",
   "metadata": {},
   "source": [
    "## Add target labels and make train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array([y for y in train_alldata['label_bin']])\n",
    "Y_test = np.array([y for y in test_alldata['label_bin']])\n",
    "Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f08e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])\n",
    "X_test = np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93f3aa",
   "metadata": {},
   "source": [
    "Make a validation split for training the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44851fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and validation splits keeping the composition of labels balanced between them using a random state '1 '\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "X_test = torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "\n",
    "X_val = torch.tensor(X_val).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e42f7",
   "metadata": {},
   "source": [
    "## DNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            \n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        #logits = self.linear_relu_stack(x)\n",
    "        logits_new = self.linear_relu_stack(x)\n",
    "        logits  = self.dropout(logits_new)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits_new\n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "#model = NeuralNetwork(X_test.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5) \n",
    "    return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fa7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad1111",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ce499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 5000 epochs and get the logits \n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_accur = []\n",
    "train_accur = []\n",
    "logits = []\n",
    "for epoch in range(5000):\n",
    "\n",
    "    y_pred = model(X_train.float())[0]\n",
    "    logits = model(X_train.float())[1]\n",
    "    #getting logits for test set \n",
    "#     y_pred = model(X_test.float())[0]\n",
    "#     logits = model(X_test.float())[1]\n",
    "    #y_pred = model(X_train) \n",
    "    #print(y_pred)\n",
    "\n",
    "    #y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, Y_train.float())\n",
    "    \n",
    "    #test_loss = criterion(y_pred, Y_test.float())\n",
    "    #train_loss = criterion(y_pred, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        train_acc,_ = calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "        y_val_pred = model(X_val.float())[0]\n",
    "        #y_test_pred = torch.squeeze(y_test_pred)\n",
    "         \n",
    "\n",
    "        val_loss = criterion(y_val_pred, Y_val.float())\n",
    "\n",
    "        val_acc, total_corr = calculate_accuracy(Y_val, y_val_pred)\n",
    "        #print(total_corr)\n",
    "        \n",
    "        print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)} Val set - loss: {round_tensor(val_loss)}, Val accuracy: {round_tensor(val_acc)}\n",
    "''')\n",
    "        #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "        #print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "        train_losses.append(train_loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        \n",
    "        val_accur.append(val_acc.detach().numpy())\n",
    "        train_accur.append(train_acc.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss.backward()\n",
    "    #test_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df878f6a",
   "metadata": {},
   "source": [
    "Plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb71617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_accur, 'b', label='vaidation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a602a53",
   "metadata": {},
   "source": [
    "## Setup evaluation datasets\n",
    "\n",
    "Get Panphon features and pad (use `train_alldata` as already defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist['features_loan'] = train_realdist.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_realdist['features_orig'] = train_realdist.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_realdist['features_loan'] = test_realdist.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_realdist['features_orig'] = test_realdist.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x:sum(x, []))\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train_balanced['features_orig'] = train_balanced.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "test_balanced['features_loan'] = test_balanced.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test_balanced['features_orig'] = test_balanced.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x:sum(x, []))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x:sum(x, []))\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x:sum(x, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d768958",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                     np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                    np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                     np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                    np.array([x for x in test_balanced['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadd528",
   "metadata": {},
   "source": [
    "Get logits from DNN for all datasets/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_logits_dnn_alldata = model(X_train_alldata.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_alldata = model(X_test_alldata.float())[1].detach().cpu().numpy()\n",
    "    train_logits_dnn_realdist = model(X_train_realdist.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_realdist = model(X_test_realdist.float())[1].detach().cpu().numpy()\n",
    "    train_logits_dnn_balanced = model(X_train_balanced.float())[1].detach().cpu().numpy()\n",
    "    test_logits_dnn_balanced = model(X_test_balanced.float())[1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f44c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits_dnn_alldata, test_logits_dnn_alldata,\\\n",
    "train_logits_dnn_realdist, test_logits_dnn_realdist,\\\n",
    "train_logits_dnn_balanced, test_logits_dnn_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b347",
   "metadata": {},
   "source": [
    "Add DNN logit column to production datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits = pd.read_csv('../Datasets/train_final_production_alldata.csv')\n",
    "test_alldata_dnnlogits = pd.read_csv('../Datasets/test_final_production_alldata.csv')\n",
    "train_realdist_dnnlogits = pd.read_csv('../Datasets/train_final_production_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../Datasets/test_final_production_realdist.csv')\n",
    "train_balanced_dnnlogits = pd.read_csv('../Datasets/train_final_production_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../Datasets/test_final_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_alldata\n",
    "test_alldata_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_alldata\n",
    "\n",
    "train_realdist_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_realdist\n",
    "test_realdist_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_realdist\n",
    "\n",
    "train_balanced_dnnlogits['DNNlogits_modelpredicted'] = train_logits_dnn_balanced\n",
    "test_balanced_dnnlogits['DNNlogits_modelpredicted'] = test_logits_dnn_balanced\n",
    "\n",
    "train_alldata_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_alldata.csv')\n",
    "test_alldata_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_alldata.csv')\n",
    "\n",
    "train_realdist_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_realdist.csv')\n",
    "\n",
    "train_balanced_dnnlogits.to_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits.to_csv('../Datasets/modelpredictedlogits_testDNN_balanced.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df5737",
   "metadata": {},
   "source": [
    "## Setup data for CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba8383",
   "metadata": {},
   "source": [
    "Current CNN approach doesn't really work.  Because the inputs are PanPhon features with padding dependent on the maxlen of the input data, and then reshaped into a square to fit into a 2D CNN, the filter is not able to capture the necessary dependency: that is, the relationship between an L1 feature and an L2 feature at the same approximate position in their respective words (cf., Persian /ɣ/ usually becomes Hindi /q/).  If we reshape the data this way for the network, because the ordering of PanPhon features are effectively conventionlized into a fixed order, a /q/ in an (e.g.) Hindi word is not guaranteed to fall in the same window as the equivalent /ɣ/ in the (e.g.) Persian source word, thus losing the dependency.  Therefore the CNN usually falls into a local minimum of predicting everything to be a non-loan word.  Loss remains relatively compared to the DNN and accuracy plateaus at about 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ad51e",
   "metadata": {},
   "source": [
    "Use `train_alldata` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df564fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])\n",
    "Y_train = np.array([y for y in train_alldata['label_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation splits for proper model training while keeping the composition of labels balanced between them using a random state '1 '\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1, stratify=Y_train)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to torch tensors for padding\n",
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "X_test = torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "\n",
    "X_val = torch.tensor(X_val).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_perfect_square = X_train.shape[1]\n",
    "while (True):\n",
    "    if np.sqrt(closest_perfect_square) - np.floor(np.sqrt(closest_perfect_square)) != 0:\n",
    "        closest_perfect_square += 1\n",
    "    else:\n",
    "        break\n",
    "view_shape = int(np.sqrt(closest_perfect_square))\n",
    "closest_perfect_square,view_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = F.pad(X_train, pad=(0, closest_perfect_square-X_train.shape[1]), value=0)\n",
    "X_val = F.pad(X_val, pad=(0, closest_perfect_square-X_val.shape[1]), value=0)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b700",
   "metadata": {},
   "source": [
    "## CNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 128, 8) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(128, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 32, 2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        #x = torch.randn(33,33).view(-1,1,33,33) #33 because its the square root of 1089\n",
    "        #x = torch.randn(30,30).view(-1,1,30,30) #30 because its the square root of 900 for real dist train set\n",
    "        #x = torch.randn(29,29).view(-1,1,29,29) #29 because its the square root of 841 for balanced train set\n",
    "        x = torch.randn(view_shape,view_shape).view(-1,1,view_shape,view_shape) # for trained model logit prediction, for all data, 1089 is sq of 33\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(torch.tanh(self.conv1(x)), (2, 2))\n",
    "        #x = F.max_pool2d(torch.tanh(self.conv2(x)), (1, 1))\n",
    "        #x = F.max_pool2d(torch.tanh(self.conv3(x)), (1, 1))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        \n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = self.dropout(x)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "CNN_Net = CCN_Net() \n",
    "CNN_Net = nn.DataParallel(CNN_Net)\n",
    "CNN_Net.to(device)\n",
    "print(CNN_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).view(-1,view_shape,view_shape).to(device)\n",
    "X_val = torch.tensor(X_val).view(-1,view_shape,view_shape).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_val = torch.tensor(Y_val).to(device)\n",
    "X_train.shape,X_val.shape,Y_train.shape,Y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(CNN_Net.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(CNN_Net.parameters(),lr=0.001, momentum=0.0,  weight_decay=0.0, nesterov=False)\n",
    "#optimizer = torch.optim.RMSprop(CNN_Net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#loss_function = nn.MSELoss()\n",
    "scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.unsqueeze(1) #just do it once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the seed manually to 42\n",
    "torch.manual_seed(42)\n",
    "# a = X_val_CNN[torch.randint(len(X_val_CNN), (120,))]  \n",
    "# # b = Y_val_CNN[torch.randint(len(X_val_CNN), (120,))]  \n",
    "# # a.shape\n",
    "# b\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea985a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train for 10000 epochs and get the logits \n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_accur = []\n",
    "train_accur = []\n",
    "train_losses_batch = []\n",
    "logits = []\n",
    "BATCH_SIZE = 512\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for i in tqdm(range(0, len(X_train), BATCH_SIZE)):\n",
    "        batch_X = X_train[i:i+BATCH_SIZE].view(-1, 1, view_shape,view_shape)  \n",
    "        #batch_X = X_train_CNN.view(-1, 1, 29,29)  \n",
    "        batch_y = Y_train[i:i+BATCH_SIZE]\n",
    "        \n",
    "        \n",
    "\n",
    "        #X_train_CNN = X_train_CNN.view(-1, 1, 33,33) # for balanced train set\n",
    "        CNN_Net.zero_grad()\n",
    "        \n",
    "        y_pred = CNN_Net(batch_X .float())[0]\n",
    "        #print(y_pred)\n",
    "        logits = CNN_Net(batch_X.float())[1]\n",
    "        #getting logits for test set \n",
    "    #     y_pred = model(X_test.float())[0]\n",
    "    #     logits = model(X_test.float())[1]\n",
    "        #y_pred = model(X_train) \n",
    "        #print(y_pred)\n",
    "\n",
    "        #y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = loss_function(y_pred, batch_y.float())\n",
    "\n",
    "        #test_loss = criterion(y_pred, Y_test.float())\n",
    "        #train_loss = criterion(y_pred, Y_train)\n",
    "        train_losses.append(train_loss)\n",
    "        if epoch % (n_epochs // 20) == 0:\n",
    "            with torch.no_grad():\n",
    "                CNN_Net.eval()\n",
    "\n",
    "                val_batch_X = X_val[torch.randint(len(X_val), (BATCH_SIZE,))] \n",
    "                val_batch_Y = Y_val[torch.randint(len(X_val), (BATCH_SIZE,))] \n",
    "\n",
    "                train_acc,_ = calculate_accuracy(batch_y, y_pred)\n",
    "                #X_val_CNN= X_val_CNN.unsqueeze(1) don't do it here, it will keep adding a channel dimension every time the for loop operates\n",
    "                y_val_pred = CNN_Net(val_batch_X.float())[0]\n",
    "                #print(y_val_pred)\n",
    "                #y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "\n",
    "                val_loss = loss_function(y_val_pred, val_batch_Y.float())\n",
    "\n",
    "                val_acc, total_corr = calculate_accuracy(val_batch_Y, y_val_pred)\n",
    "                #print(total_corr)\n",
    "\n",
    "                print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)} Val  set - loss: {round_tensor(val_loss)}, Val accuracy: {round_tensor(val_acc)}\n",
    "        ''')\n",
    "                #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "                #print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "                train_losses_batch.append(train_loss.detach().numpy())\n",
    "                val_losses.append(val_loss.detach().numpy())\n",
    "\n",
    "                train_accur.append(train_acc.detach().numpy())\n",
    "                val_accur.append(val_acc.detach().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.backward()\n",
    "        #test_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    scheduler1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10738be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_accur, 'b', label='vaidation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses_batch, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda54c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata['features_loan'] = test_alldata['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_alldata['features_orig'] = test_alldata['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                     np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                    np.array([x for x in test_alldata['features_orig']])])).to(device)\n",
    "\n",
    "train_realdist['features_loan'] = train_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_realdist['features_orig'] = train_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_realdist['features_loan'] = test_realdist['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_realdist['features_orig'] = test_realdist['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                     np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                    np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "train_balanced['features_loan'] = train_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "train_balanced['features_orig'] = train_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "test_balanced['features_loan'] = test_balanced['features_loan'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[0]-len(x)), 'constant'))\n",
    "test_balanced['features_orig'] = test_balanced['features_orig'].apply(lambda x: \\\n",
    "                                np.pad(x,\\\n",
    "                                (0,train_alldata_maxlen[1]-len(x)), 'constant'))\n",
    "\n",
    "X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                     np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                    np.array([x for x in test_balanced['features_orig']])])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae325ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alldata = F.pad(X_train_alldata, pad=(0, closest_perfect_square-X_train_alldata.shape[1]), value=0)\n",
    "X_test_alldata = F.pad(X_test_alldata, pad=(0, closest_perfect_square-X_test_alldata.shape[1]), value=0)\n",
    "X_train_realdist = F.pad(X_train_realdist, pad=(0, closest_perfect_square-X_train_realdist.shape[1]), value=0)\n",
    "X_test_realdist = F.pad(X_test_realdist, pad=(0, closest_perfect_square-X_test_realdist.shape[1]), value=0)\n",
    "X_train_balanced = F.pad(X_train_balanced, pad=(0, closest_perfect_square-X_train_balanced.shape[1]), value=0)\n",
    "X_test_balanced = F.pad(X_test_balanced, pad=(0, closest_perfect_square-X_test_balanced.shape[1]), value=0)\n",
    "X_train_alldata.shape,X_test_alldata.shape,X_train_realdist.shape,X_test_realdist.shape,X_train_balanced.shape,X_test_balanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alldata = torch.tensor(X_train_alldata).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_alldata = torch.tensor(X_test_alldata).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_realdist = torch.tensor(X_train_realdist).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_realdist = torch.tensor(X_test_realdist).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_balanced = torch.tensor(X_train_balanced).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_test_balanced = torch.tensor(X_test_balanced).view(-1,1,view_shape,view_shape).to(device)\n",
    "X_train_alldata.shape,X_test_alldata.shape,X_train_realdist.shape,X_test_realdist.shape,X_train_balanced.shape,X_test_balanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained CNN model to get logits for the three different splits\n",
    "\n",
    "CNN_Net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_logits_cnn_alldata = CNN_Net(X_train_alldata.float())[1]\n",
    "    test_logits_cnn_alldata = CNN_Net(X_test_alldata.float())[1]\n",
    "    \n",
    "    train_logits_cnn_realdist = CNN_Net(X_train_realdist.float())[1]\n",
    "    test_logits_cnn_realdist = CNN_Net(X_test_realdist.float())[1]\n",
    "    \n",
    "    train_logits_cnn_balanced = CNN_Net(X_train_balanced.float())[1]\n",
    "    test_logits_cnn_balanced = CNN_Net(X_test_balanced.float())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3159efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CNN_Net(X_train_alldata.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits_cnn_alldata,\\\n",
    "test_logits_cnn_alldata,\\\n",
    "train_logits_cnn_realdist,\\\n",
    "test_logits_cnn_realdist,\\\n",
    "train_logits_cnn_balanced,\\\n",
    "test_logits_cnn_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b352d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Hindi-Persian\n",
    "\n",
    "# train_alldata_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_alldata.csv')\n",
    " \n",
    "# test_alldata_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_alldata.csv')\n",
    "# train_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "# test_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_realdist.csv')\n",
    "# train_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "# test_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_balanced.csv')\n",
    "\n",
    "\n",
    "# for English-French \n",
    "\n",
    "train_alldata_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_alldata.csv')\n",
    " \n",
    "test_alldata_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_alldata.csv')\n",
    "train_realdist_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_realdist.csv')\n",
    "train_balanced_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_balanced.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ba1efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14876, 19), (1655, 19), (6156, 19), (509, 19), (5502, 19), (436, 19))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_alldata_dnnlogits.shape, test_alldata_dnnlogits.shape, train_realdist_dnnlogits.shape, test_realdist_dnnlogits.shape,train_balanced_dnnlogits.shape,test_balanced_dnnlogits.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50e0fe",
   "metadata": {},
   "source": [
    "# this part includes steps to get the cosine similarities from the two multi-lingual transformer model we are using : M-bert multilingual cased and XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed510e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer specific imports \n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\\\n",
    "    , BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51427b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "xlm_tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "xlm_model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8b35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seeds for reproducibility even though we are not fine-tuning or training and the weights \n",
    "#for both these models are effectively frozen for our purpose \n",
    "\n",
    "torch.manual_seed(7)\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "# Setting PyTorch's required configuration variables for reproducibility.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a4c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_bert_MODEL = 'bert-base-multilingual-cased'\n",
    "PRE_TRAINED_xlm_MODEL = 'xlm-mlm-100-1280'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014205c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 5\n",
    "NUM_EPOCHS = 2000  # default maximum number of epochs\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "BS = 8  # batch size\n",
    "INITIAL_LR = 1e-5  # initial learning rate\n",
    "save_epochs = [1, 2, 3, 4, 5, 6, 7]  # these are the epoch numbers (starting from 1) to test the model on the test set\n",
    "# and save the model checkpoint.\n",
    "EARLY_STOP_PATIENCE = 30  # If model does not improve for this number of epochs, training stops.\n",
    "\n",
    "# Setting GPU cards to use for training the model. Make sure you read our paper to figure out if you have enough GPU\n",
    "# memory. If not, you can change all of them to 'cpu' to use CPU instead of GPU. By the way, two 24 GB GPU cards are\n",
    "# enough for current configuration, but in case of developing based on this you may need more (that's why there are\n",
    "# three cards declared here)\n",
    "# CUDA_0 = 'cuda:1'\n",
    "# CUDA_1 = 'cuda:1'\n",
    "# CUDA_2 = 'cuda:1'\n",
    "args = sys.argv\n",
    "epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf2820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 512\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "#CUDA_0 = 'cuda:1'\n",
    "#CUDA_1 = 'cuda:1'\n",
    "#CUDA_2 = 'cuda:1'\n",
    "CUDA_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#CUDA_1 = 'cuda:0'\n",
    "#CUDA_2 = 'cuda:0'\n",
    "\n",
    "# The function for printing in both console and a given log file.\n",
    "def myprint(mystr, logfile):\n",
    "    print(mystr)\n",
    "    print(mystr, file=logfile)\n",
    "\n",
    "\n",
    "# The function for loading datasets from parallel tsv files and returning texts in lists.\n",
    "def load_data(file_name):\n",
    "    try:\n",
    "        # f = open(file_name)\n",
    "        f = pd.read_csv(file_name, sep='\\t', names=['l1_text', 'l2_text'])#, 'extra'])\n",
    "    except:\n",
    "        print('my log: could not read file')\n",
    "        exit()\n",
    "    print(\"This many number of rows were removed from \" + file_name.split(\"/\")[-1] + \" due to having missing values: \",\n",
    "          f.shape[0] - f.dropna().shape[0])\n",
    "    f.dropna(inplace=True)\n",
    "    l1_texts = f['l1_text'].values.tolist()\n",
    "    l2_texts = f['l2_text'].values.tolist()\n",
    "    print(len(l1_texts), len(l2_texts))\n",
    "    print(l1_texts[500])\n",
    "    print(\"\\n\")\n",
    "    print(l2_texts[500])\n",
    "    return l1_texts, l2_texts\n",
    "\n",
    "\n",
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyDataset1(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    # Each component other than the Transformer, are in a sequential layer (it is not required obviously, but it is\n",
    "    # possible to stack them with other layers if desired)\n",
    "    def __init__(self, base_model, n_classes, dropout=0.05):\n",
    "        super().__init__()\n",
    "        # self.base_model = base_model.to(CUDA_0)\n",
    "        self.transformation_learner = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU()\n",
    "        ).to(CUDA_0)\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        l1_pooler_output = input\n",
    "        # l2 = input2\n",
    "        # if 'l1_attention_mask' in kwargs:\n",
    "        #     l1_attention_mask = kwargs['l1_attention_mask']\n",
    "            # l2_attention_mask = kwargs['l2_attention_mask']\n",
    "        # else:\n",
    "        #     print(\"my err: attention mask is not set, error maybe\")\n",
    "        # here we use only the CLS token\n",
    "        # l1_pooler_output = self.base_model(l1.to(CUDA_0), attention_mask=l1_attention_mask.to(CUDA_0)).pooler_output\n",
    "        myoutput = self.transformation_learner(l1_pooler_output)\n",
    "        return myoutput\n",
    "\n",
    "\n",
    "# The function to compute and print the performance measure scores using sklearn implementations.\n",
    "def evaluate_model(labels, predictions, titlestr, logfile):\n",
    "    myprint(titlestr, logfile)\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    myprint(\"Confusion matrix- \\n\" + str(conf_matrix), logfile)\n",
    "    acc_score = accuracy_score(labels, predictions)\n",
    "    myprint('  Accuracy Score: {0:.2f}'.format(acc_score), logfile)\n",
    "    myprint('Report', logfile)\n",
    "    cls_rep = classification_report(labels, predictions)\n",
    "    myprint(cls_rep, logfile)\n",
    "    return f1_score(labels, predictions)  # return f-1 for positive class (sarcasm) as the early stopping measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3dbb98",
   "metadata": {},
   "source": [
    "# get the cosine similarties for all three types of data from M-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f010f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of loan word and original word pairs that we are feeding inside the transformer models with tokenizers \n",
    "#to get their vector embedding of the CLS or classification token and \n",
    "#then calculating their cosine similarities between those embedding pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a1cdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14876, 6156, 5502, 1655, 509, 436)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of loan-original words for train sets\n",
    "\n",
    "l1_train_alldata = list(train_alldata_dnnlogits[\"loan_word\"])\n",
    "l2_train_alldata = list(train_alldata_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_train_realdist = list(train_realdist_dnnlogits[\"loan_word\"])\n",
    "l2_train_realdist = list(train_realdist_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_train_balanced = list(train_balanced_dnnlogits[\"loan_word\"])\n",
    "l2_train_balanced = list(train_balanced_dnnlogits[\"original_word\"])\n",
    "\n",
    "#list of loan-original words for test sets\n",
    "\n",
    "\n",
    "l1_test_alldata = list(test_alldata_dnnlogits[\"loan_word\"])\n",
    "l2_test_alldata = list(test_alldata_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_test_realdist = list(test_realdist_dnnlogits[\"loan_word\"])\n",
    "l2_test_realdist = list(test_realdist_dnnlogits[\"original_word\"])\n",
    "\n",
    "l1_test_balanced = list(test_balanced_dnnlogits[\"loan_word\"])\n",
    "l2_test_balanced = list(test_balanced_dnnlogits[\"original_word\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(l1_train_alldata), len(l1_train_realdist), len(l1_train_balanced), len(l1_test_alldata), len(l1_test_realdist), len(l1_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33583f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-multilingual-cased'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_TRAINED_bert_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db6fb5",
   "metadata": {},
   "source": [
    "# get cosine similarities for train set for all three data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings_alldata = tokenizer(l1_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_alldata = tokenizer(l2_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_realdist = tokenizer(l1_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_realdist = tokenizer(l2_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_balanced = tokenizer(l1_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_balanced = tokenizer(l2_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    \n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_alldata = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_realdist = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_balanced = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48655897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['m-bert_cosim'] = sim_lst_alldata\n",
    "train_realdist_dnnlogits['m-bert_cosim'] = sim_lst_realdist\n",
    "train_balanced_dnnlogits['m-bert_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef8a35",
   "metadata": {},
   "source": [
    "# Get cosim for test set for three types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings_alldata = tokenizer(l1_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_alldata = tokenizer(l2_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_realdist = tokenizer(l1_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_realdist = tokenizer(l2_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l1_encodings_balanced = tokenizer(l1_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings_balanced = tokenizer(l2_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    \n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_alldata = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_realdist = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims_balanced = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits['m-bert_cosim'] = sim_lst_alldata\n",
    "test_realdist_dnnlogits['m-bert_cosim'] = sim_lst_realdist\n",
    "test_balanced_dnnlogits['m-bert_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e119ca",
   "metadata": {},
   "source": [
    "# get cosine sims from the XLM-100 model now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270209fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   # tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    \n",
    "    l1_encodings_alldata = tokenizer(l1_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_alldata = tokenizer(l2_train_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_realdist = tokenizer(l1_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_realdist = tokenizer(l2_train_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_balanced = tokenizer(l1_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_balanced = tokenizer(l2_train_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        \n",
    "        \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_alldata = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_realdist = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_balanced = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits['xlm_cosim'] = sim_lst_alldata\n",
    "train_realdist_dnnlogits['xlm_cosim'] = sim_lst_realdist\n",
    "train_balanced_dnnlogits['xlm_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits.loc[train_alldata_dnnlogits['m-bert_cosim']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef44ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the new datafames as the notebook is running out of cuda memory, please proceed without running this cell if no \n",
    "#cuda memory issues \n",
    "\n",
    "train_alldata_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_alldata.csv')\n",
    "train_realdist_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_realdist.csv')\n",
    "train_balanced_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_balanced.csv')\n",
    "\n",
    "test_alldata_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "test_realdist_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "test_balanced_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_balanced.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45218e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8d353",
   "metadata": {},
   "source": [
    "# get XLM cosine sims for test splits now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "072ca82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2268: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655\n",
      "509\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   # tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    \n",
    "    l1_encodings_alldata = tokenizer(l1_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_alldata = tokenizer(l2_test_alldata, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_realdist = tokenizer(l1_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_realdist = tokenizer(l2_test_realdist, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    l1_encodings_balanced = tokenizer(l1_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings_balanced = tokenizer(l2_test_balanced, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_alldata = MyDataset(l1_encodings_alldata, l2_encodings_alldata)\n",
    "    dataset_realdist = MyDataset(l1_encodings_realdist, l2_encodings_realdist)\n",
    "    dataset_balanced  = MyDataset(l1_encodings_balanced, l2_encodings_balanced)\n",
    "    \n",
    "    \n",
    "    data_loader_alldata = DataLoader(dataset_alldata, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    data_loader_realdist = DataLoader(dataset_realdist, batch_size=BS, shuffle=False)\n",
    "    data_loader_balanced = DataLoader(dataset_balanced, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst_alldata = []\n",
    "    sim_lst_realdist = []\n",
    "    sim_lst_balanced = []\n",
    "     \n",
    "    \n",
    "    #loop for all data \n",
    "    for step, batch in enumerate(data_loader_alldata):\n",
    "        \n",
    "        \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_alldata = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_alldata.extend(list(sims_alldata))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_alldata))\n",
    "    \n",
    "    #loop for real dist\n",
    "\n",
    "    for step, batch in enumerate(data_loader_realdist):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_realdist = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_realdist.extend(list(sims_realdist))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_realdist))\n",
    "\n",
    "# loop for balanced dataset\n",
    "    for step, batch in enumerate(data_loader_balanced):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        sims_balanced = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        sim_lst_balanced.extend(list(sims_balanced))\n",
    "        #sim_lst_test.extend(list(sims))\n",
    "    print(len(sim_lst_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae15abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_alldata_dnnlogits = pd.read_csv('../TorchClassifier_Panphon_features/modelpredictedlogits_trainDNN_alldata.csv')\n",
    " \n",
    "test_alldata_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "#train_realdist_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_realdist.csv')\n",
    "test_realdist_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "#train_balanced_dnnlogits = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_balanced.csv')\n",
    "test_balanced_dnnlogits = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_balanced.csv')\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd7e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4934d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa63f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alldata_dnnlogits['xlm_cosim'] = sim_lst_alldata\n",
    "test_realdist_dnnlogits['xlm_cosim'] = sim_lst_realdist\n",
    "test_balanced_dnnlogits['xlm_cosim'] = sim_lst_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cba58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>Weighted Feature Distance Div Maxlen</th>\n",
       "      <th>Partial Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>plain Levenshtein</th>\n",
       "      <th>label</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>DNNlogits_modelpredicted</th>\n",
       "      <th>m-bert_cosim</th>\n",
       "      <th>xlm_cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4202</td>\n",
       "      <td>cougar</td>\n",
       "      <td>cafard</td>\n",
       "      <td>ˈkugər</td>\n",
       "      <td>kafaʀ</td>\n",
       "      <td>cougar</td>\n",
       "      <td>cockroach</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>2.575000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.154968</td>\n",
       "      <td>0.554451</td>\n",
       "      <td>0.631995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>829</td>\n",
       "      <td>residue</td>\n",
       "      <td>résidu</td>\n",
       "      <td>ˈrɛzəˌdu</td>\n",
       "      <td>ʀezidy</td>\n",
       "      <td>residue</td>\n",
       "      <td>residue</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>2</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.845915</td>\n",
       "      <td>0.641440</td>\n",
       "      <td>0.420390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1984</td>\n",
       "      <td>galvanize</td>\n",
       "      <td>galvaniser</td>\n",
       "      <td>ˈgælvəˌnaɪz</td>\n",
       "      <td>ɡalvanizəʀ</td>\n",
       "      <td>galvanize</td>\n",
       "      <td>galvanize</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254167</td>\n",
       "      <td>2.162500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.857455</td>\n",
       "      <td>0.822515</td>\n",
       "      <td>0.890437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9659</td>\n",
       "      <td>angélique</td>\n",
       "      <td>amunition</td>\n",
       "      <td>angélique*</td>\n",
       "      <td>amynisjɔ̃</td>\n",
       "      <td>angelic</td>\n",
       "      <td>ammunition</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>8</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.178938</td>\n",
       "      <td>0.456421</td>\n",
       "      <td>0.733871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1181</td>\n",
       "      <td>deportment</td>\n",
       "      <td>déporter</td>\n",
       "      <td>dəˈpɔrtmənt</td>\n",
       "      <td>depɔʀtəʀ</td>\n",
       "      <td>deportment</td>\n",
       "      <td>deport</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.505590</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>0.707137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>1650</td>\n",
       "      <td>1650</td>\n",
       "      <td>10769</td>\n",
       "      <td>pendentive</td>\n",
       "      <td>chardon béni</td>\n",
       "      <td>pendentive*</td>\n",
       "      <td>ʃaʀdɔn beni</td>\n",
       "      <td>pendentive</td>\n",
       "      <td>blessed thistle</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>1.487500</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>10</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.449093</td>\n",
       "      <td>0.400846</td>\n",
       "      <td>0.670738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>1651</td>\n",
       "      <td>1651</td>\n",
       "      <td>6492</td>\n",
       "      <td>baba au rhum</td>\n",
       "      <td>demi-relief</td>\n",
       "      <td>ˈbəbə oʊ rhum*</td>\n",
       "      <td>dəmi-ʀɛljəf</td>\n",
       "      <td>rum baba</td>\n",
       "      <td>demi-relief</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>2.137500</td>\n",
       "      <td>0.122917</td>\n",
       "      <td>12</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.230941</td>\n",
       "      <td>0.419612</td>\n",
       "      <td>0.219390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>1652</td>\n",
       "      <td>1652</td>\n",
       "      <td>7753</td>\n",
       "      <td>forcené</td>\n",
       "      <td>pasquinade</td>\n",
       "      <td>forcené*é</td>\n",
       "      <td>paskinad</td>\n",
       "      <td>mad</td>\n",
       "      <td>pasquinade</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>1.921875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>9</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.169317</td>\n",
       "      <td>0.597328</td>\n",
       "      <td>0.699115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>1653</td>\n",
       "      <td>1653</td>\n",
       "      <td>8429</td>\n",
       "      <td>parfait</td>\n",
       "      <td>bastille</td>\n",
       "      <td>ˌpɑrˈfeɪ</td>\n",
       "      <td>bastij</td>\n",
       "      <td>perfect</td>\n",
       "      <td>bastille</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>7</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-13.261622</td>\n",
       "      <td>0.403259</td>\n",
       "      <td>0.461173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>1654</td>\n",
       "      <td>1654</td>\n",
       "      <td>9627</td>\n",
       "      <td>alert</td>\n",
       "      <td>énarque</td>\n",
       "      <td>əˈlərt</td>\n",
       "      <td>enaʀk</td>\n",
       "      <td>alert</td>\n",
       "      <td>enarque</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>6</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.621950</td>\n",
       "      <td>0.467537</td>\n",
       "      <td>0.758153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1655 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1     loan_word original_word  \\\n",
       "0              0             0            4202        cougar        cafard   \n",
       "1              1             1             829       residue        résidu   \n",
       "2              2             2            1984     galvanize    galvaniser   \n",
       "3              3             3            9659     angélique     amunition   \n",
       "4              4             4            1181    deportment      déporter   \n",
       "...          ...           ...             ...           ...           ...   \n",
       "1650        1650          1650           10769    pendentive  chardon béni   \n",
       "1651        1651          1651            6492  baba au rhum   demi-relief   \n",
       "1652        1652          1652            7753       forcené    pasquinade   \n",
       "1653        1653          1653            8429       parfait      bastille   \n",
       "1654        1654          1654            9627         alert       énarque   \n",
       "\n",
       "     loan_word_epitran original_word_epitran loan_english original_english  \\\n",
       "0               ˈkugər                 kafaʀ       cougar        cockroach   \n",
       "1             ˈrɛzəˌdu                ʀezidy      residue          residue   \n",
       "2          ˈgælvəˌnaɪz            ɡalvanizəʀ    galvanize        galvanize   \n",
       "3           angélique*             amynisjɔ̃      angelic       ammunition   \n",
       "4          dəˈpɔrtmənt              depɔʀtəʀ   deportment           deport   \n",
       "...                ...                   ...          ...              ...   \n",
       "1650       pendentive*           ʃaʀdɔn beni   pendentive  blessed thistle   \n",
       "1651    ˈbəbə oʊ rhum*           dəmi-ʀɛljəf     rum baba      demi-relief   \n",
       "1652         forcené*é              paskinad          mad       pasquinade   \n",
       "1653          ˌpɑrˈfeɪ                bastij      perfect         bastille   \n",
       "1654            əˈlərt                 enaʀk        alert          enarque   \n",
       "\n",
       "      Fast Levenshtein Distance Div Maxlen  ...  \\\n",
       "0                                 0.833333  ...   \n",
       "1                                 0.750000  ...   \n",
       "2                                 0.818182  ...   \n",
       "3                                 0.800000  ...   \n",
       "4                                 0.545455  ...   \n",
       "...                                    ...  ...   \n",
       "1650                              0.818182  ...   \n",
       "1651                              0.928571  ...   \n",
       "1652                              0.888889  ...   \n",
       "1653                              1.000000  ...   \n",
       "1654                              1.000000  ...   \n",
       "\n",
       "      Hamming Feature Distance Div Maxlen  \\\n",
       "0                                0.316667   \n",
       "1                                0.076389   \n",
       "2                                0.254167   \n",
       "3                                0.125000   \n",
       "4                                0.262500   \n",
       "...                                   ...   \n",
       "1650                             0.120833   \n",
       "1651                             0.141667   \n",
       "1652                             0.114583   \n",
       "1653                             0.125000   \n",
       "1654                             0.158333   \n",
       "\n",
       "      Weighted Feature Distance Div Maxlen  \\\n",
       "0                                 2.575000   \n",
       "1                                 0.645833   \n",
       "2                                 2.162500   \n",
       "3                                 1.609375   \n",
       "4                                 2.050000   \n",
       "...                                    ...   \n",
       "1650                              1.487500   \n",
       "1651                              2.137500   \n",
       "1652                              1.921875   \n",
       "1653                              1.562500   \n",
       "1654                              1.600000   \n",
       "\n",
       "      Partial Hamming Feature Distance Div Maxlen  plain Levenshtein  \\\n",
       "0                                        0.300000                  4   \n",
       "1                                        0.062500                  2   \n",
       "2                                        0.250000                  2   \n",
       "3                                        0.114583                  8   \n",
       "4                                        0.250000                  4   \n",
       "...                                           ...                ...   \n",
       "1650                                     0.108333                 10   \n",
       "1651                                     0.122917                 12   \n",
       "1652                                     0.093750                  9   \n",
       "1653                                     0.093750                  7   \n",
       "1654                                     0.137500                  6   \n",
       "\n",
       "              label  label_bin Unnamed: 0.1.1.1  DNNlogits_modelpredicted  \\\n",
       "0     hard_negative          0              NaN                 -5.154968   \n",
       "1           synonym          0              NaN                  2.845915   \n",
       "2              loan          1              NaN                 -1.857455   \n",
       "3     hard_negative          0              NaN                 -9.178938   \n",
       "4     hard_negative          0              NaN                 -3.505590   \n",
       "...             ...        ...              ...                       ...   \n",
       "1650  hard_negative          0              NaN                -18.449093   \n",
       "1651  hard_negative          0              NaN                 -8.230941   \n",
       "1652  hard_negative          0              NaN                -10.169317   \n",
       "1653  hard_negative          0              NaN                -13.261622   \n",
       "1654  hard_negative          0              NaN                 -3.621950   \n",
       "\n",
       "      m-bert_cosim  xlm_cosim  \n",
       "0         0.554451   0.631995  \n",
       "1         0.641440   0.420390  \n",
       "2         0.822515   0.890437  \n",
       "3         0.456421   0.733871  \n",
       "4         0.701201   0.707137  \n",
       "...            ...        ...  \n",
       "1650      0.400846   0.670738  \n",
       "1651      0.419612   0.219390  \n",
       "1652      0.597328   0.699115  \n",
       "1653      0.403259   0.461173  \n",
       "1654      0.467537   0.758153  \n",
       "\n",
       "[1655 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_alldata_dnnlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8b60aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes as csv files \n",
    "\n",
    "test_alldata_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_alldata.csv')\n",
    "test_realdist_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_realdist.csv')\n",
    "test_balanced_dnnlogits.to_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_balanced.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a155c0",
   "metadata": {},
   "source": [
    "# Hindi -Persian Evaluation of the phonetic, edit distance and logits as features for our classifiers for the three datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff80af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fe33e",
   "metadata": {},
   "source": [
    "## all data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d11b91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #for Hindi Persian \n",
    "# test_alldata = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_cosims_alldata.csv', index_col=[0])\n",
    "# train_alldata = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_cosims_alldata.csv', index_col=[0])\n",
    " \n",
    "# test_realdist = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_cosims_realdist.csv', index_col=[0])\n",
    "# train_realdist = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_cosims_realdist.csv', index_col=[0])\n",
    "\n",
    "# train_balanced = pd.read_csv('../Datasets/modelpredictedlogits_trainDNN_cosims_balanced.csv', index_col=[0])\n",
    "# test_balanced = pd.read_csv('../Datasets/modelpredictedlogits_testDNN_cosims_balanced.csv', index_col=[0])\n",
    "\n",
    "# for English French \n",
    "test_alldata = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_alldata.csv', index_col=[0])\n",
    "train_alldata = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_alldata.csv', index_col=[0])\n",
    " \n",
    "test_realdist = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_realdist.csv', index_col=[0])\n",
    "train_realdist = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_realdist.csv', index_col=[0])\n",
    "\n",
    "train_balanced = pd.read_csv('../Datasets/English-French-modelpredictedlogits_trainDNN_cosims_balanced.csv', index_col=[0])\n",
    "test_balanced = pd.read_csv('../Datasets/English-French-modelpredictedlogits_testDNN_cosims_balanced.csv', index_col=[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd933f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1655, 22), (14876, 21), (509, 22), (6156, 21), (5502, 21), (436, 22))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the extra column in the shape is due to an extra column index, \n",
    "test_alldata.shape,train_alldata.shape, test_realdist.shape, train_realdist.shape, train_balanced.shape, test_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24984790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature Edit Distance Div Maxlen</th>\n",
       "      <th>Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>Weighted Feature Distance Div Maxlen</th>\n",
       "      <th>Partial Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>plain Levenshtein</th>\n",
       "      <th>label</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>DNNlogits_modelpredicted</th>\n",
       "      <th>m-bert_cosim</th>\n",
       "      <th>xlm_cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2180</td>\n",
       "      <td>नौकर</td>\n",
       "      <td>ناهار</td>\n",
       "      <td>nɔːkər</td>\n",
       "      <td>nɒhɒr</td>\n",
       "      <td>Servant</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>5</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.632645</td>\n",
       "      <td>0.651157</td>\n",
       "      <td>0.698117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2185</td>\n",
       "      <td>प्रभाव</td>\n",
       "      <td>نتیجه</td>\n",
       "      <td>prəb̤aːv</td>\n",
       "      <td>ntjd͡ʒh</td>\n",
       "      <td>Effect</td>\n",
       "      <td>Result</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.336806</td>\n",
       "      <td>6</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.517828</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>0.722407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3050</td>\n",
       "      <td>माही</td>\n",
       "      <td>ماب</td>\n",
       "      <td>maːɦiː</td>\n",
       "      <td>mɒb</td>\n",
       "      <td>lover</td>\n",
       "      <td>map</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.893986</td>\n",
       "      <td>0.676401</td>\n",
       "      <td>0.534651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>818</td>\n",
       "      <td>बरामद</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>bəraːməd</td>\n",
       "      <td>rhɒjʔj</td>\n",
       "      <td>found</td>\n",
       "      <td>رهائی</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315476</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>4.607143</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.171056</td>\n",
       "      <td>0.478386</td>\n",
       "      <td>0.650366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2571</td>\n",
       "      <td>बराबर</td>\n",
       "      <td>بارآور</td>\n",
       "      <td>bəraːbər</td>\n",
       "      <td>bɒrɒvr</td>\n",
       "      <td>Equal</td>\n",
       "      <td>Fertile</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172619</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>1.589286</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>6</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.876501</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>0.447921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1315</td>\n",
       "      <td>1315</td>\n",
       "      <td>3170</td>\n",
       "      <td>वाहवाही</td>\n",
       "      <td>تشویق و تمجید</td>\n",
       "      <td>vaːɦvaːɦiː</td>\n",
       "      <td>tʃvjɣ v tmd͡ʒjd</td>\n",
       "      <td>Praise</td>\n",
       "      <td>Applause</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422348</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>4.568182</td>\n",
       "      <td>0.452652</td>\n",
       "      <td>13</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-16.205833</td>\n",
       "      <td>0.523541</td>\n",
       "      <td>0.581920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1316</td>\n",
       "      <td>1316</td>\n",
       "      <td>4560</td>\n",
       "      <td>नुकसान पहुचने वाला</td>\n",
       "      <td>خطرناک</td>\n",
       "      <td>nuksaːnə pəɦut͡ʃne vaːlaː</td>\n",
       "      <td>xtrnɒk</td>\n",
       "      <td>Harmful</td>\n",
       "      <td>Dangerous</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.702546</td>\n",
       "      <td>18</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-39.144264</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.202958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>1317</td>\n",
       "      <td>1317</td>\n",
       "      <td>140</td>\n",
       "      <td>कबाब</td>\n",
       "      <td>کباب</td>\n",
       "      <td>kəbaːb</td>\n",
       "      <td>kbɒb</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>Kebab</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>4</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>4.328829</td>\n",
       "      <td>0.677797</td>\n",
       "      <td>0.805250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1318</td>\n",
       "      <td>1318</td>\n",
       "      <td>4493</td>\n",
       "      <td>अटल</td>\n",
       "      <td>یک دنده</td>\n",
       "      <td>aʈəl</td>\n",
       "      <td>jk dndh</td>\n",
       "      <td>Firm</td>\n",
       "      <td>stubborn</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413194</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>7</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.486457</td>\n",
       "      <td>0.437662</td>\n",
       "      <td>0.476721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1319</td>\n",
       "      <td>1319</td>\n",
       "      <td>663</td>\n",
       "      <td>नवाज़</td>\n",
       "      <td>نواز</td>\n",
       "      <td>nəvaːz</td>\n",
       "      <td>nvɒz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>Nawaz</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>5</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>6.445583</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.467944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1           loan_word  \\\n",
       "0                0               0              2180                नौकर   \n",
       "1                1               1              2185              प्रभाव   \n",
       "2                2               2              3050                माही   \n",
       "3                3               3               818               बरामद   \n",
       "4                4               4              2571               बराबर   \n",
       "...            ...             ...               ...                 ...   \n",
       "1315          1315            1315              3170             वाहवाही   \n",
       "1316          1316            1316              4560  नुकसान पहुचने वाला   \n",
       "1317          1317            1317               140                कबाब   \n",
       "1318          1318            1318              4493                 अटल   \n",
       "1319          1319            1319               663               नवाज़   \n",
       "\n",
       "      original_word          loan_word_epitran original_word_epitran  \\\n",
       "0             ناهار                     nɔːkər                 nɒhɒr   \n",
       "1             نتیجه                   prəb̤aːv               ntjd͡ʒh   \n",
       "2               ماب                     maːɦiː                   mɒb   \n",
       "3             رهائی                   bəraːməd                rhɒjʔj   \n",
       "4            بارآور                   bəraːbər                bɒrɒvr   \n",
       "...             ...                        ...                   ...   \n",
       "1315  تشویق و تمجید                 vaːɦvaːɦiː       tʃvjɣ v tmd͡ʒjd   \n",
       "1316         خطرناک  nuksaːnə pəɦut͡ʃne vaːlaː                xtrnɒk   \n",
       "1317           کباب                     kəbaːb                  kbɒb   \n",
       "1318        یک دنده                       aʈəl               jk dndh   \n",
       "1319           نواز                     nəvaːz                  nvɒz   \n",
       "\n",
       "     loan_english original_english  Fast Levenshtein Distance Div Maxlen  ...  \\\n",
       "0         Servant            Lunch                              0.666667  ...   \n",
       "1          Effect           Result                              1.000000  ...   \n",
       "2           lover              map                              0.833333  ...   \n",
       "3           found            رهائی                              0.875000  ...   \n",
       "4           Equal          Fertile                              0.625000  ...   \n",
       "...           ...              ...                                   ...  ...   \n",
       "1315       Praise         Applause                              0.866667  ...   \n",
       "1316      Harmful        Dangerous                              0.920000  ...   \n",
       "1317        Kebab            Kebab                              0.500000  ...   \n",
       "1318         Firm         stubborn                              1.000000  ...   \n",
       "1319        Nawaz            Nawaz                              0.500000  ...   \n",
       "\n",
       "      Feature Edit Distance Div Maxlen  Hamming Feature Distance Div Maxlen  \\\n",
       "0                             0.083333                             0.083333   \n",
       "1                             0.319444                             0.368056   \n",
       "2                             0.296875                             0.322917   \n",
       "3                             0.315476                             0.369048   \n",
       "4                             0.172619                             0.190476   \n",
       "...                                ...                                  ...   \n",
       "1315                          0.422348                             0.465909   \n",
       "1316                          0.636574                             0.708333   \n",
       "1317                          0.204167                             0.225000   \n",
       "1318                          0.413194                             0.465278   \n",
       "1319                          0.204167                             0.225000   \n",
       "\n",
       "      Weighted Feature Distance Div Maxlen  \\\n",
       "0                                 1.300000   \n",
       "1                                 4.083333   \n",
       "2                                 3.187500   \n",
       "3                                 4.607143   \n",
       "4                                 1.589286   \n",
       "...                                    ...   \n",
       "1315                              4.568182   \n",
       "1316                              5.333333   \n",
       "1317                              1.750000   \n",
       "1318                              4.583333   \n",
       "1319                              1.750000   \n",
       "\n",
       "      Partial Hamming Feature Distance Div Maxlen  plain Levenshtein  \\\n",
       "0                                        0.083333                  5   \n",
       "1                                        0.336806                  6   \n",
       "2                                        0.322917                  4   \n",
       "3                                        0.339286                  5   \n",
       "4                                        0.187500                  6   \n",
       "...                                           ...                ...   \n",
       "1315                                     0.452652                 13   \n",
       "1316                                     0.702546                 18   \n",
       "1317                                     0.225000                  4   \n",
       "1318                                     0.447917                  7   \n",
       "1319                                     0.225000                  5   \n",
       "\n",
       "              label label_bin  DNNlogits_modelpredicted  m-bert_cosim  \\\n",
       "0     hard_negative         0                 -6.632645      0.651157   \n",
       "1           synonym         0                -12.517828      0.388871   \n",
       "2     hard_negative         0                 -4.893986      0.676401   \n",
       "3            random         0                 -4.171056      0.478386   \n",
       "4     hard_negative         0                 -8.876501      0.472573   \n",
       "...             ...       ...                       ...           ...   \n",
       "1315        synonym         0                -16.205833      0.523541   \n",
       "1316        synonym         0                -39.144264      0.398689   \n",
       "1317           loan         1                  4.328829      0.677797   \n",
       "1318        synonym         0                 -5.486457      0.437662   \n",
       "1319           loan         1                  6.445583      0.555900   \n",
       "\n",
       "      xlm_cosim  \n",
       "0      0.698117  \n",
       "1      0.722407  \n",
       "2      0.534651  \n",
       "3      0.650366  \n",
       "4      0.447921  \n",
       "...         ...  \n",
       "1315   0.581920  \n",
       "1316   0.202958  \n",
       "1317   0.805250  \n",
       "1318   0.476721  \n",
       "1319   0.467944  \n",
       "\n",
       "[1320 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e551954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization of logits is optional, have to dig into this more. \n",
    "\n",
    "# train_alldata['DNNlogits_modelpredicted'] = (train_alldata['DNNlogits_modelpredicted']-train_alldata['DNNlogits_modelpredicted'].min()) / (train_alldata['DNN_logits'].max()-train_alldata['DNNlogits_modelpredicted'].min())\n",
    "# test_alldata['DNNlogits_modelpredicted'] = (test_alldata['DNNlogits_modelpredicted']-test_alldata['DNNlogits_modelpredicted'].min()) / (test_alldata['DNNlogits_modelpredicted'].max()-test_alldata['DNNlogits_modelpredicted'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f613dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_all =  ['Fast Levenshtein Distance Div Maxlen',\n",
    "       'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen',\n",
    "       'Hamming Feature Distance Div Maxlen',\n",
    "       'Weighted Feature Distance Div Maxlen',\n",
    "       'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein' ,'DNNlogits_modelpredicted','m-bert_cosim', 'xlm_cosim',\n",
    "           \n",
    "        ]\n",
    "\n",
    "features_nologits =  ['Fast Levenshtein Distance Div Maxlen',\n",
    "       'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen',\n",
    "       'Hamming Feature Distance Div Maxlen',\n",
    "       'Weighted Feature Distance Div Maxlen',\n",
    "       'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein' ,'m-bert_cosim', 'xlm_cosim',\n",
    "           \n",
    "        ]\n",
    "\n",
    "features_logits_nocosine =['Fast Levenshtein Distance Div Maxlen',\n",
    "       'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen',\n",
    "       'Hamming Feature Distance Div Maxlen',\n",
    "       'Weighted Feature Distance Div Maxlen',\n",
    "       'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein' ,'DNNlogits_modelpredicted', \n",
    "           \n",
    "        ]\n",
    "\n",
    "edit_features = ['Fast Levenshtein Distance Div Maxlen',\n",
    "       'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen',\n",
    "       'Hamming Feature Distance Div Maxlen',\n",
    "       'Weighted Feature Distance Div Maxlen',\n",
    "       'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein' , \n",
    "           \n",
    "        ]\n",
    "\n",
    "features_plainLV_cosim = [  'plain Levenshtein' ,'m-bert_cosim', 'xlm_cosim', \n",
    "           \n",
    "        ]\n",
    "\n",
    "features_logits_plainLV_cosim = [  'plain Levenshtein' ,'m-bert_cosim', 'xlm_cosim', 'DNNlogits_modelpredicted'\n",
    "           \n",
    "        ]\n",
    "\n",
    "\n",
    "features_cosim_logits = ['m-bert_cosim', 'xlm_cosim', 'DNNlogits_modelpredicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc94ec",
   "metadata": {},
   "source": [
    "## all features includes cosim and logits for all three data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b519beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['label_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242f23b",
   "metadata": {},
   "source": [
    "## alldata evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0b60615",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_all].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26bbdb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11857, 10), (11857,), (1320, 10), (1320,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96e8ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99594887",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8bf5c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.7547169811320754\n",
      "precision :  0.7692307692307693\n",
      "recall :  0.7407407407407407\n",
      "accuracy :  0.9507575757575758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1185\n",
      "           1       0.77      0.74      0.75       135\n",
      "\n",
      "    accuracy                           0.95      1320\n",
      "   macro avg       0.87      0.86      0.86      1320\n",
      "weighted avg       0.95      0.95      0.95      1320\n",
      "\n",
      "[[1155   30]\n",
      " [  35  100]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b9ba6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_pred, y_test, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66da493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore: [0.97263158 0.75471698]\n",
      "precision: [0.97468354 0.74074074]\n",
      "recall: [0.97058824 0.76923077]\n",
      "accuracy :  0.9507575757575758\n",
      "support: [1190  130]\n"
     ]
    }
   ],
   "source": [
    "print('fscore: {}'.format(fscore))\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79045374",
   "metadata": {},
   "source": [
    "## real distribution , all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bbde83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_realdist[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[features_all].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a23614d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4030, 10), (4030,), (450, 10), (450,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3feddf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c0c186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5341a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9307692307692307\n",
      "precision :  0.968\n",
      "recall :  0.8962962962962963\n",
      "accuracy :  0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       315\n",
      "           1       0.97      0.90      0.93       135\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.96      0.94      0.95       450\n",
      "weighted avg       0.96      0.96      0.96       450\n",
      "\n",
      "[[311   4]\n",
      " [ 14 121]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35cd15",
   "metadata": {},
   "source": [
    "## balanced , all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a8a0c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_balanced[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[features_all].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9952c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 10), (2417,), (271, 10), (271,))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3840d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9e583e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a5c3d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9461538461538462\n",
      "precision :  0.984\n",
      "recall :  0.9111111111111111\n",
      "accuracy :  0.948339483394834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       136\n",
      "           1       0.98      0.91      0.95       135\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.95      0.95      0.95       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "[[134   2]\n",
      " [ 12 123]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44bbc9",
   "metadata": {},
   "source": [
    "## try an SVM classifier for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5278b878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "00ae037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d72e5659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9498069498069498\n",
      "precision :  0.9919354838709677\n",
      "recall :  0.9111111111111111\n",
      "accuracy :  0.9520295202952029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       136\n",
      "           1       0.99      0.91      0.95       135\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.96      0.95      0.95       271\n",
      "weighted avg       0.96      0.95      0.95       271\n",
      "\n",
      "[[135   1]\n",
      " [ 12 123]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f52e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4464cef",
   "metadata": {},
   "source": [
    "## features_logits_nocosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3506b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_logits_nocosine].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_logits_nocosine].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4592c5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11857, 8), (11857,), (1320, 8), (1320,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75fac186",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c2695e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13548f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.7547169811320754\n",
      "precision :  0.7692307692307693\n",
      "recall :  0.7407407407407407\n",
      "accuracy :  0.9507575757575758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1185\n",
      "           1       0.77      0.74      0.75       135\n",
      "\n",
      "    accuracy                           0.95      1320\n",
      "   macro avg       0.87      0.86      0.86      1320\n",
      "weighted avg       0.95      0.95      0.95      1320\n",
      "\n",
      "[[1155   30]\n",
      " [  35  100]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6b2bc",
   "metadata": {},
   "source": [
    "## real dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f8980f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_realdist[features_logits_nocosine].values \n",
    "\n",
    "\n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[features_logits_nocosine].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d812040",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7e8bfc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4248f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9302325581395349\n",
      "precision :  0.975609756097561\n",
      "recall :  0.8888888888888888\n",
      "accuracy :  0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       315\n",
      "           1       0.98      0.89      0.93       135\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.96      0.94      0.95       450\n",
      "weighted avg       0.96      0.96      0.96       450\n",
      "\n",
      "[[312   3]\n",
      " [ 15 120]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bafba4",
   "metadata": {},
   "source": [
    "## balanced split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bb3dcabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_balanced[features_logits_nocosine].values \n",
    "\n",
    "\n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[features_logits_nocosine].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac980b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 8), (2417,), (271, 8), (271,))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "552402bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a88fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "756cf52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9461538461538462\n",
      "precision :  0.984\n",
      "recall :  0.9111111111111111\n",
      "accuracy :  0.948339483394834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       136\n",
      "           1       0.98      0.91      0.95       135\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.95      0.95      0.95       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "[[134   2]\n",
      " [ 12 123]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bc304",
   "metadata": {},
   "source": [
    "## Features with no logits but with cos sims, all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "62e025f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_nologits].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_nologits].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "75c7a560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11857, 9), (11857,), (1320, 9), (1320,))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e03120b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "90b4f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f5aa2e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.6\n",
      "precision :  0.6857142857142857\n",
      "recall :  0.5333333333333333\n",
      "accuracy :  0.9272727272727272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1185\n",
      "           1       0.69      0.53      0.60       135\n",
      "\n",
      "    accuracy                           0.93      1320\n",
      "   macro avg       0.82      0.75      0.78      1320\n",
      "weighted avg       0.92      0.93      0.92      1320\n",
      "\n",
      "[[1152   33]\n",
      " [  63   72]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a341bca",
   "metadata": {},
   "source": [
    "## real dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8ccd529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_realdist[features_nologits].values \n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[features_nologits].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73c1aab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4030, 9), (4030,), (450, 9), (450,))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2a19f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c622df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = LogisticRegression(random_state=1).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c3cc1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0029d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = LR2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "95729ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8498168498168497\n",
      "precision :  0.8405797101449275\n",
      "recall :  0.8592592592592593\n",
      "accuracy :  0.9088888888888889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       315\n",
      "           1       0.84      0.86      0.85       135\n",
      "\n",
      "    accuracy                           0.91       450\n",
      "   macro avg       0.89      0.89      0.89       450\n",
      "weighted avg       0.91      0.91      0.91       450\n",
      "\n",
      "[[293  22]\n",
      " [ 19 116]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "eeb32b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8498168498168497\n",
      "precision :  0.8405797101449275\n",
      "recall :  0.8592592592592593\n",
      "accuracy :  0.9088888888888889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       315\n",
      "           1       0.84      0.86      0.85       135\n",
      "\n",
      "    accuracy                           0.91       450\n",
      "   macro avg       0.89      0.89      0.89       450\n",
      "weighted avg       0.91      0.91      0.91       450\n",
      "\n",
      "[[293  22]\n",
      " [ 19 116]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred2 ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred2))\n",
    "print(\"recall : \",recall_score(y_test, y_pred2 )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec9d95",
   "metadata": {},
   "source": [
    "## balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4e6b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_balanced[features_nologits].values \n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[features_nologits].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "44090c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 9), (2417,), (271, 9), (271,))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d8571f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8383c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "19e87f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9259259259259259\n",
      "precision :  0.9259259259259259\n",
      "recall :  0.9259259259259259\n",
      "accuracy :  0.9261992619926199\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       136\n",
      "           1       0.93      0.93      0.93       135\n",
      "\n",
      "    accuracy                           0.93       271\n",
      "   macro avg       0.93      0.93      0.93       271\n",
      "weighted avg       0.93      0.93      0.93       271\n",
      "\n",
      "[[126  10]\n",
      " [ 10 125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01ea08",
   "metadata": {},
   "source": [
    "## only edit_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "97a497c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9d109f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[edit_features].values \n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[edit_features].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f4b084ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11857, 7), (11857,), (1320, 7), (1320,))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b2b658d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "44154a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "03f93f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.5975103734439834\n",
      "precision :  0.6792452830188679\n",
      "recall :  0.5333333333333333\n",
      "accuracy :  0.9265151515151515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1185\n",
      "           1       0.68      0.53      0.60       135\n",
      "\n",
      "    accuracy                           0.93      1320\n",
      "   macro avg       0.81      0.75      0.78      1320\n",
      "weighted avg       0.92      0.93      0.92      1320\n",
      "\n",
      "[[1151   34]\n",
      " [  63   72]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae3693b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real dist\n",
    "\n",
    "x_train = train_realdist[edit_features].values \n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[edit_features].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f9ad4c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4030, 7), (4030,), (450, 7), (450,))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b41b2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "585dbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2d5e0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8581818181818183\n",
      "precision :  0.8428571428571429\n",
      "recall :  0.8740740740740741\n",
      "accuracy :  0.9133333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       315\n",
      "           1       0.84      0.87      0.86       135\n",
      "\n",
      "    accuracy                           0.91       450\n",
      "   macro avg       0.89      0.90      0.90       450\n",
      "weighted avg       0.91      0.91      0.91       450\n",
      "\n",
      "[[293  22]\n",
      " [ 17 118]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "dda5af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_balanced[edit_features].values \n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[edit_features].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2d3c4505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 7), (2417,), (271, 7), (271,))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "58cad339",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cba2473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "042b730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.929368029739777\n",
      "precision :  0.9328358208955224\n",
      "recall :  0.9259259259259259\n",
      "accuracy :  0.9298892988929889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       136\n",
      "           1       0.93      0.93      0.93       135\n",
      "\n",
      "    accuracy                           0.93       271\n",
      "   macro avg       0.93      0.93      0.93       271\n",
      "weighted avg       0.93      0.93      0.93       271\n",
      "\n",
      "[[127   9]\n",
      " [ 10 125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621f586",
   "metadata": {},
   "source": [
    "# For English-French pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ffad13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>Dolgo Prime Distance Div Maxlen</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>Weighted Feature Distance Div Maxlen</th>\n",
       "      <th>Partial Hamming Feature Distance Div Maxlen</th>\n",
       "      <th>plain Levenshtein</th>\n",
       "      <th>label</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>DNNlogits_modelpredicted</th>\n",
       "      <th>m-bert_cosim</th>\n",
       "      <th>xlm_cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2485</td>\n",
       "      <td>maquis</td>\n",
       "      <td>maquis</td>\n",
       "      <td>maquis*</td>\n",
       "      <td>maki</td>\n",
       "      <td>maquis</td>\n",
       "      <td>maquis</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340278</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.340278</td>\n",
       "      <td>0</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.269493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7720</td>\n",
       "      <td>fin de siècle</td>\n",
       "      <td>dépréciation</td>\n",
       "      <td>fɪn də siècle*</td>\n",
       "      <td>depʀesjasjɔ̃</td>\n",
       "      <td>end of century</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>2.193182</td>\n",
       "      <td>0.142045</td>\n",
       "      <td>13</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-13.580923</td>\n",
       "      <td>0.365554</td>\n",
       "      <td>0.348418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3292</td>\n",
       "      <td>roman à thèse</td>\n",
       "      <td>roman à thèse</td>\n",
       "      <td>ˈroʊmən à* thèse*</td>\n",
       "      <td>ʀɔman a tə̀s</td>\n",
       "      <td>thesis novel</td>\n",
       "      <td>thesis novel</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.156250</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.820937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2151</td>\n",
       "      <td>hausse</td>\n",
       "      <td>hausse</td>\n",
       "      <td>hausse*</td>\n",
       "      <td>os</td>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0</td>\n",
       "      <td>loan</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.353779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4628</td>\n",
       "      <td>enfile</td>\n",
       "      <td>aspirant</td>\n",
       "      <td>enfile*</td>\n",
       "      <td>aspiʀɑ̃</td>\n",
       "      <td>put on</td>\n",
       "      <td>aspirant</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>7</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.874206</td>\n",
       "      <td>0.418582</td>\n",
       "      <td>0.607021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14871</th>\n",
       "      <td>14871</td>\n",
       "      <td>795</td>\n",
       "      <td>chichi</td>\n",
       "      <td>effigie</td>\n",
       "      <td>ˈʧiʧi</td>\n",
       "      <td>ɛfiʒi</td>\n",
       "      <td>father</td>\n",
       "      <td>effigy</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.745490</td>\n",
       "      <td>0.487566</td>\n",
       "      <td>0.679311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14872</th>\n",
       "      <td>14872</td>\n",
       "      <td>9423</td>\n",
       "      <td>lorgnon</td>\n",
       "      <td>jornel</td>\n",
       "      <td>lorgnon*</td>\n",
       "      <td>ʒɔʀnəl</td>\n",
       "      <td>lorgnon</td>\n",
       "      <td>jornel</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.354167</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.068455</td>\n",
       "      <td>0.500577</td>\n",
       "      <td>0.711260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14873</th>\n",
       "      <td>14873</td>\n",
       "      <td>5524</td>\n",
       "      <td>pas de chat</td>\n",
       "      <td>pastillage</td>\n",
       "      <td>pɑz də ʧæt</td>\n",
       "      <td>pastijaʒ</td>\n",
       "      <td>no cat</td>\n",
       "      <td>pastillage</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>8</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.408354</td>\n",
       "      <td>0.628722</td>\n",
       "      <td>0.582361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874</th>\n",
       "      <td>14874</td>\n",
       "      <td>9510</td>\n",
       "      <td>publicist</td>\n",
       "      <td>conclaviste</td>\n",
       "      <td>ˈpəblɪsɪst</td>\n",
       "      <td>kɔ̃klavist</td>\n",
       "      <td>publicist</td>\n",
       "      <td>conclaved</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.087963</td>\n",
       "      <td>7</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.373606</td>\n",
       "      <td>0.392996</td>\n",
       "      <td>0.804365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14875</th>\n",
       "      <td>14875</td>\n",
       "      <td>2264</td>\n",
       "      <td>muscat</td>\n",
       "      <td>mascotte</td>\n",
       "      <td>ˈməskæt</td>\n",
       "      <td>maskɔt</td>\n",
       "      <td>muscat</td>\n",
       "      <td>mascot</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>4</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.971308</td>\n",
       "      <td>0.432525</td>\n",
       "      <td>0.817558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14876 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0.1.1      loan_word  original_word  \\\n",
       "0                 0            2485         maquis         maquis   \n",
       "1                 1            7720  fin de siècle   dépréciation   \n",
       "2                 2            3292  roman à thèse  roman à thèse   \n",
       "3                 3            2151         hausse         hausse   \n",
       "4                 4            4628         enfile       aspirant   \n",
       "...             ...             ...            ...            ...   \n",
       "14871         14871             795         chichi        effigie   \n",
       "14872         14872            9423        lorgnon         jornel   \n",
       "14873         14873            5524    pas de chat     pastillage   \n",
       "14874         14874            9510      publicist    conclaviste   \n",
       "14875         14875            2264         muscat       mascotte   \n",
       "\n",
       "       loan_word_epitran original_word_epitran    loan_english  \\\n",
       "0                maquis*                  maki          maquis   \n",
       "1         fɪn də siècle*          depʀesjasjɔ̃  end of century   \n",
       "2      ˈroʊmən à* thèse*          ʀɔman a tə̀s    thesis novel   \n",
       "3                hausse*                    os            rise   \n",
       "4                enfile*               aspiʀɑ̃          put on   \n",
       "...                  ...                   ...             ...   \n",
       "14871              ˈʧiʧi                 ɛfiʒi          father   \n",
       "14872           lorgnon*                ʒɔʀnəl         lorgnon   \n",
       "14873         pɑz də ʧæt              pastijaʒ          no cat   \n",
       "14874         ˈpəblɪsɪst            kɔ̃klavist       publicist   \n",
       "14875            ˈməskæt                maskɔt          muscat   \n",
       "\n",
       "      original_english  Fast Levenshtein Distance Div Maxlen  \\\n",
       "0               maquis                              0.571429   \n",
       "1         depreciation                              0.928571   \n",
       "2         thesis novel                              0.647059   \n",
       "3                 rise                              0.857143   \n",
       "4             aspirant                              0.857143   \n",
       "...                ...                                   ...   \n",
       "14871           effigy                              0.600000   \n",
       "14872           jornel                              0.875000   \n",
       "14873       pastillage                              0.900000   \n",
       "14874        conclaved                              0.700000   \n",
       "14875           mascot                              0.428571   \n",
       "\n",
       "       Dolgo Prime Distance Div Maxlen  ...  \\\n",
       "0                             0.333333  ...   \n",
       "1                             0.454545  ...   \n",
       "2                             0.400000  ...   \n",
       "3                             0.666667  ...   \n",
       "4                             0.166667  ...   \n",
       "...                                ...  ...   \n",
       "14871                         0.600000  ...   \n",
       "14872                         0.333333  ...   \n",
       "14873                         0.125000  ...   \n",
       "14874                         0.333333  ...   \n",
       "14875                         0.000000  ...   \n",
       "\n",
       "       Hamming Feature Distance Div Maxlen  \\\n",
       "0                                 0.340278   \n",
       "1                                 0.159091   \n",
       "2                                 0.291667   \n",
       "3                                 0.680556   \n",
       "4                                 0.118056   \n",
       "...                                    ...   \n",
       "14871                             0.600000   \n",
       "14872                             0.111111   \n",
       "14873                             0.182292   \n",
       "14874                             0.092593   \n",
       "14875                             0.048611   \n",
       "\n",
       "       Weighted Feature Distance Div Maxlen  \\\n",
       "0                                  2.500000   \n",
       "1                                  2.193182   \n",
       "2                                  2.156250   \n",
       "3                                  5.000000   \n",
       "4                                  1.583333   \n",
       "...                                     ...   \n",
       "14871                              4.350000   \n",
       "14872                              1.354167   \n",
       "14873                              1.468750   \n",
       "14874                              0.916667   \n",
       "14875                              0.500000   \n",
       "\n",
       "       Partial Hamming Feature Distance Div Maxlen  plain Levenshtein  \\\n",
       "0                                         0.340278                  0   \n",
       "1                                         0.142045                 13   \n",
       "2                                         0.284722                  0   \n",
       "3                                         0.680556                  0   \n",
       "4                                         0.104167                  7   \n",
       "...                                            ...                ...   \n",
       "14871                                     0.600000                  6   \n",
       "14872                                     0.097222                  4   \n",
       "14873                                     0.179688                  8   \n",
       "14874                                     0.087963                  7   \n",
       "14875                                     0.048611                  4   \n",
       "\n",
       "               label label_bin  Unnamed: 0.1.1.1  DNNlogits_modelpredicted  \\\n",
       "0               loan         1               NaN                  4.269493   \n",
       "1      hard_negative         0               NaN                -13.580923   \n",
       "2               loan         1               NaN                  4.820937   \n",
       "3               loan         1               NaN                  4.353779   \n",
       "4      hard_negative         0               NaN                 -5.874206   \n",
       "...              ...       ...               ...                       ...   \n",
       "14871  hard_negative         0               NaN                 -4.745490   \n",
       "14872  hard_negative         0               NaN                 -4.068455   \n",
       "14873  hard_negative         0               NaN                 -4.408354   \n",
       "14874  hard_negative         0               NaN                 -5.373606   \n",
       "14875  hard_negative         0               NaN                 -2.971308   \n",
       "\n",
       "       m-bert_cosim  xlm_cosim  \n",
       "0          1.000000   1.000000  \n",
       "1          0.365554   0.348418  \n",
       "2          1.000000   0.999999  \n",
       "3          1.000000   1.000000  \n",
       "4          0.418582   0.607021  \n",
       "...             ...        ...  \n",
       "14871      0.487566   0.679311  \n",
       "14872      0.500577   0.711260  \n",
       "14873      0.628722   0.582361  \n",
       "14874      0.392996   0.804365  \n",
       "14875      0.432525   0.817558  \n",
       "\n",
       "[14876 rows x 21 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_alldata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cdaf2",
   "metadata": {},
   "source": [
    "## alldata English-French, all features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7b7ae7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_all].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "66614c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14876, 10), (14876,), (1655, 10), (1655,))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "036e62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "878a0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "456d264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.7777777777777778\n",
      "precision :  0.8061797752808989\n",
      "recall :  0.7513089005235603\n",
      "accuracy :  0.9009063444108761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1273\n",
      "           1       0.81      0.75      0.78       382\n",
      "\n",
      "    accuracy                           0.90      1655\n",
      "   macro avg       0.87      0.85      0.86      1655\n",
      "weighted avg       0.90      0.90      0.90      1655\n",
      "\n",
      "[[1204   69]\n",
      " [  95  287]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9aa541ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_realdist[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[features_all].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "adce947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "dcf1b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e2a5f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8863309352517985\n",
      "precision :  0.9840255591054313\n",
      "recall :  0.806282722513089\n",
      "accuracy :  0.8447937131630648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.96      0.76       127\n",
      "           1       0.98      0.81      0.89       382\n",
      "\n",
      "    accuracy                           0.84       509\n",
      "   macro avg       0.80      0.88      0.82       509\n",
      "weighted avg       0.89      0.84      0.85       509\n",
      "\n",
      "[[122   5]\n",
      " [ 74 308]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ce368204",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_balanced[features_all].values \n",
    "\n",
    "\n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[features_all].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "eeff9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6b9b8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8166527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9028571428571429\n",
      "precision :  0.9937106918238994\n",
      "recall :  0.8272251308900523\n",
      "accuracy :  0.8440366972477065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.96      0.60        54\n",
      "           1       0.99      0.83      0.90       382\n",
      "\n",
      "    accuracy                           0.84       436\n",
      "   macro avg       0.72      0.90      0.75       436\n",
      "weighted avg       0.93      0.84      0.87       436\n",
      "\n",
      "[[ 52   2]\n",
      " [ 66 316]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7d0c7",
   "metadata": {},
   "source": [
    "## cosims and plain lev as the only features, give high F scores which signifies the importance of plain lev and cosine sim for eng-french pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1de53b",
   "metadata": {},
   "source": [
    "## all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "27611797",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_plainLV_cosim].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_plainLV_cosim].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c44c59a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14876, 3), (14876,), (1655, 3), (1655,))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8b44b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9e3028e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f449f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8659517426273459\n",
      "precision :  0.8873626373626373\n",
      "recall :  0.8455497382198953\n",
      "accuracy :  0.9395770392749244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1273\n",
      "           1       0.89      0.85      0.87       382\n",
      "\n",
      "    accuracy                           0.94      1655\n",
      "   macro avg       0.92      0.91      0.91      1655\n",
      "weighted avg       0.94      0.94      0.94      1655\n",
      "\n",
      "[[1232   41]\n",
      " [  59  323]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617bd54",
   "metadata": {},
   "source": [
    "## real dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a0d55ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## real dist\n",
    "x_train = train_realdist[features_plainLV_cosim].values \n",
    "\n",
    "\n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[features_plainLV_cosim].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eda937bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9686e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e63a6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9414965986394558\n",
      "precision :  0.9801699716713881\n",
      "recall :  0.9057591623036649\n",
      "accuracy :  0.9155206286836935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85       127\n",
      "           1       0.98      0.91      0.94       382\n",
      "\n",
      "    accuracy                           0.92       509\n",
      "   macro avg       0.87      0.93      0.89       509\n",
      "weighted avg       0.93      0.92      0.92       509\n",
      "\n",
      "[[120   7]\n",
      " [ 36 346]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd2473",
   "metadata": {},
   "source": [
    "## Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d90913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_balanced[features_plainLV_cosim].values \n",
    "\n",
    "\n",
    "y_train = train_balanced[labels].values.ravel()\n",
    "x_test = test_balanced[features_plainLV_cosim].values\n",
    "y_test = test_balanced[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "60314699",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1eacd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "dea3e96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9525101763907734\n",
      "precision :  0.9887323943661972\n",
      "recall :  0.918848167539267\n",
      "accuracy :  0.9197247706422018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.93      0.74        54\n",
      "           1       0.99      0.92      0.95       382\n",
      "\n",
      "    accuracy                           0.92       436\n",
      "   macro avg       0.80      0.92      0.85       436\n",
      "weighted avg       0.94      0.92      0.93       436\n",
      "\n",
      "[[ 50   4]\n",
      " [ 31 351]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f976b",
   "metadata": {},
   "source": [
    "## features_nologits for english french "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8aab151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_alldata[features_nologits].values \n",
    "\n",
    "\n",
    "y_train = train_alldata[labels].values.ravel()\n",
    "x_test = test_alldata[features_nologits].values\n",
    "y_test = test_alldata[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "abdd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "313e5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "08b92e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8633288227334236\n",
      "precision :  0.8935574229691877\n",
      "recall :  0.8350785340314136\n",
      "accuracy :  0.9389728096676737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1273\n",
      "           1       0.89      0.84      0.86       382\n",
      "\n",
      "    accuracy                           0.94      1655\n",
      "   macro avg       0.92      0.90      0.91      1655\n",
      "weighted avg       0.94      0.94      0.94      1655\n",
      "\n",
      "[[1235   38]\n",
      " [  63  319]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "206297a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_realdist[\n",
    "features_cosim_logits].values \n",
    "\n",
    "\n",
    "y_train = train_realdist[labels].values.ravel()\n",
    "x_test = test_realdist[\n",
    "features_cosim_logits].values\n",
    "y_test = test_realdist[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0500fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    " LR = LogisticRegression(random_state=1, solver='lbfgs',penalty = 'l2', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "6d61ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "110b1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.8843930635838151\n",
      "precision :  0.9870967741935484\n",
      "recall :  0.8010471204188482\n",
      "accuracy :  0.8428290766208252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.97      0.75       127\n",
      "           1       0.99      0.80      0.88       382\n",
      "\n",
      "    accuracy                           0.84       509\n",
      "   macro avg       0.80      0.88      0.82       509\n",
      "weighted avg       0.90      0.84      0.85       509\n",
      "\n",
      "[[123   4]\n",
      " [ 76 306]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b154ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
