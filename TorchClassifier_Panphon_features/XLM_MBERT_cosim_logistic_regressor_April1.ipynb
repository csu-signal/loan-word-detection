{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "416abd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\\\n",
    "    , BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "ad80a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "477826e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "# Setting PyTorch's required configuration variables for reproducibility.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9fb6e",
   "metadata": {},
   "source": [
    "# choose the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "051b5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE_TRAINED_MODEL = 'bert-base-multilingual-cased'\n",
    "PRE_TRAINED_MODEL = 'xlm-mlm-100-1280'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "0289d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 5\n",
    "NUM_EPOCHS = 2000  # default maximum number of epochs\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "BS = 8  # batch size\n",
    "INITIAL_LR = 1e-5  # initial learning rate\n",
    "save_epochs = [1, 2, 3, 4, 5, 6, 7]  # these are the epoch numbers (starting from 1) to test the model on the test set\n",
    "# and save the model checkpoint.\n",
    "EARLY_STOP_PATIENCE = 30  # If model does not improve for this number of epochs, training stops.\n",
    "\n",
    "# Setting GPU cards to use for training the model. Make sure you read our paper to figure out if you have enough GPU\n",
    "# memory. If not, you can change all of them to 'cpu' to use CPU instead of GPU. By the way, two 24 GB GPU cards are\n",
    "# enough for current configuration, but in case of developing based on this you may need more (that's why there are\n",
    "# three cards declared here)\n",
    "# CUDA_0 = 'cuda:1'\n",
    "# CUDA_1 = 'cuda:1'\n",
    "# CUDA_2 = 'cuda:1'\n",
    "args = sys.argv\n",
    "epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "3db477c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import csv\n",
    " \n",
    "url_train = 'https://raw.githubusercontent.com/csu-signal/loan-word-detection/main/Datasets/production_train_test/train_final_production.csv'\n",
    "url_test = 'https://raw.githubusercontent.com/csu-signal/loan-word-detection/main/Datasets/production_train_test/test_final_production.csv'\n",
    "url_train = requests.get(url_train).content\n",
    "url_test = requests.get(url_test).content\n",
    "train=pd.read_csv(io.StringIO(url_train.decode('utf-8')))\n",
    "test = pd.read_csv(io.StringIO(url_test.decode('utf-8')))\n",
    "\n",
    "\n",
    "# original_df = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/train_final.csv\")\n",
    "# original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1764c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1 = list(original_df[\"loan_word\"])\n",
    "#l2 = list(original_df[\"original_word\"])\n",
    "#get the cosine similarities from M BERT from the train set\n",
    "# l1 = list(original_df[\"loan_word\"])\n",
    "# l2 = list(original_df[\"original_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "5f4a1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the word 'Refulgent', don't know what it means ! \n",
    "\n",
    "train = train.loc[train[\"original_word\"]!='Refulgent'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "1f960803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 15)"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a1416",
   "metadata": {},
   "source": [
    "# Create list of words for both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "6a2e7980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 466)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l1 = list(train[\"loan_word\"])\n",
    "# l2 = list(train[\"original_word\"])\n",
    "\n",
    "l1 = list(test[\"loan_word\"])\n",
    "l2 = list(test[\"original_word\"])\n",
    "len(l1), len(l2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "f3c1417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENS = 512\n",
    "BERT_EMB = 768  # set to either 768 or 1024 for BERT-Base and BERT-Large models respectively\n",
    "#CUDA_0 = 'cuda:1'\n",
    "#CUDA_1 = 'cuda:1'\n",
    "#CUDA_2 = 'cuda:1'\n",
    "CUDA_0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#CUDA_1 = 'cuda:0'\n",
    "#CUDA_2 = 'cuda:0'\n",
    "\n",
    "# The function for printing in both console and a given log file.\n",
    "def myprint(mystr, logfile):\n",
    "    print(mystr)\n",
    "    print(mystr, file=logfile)\n",
    "\n",
    "\n",
    "# The function for loading datasets from parallel tsv files and returning texts in lists.\n",
    "def load_data(file_name):\n",
    "    try:\n",
    "        # f = open(file_name)\n",
    "        f = pd.read_csv(file_name, sep='\\t', names=['l1_text', 'l2_text'])#, 'extra'])\n",
    "    except:\n",
    "        print('my log: could not read file')\n",
    "        exit()\n",
    "    print(\"This many number of rows were removed from \" + file_name.split(\"/\")[-1] + \" due to having missing values: \",\n",
    "          f.shape[0] - f.dropna().shape[0])\n",
    "    f.dropna(inplace=True)\n",
    "    l1_texts = f['l1_text'].values.tolist()\n",
    "    l2_texts = f['l2_text'].values.tolist()\n",
    "    print(len(l1_texts), len(l2_texts))\n",
    "    print(l1_texts[500])\n",
    "    print(\"\\n\")\n",
    "    print(l2_texts[500])\n",
    "    return l1_texts, l2_texts\n",
    "\n",
    "\n",
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyDataset1(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    # Each component other than the Transformer, are in a sequential layer (it is not required obviously, but it is\n",
    "    # possible to stack them with other layers if desired)\n",
    "    def __init__(self, base_model, n_classes, dropout=0.05):\n",
    "        super().__init__()\n",
    "        # self.base_model = base_model.to(CUDA_0)\n",
    "        self.transformation_learner = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(BERT_EMB, BERT_EMB),\n",
    "            nn.LeakyReLU()\n",
    "        ).to(CUDA_0)\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        l1_pooler_output = input\n",
    "        # l2 = input2\n",
    "        # if 'l1_attention_mask' in kwargs:\n",
    "        #     l1_attention_mask = kwargs['l1_attention_mask']\n",
    "            # l2_attention_mask = kwargs['l2_attention_mask']\n",
    "        # else:\n",
    "        #     print(\"my err: attention mask is not set, error maybe\")\n",
    "        # here we use only the CLS token\n",
    "        # l1_pooler_output = self.base_model(l1.to(CUDA_0), attention_mask=l1_attention_mask.to(CUDA_0)).pooler_output\n",
    "        myoutput = self.transformation_learner(l1_pooler_output)\n",
    "        return myoutput\n",
    "\n",
    "\n",
    "# The function to compute and print the performance measure scores using sklearn implementations.\n",
    "def evaluate_model(labels, predictions, titlestr, logfile):\n",
    "    myprint(titlestr, logfile)\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    myprint(\"Confusion matrix- \\n\" + str(conf_matrix), logfile)\n",
    "    acc_score = accuracy_score(labels, predictions)\n",
    "    myprint('  Accuracy Score: {0:.2f}'.format(acc_score), logfile)\n",
    "    myprint('Report', logfile)\n",
    "    cls_rep = classification_report(labels, predictions)\n",
    "    myprint(cls_rep, logfile)\n",
    "    return f1_score(labels, predictions)  # return f-1 for positive class (sarcasm) as the early stopping measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef910ca6",
   "metadata": {},
   "source": [
    " \n",
    "# get cos sim from mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "06ce0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2268: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "466\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l1, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    l2_encodings = tokenizer(l2, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = BertModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    sim_lst = []\n",
    "    sim_lst_test = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "                                      attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "                                      return_dict=True).last_hidden_state[:, 1, :]\n",
    "        sims = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "        #sim_lst.extend(list(sims))\n",
    "        sim_lst_test.extend(list(sims))\n",
    "print(len(sim_lst_test))\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "bffb1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column for mbert cos sim\n",
    "train['mbert_cos_similarity'] = sim_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "11d20e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "1ae83987",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['mbert_cos_similarity'] = sim_lst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "39f2e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812</td>\n",
       "      <td>क़र्ज़</td>\n",
       "      <td>چاروا</td>\n",
       "      <td>qərzə</td>\n",
       "      <td>t͡ʃɒrvɒ</td>\n",
       "      <td>debt</td>\n",
       "      <td>Charwa</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5527</td>\n",
       "      <td>प्रमोद</td>\n",
       "      <td>لذت بسیار</td>\n",
       "      <td>prəmod</td>\n",
       "      <td>lzt bsjɒr</td>\n",
       "      <td>Pramod</td>\n",
       "      <td>So much fun</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246</td>\n",
       "      <td>अधीनस्थ</td>\n",
       "      <td>کمک</td>\n",
       "      <td>ad̤iːnstʰə</td>\n",
       "      <td>kmk</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>Help</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>3.512500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>آرزو</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ɒrzv</td>\n",
       "      <td>position</td>\n",
       "      <td>Wish</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>746</td>\n",
       "      <td>ढुलमुल</td>\n",
       "      <td>اسرار امیز</td>\n",
       "      <td>ɖ̤ulmul</td>\n",
       "      <td>ɒsrɒr ɒmjz</td>\n",
       "      <td>wavering</td>\n",
       "      <td>Mysterious</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>490</td>\n",
       "      <td>तजुर्बा</td>\n",
       "      <td>بهائی</td>\n",
       "      <td>təd͡ʒurbaː</td>\n",
       "      <td>bhɒيʔj</td>\n",
       "      <td>experience</td>\n",
       "      <td>Baha'i</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1118</td>\n",
       "      <td>वरना</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>vərnaː</td>\n",
       "      <td>vrnh</td>\n",
       "      <td>Otherwise</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>811</td>\n",
       "      <td>लवेबल</td>\n",
       "      <td>دوست داشتنی</td>\n",
       "      <td>ləvebəl</td>\n",
       "      <td>dvst dɒʃtnj</td>\n",
       "      <td>lovable</td>\n",
       "      <td>Lovely</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.359848</td>\n",
       "      <td>0.401515</td>\n",
       "      <td>4.261364</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1661</td>\n",
       "      <td>तेज़ी</td>\n",
       "      <td>فحش دادن</td>\n",
       "      <td>teziː</td>\n",
       "      <td>fhʃ dɒdn</td>\n",
       "      <td>swiftness</td>\n",
       "      <td>Swearing</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411458</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>252</td>\n",
       "      <td>ख़ानदान</td>\n",
       "      <td>شکاری</td>\n",
       "      <td>xaːndaːn</td>\n",
       "      <td>ʃkɒrj</td>\n",
       "      <td>family</td>\n",
       "      <td>Hunting</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 loan_word original_word loan_word_epitran  \\\n",
       "0           812    क़र्ज़        چاروا              qərzə   \n",
       "1          5527    प्रमोद     لذت بسیار            prəmod   \n",
       "2          1246   अधीनस्थ           کمک        ad̤iːnstʰə   \n",
       "3           137      ओहदा          آرزو             oɦdaː   \n",
       "4           746    ढुलमुल    اسرار امیز           ɖ̤ulmul   \n",
       "..          ...       ...           ...               ...   \n",
       "461         490   तजुर्बा         بهائی        təd͡ʒurbaː   \n",
       "462        1118      वरना          ورنه            vərnaː   \n",
       "463         811     लवेबल   دوست داشتنی           ləvebəl   \n",
       "464        1661     तेज़ी      فحش دادن             teziː   \n",
       "465         252   ख़ानदान         شکاری          xaːndaːn   \n",
       "\n",
       "    original_word_epitran loan_english original_english          label  \\\n",
       "0                t͡ʃɒrvɒ          debt           Charwa  hard_negative   \n",
       "1               lzt bsjɒr       Pramod      So much fun        synonym   \n",
       "2                     kmk  subordinate             Help        synonym   \n",
       "3                    ɒrzv     position             Wish         random   \n",
       "4              ɒsrɒr ɒmjz     wavering       Mysterious        synonym   \n",
       "..                    ...          ...              ...            ...   \n",
       "461                bhɒيʔj   experience           Baha'i         random   \n",
       "462                  vrnh    Otherwise             ورنه           loan   \n",
       "463           dvst dɒʃtnj      lovable           Lovely        synonym   \n",
       "464              fhʃ dɒdn    swiftness         Swearing        synonym   \n",
       "465                 ʃkɒrj       family          Hunting         random   \n",
       "\n",
       "     Fast Levenshtein  Dolgo Prime Distance  Feature Edit Distance  \\\n",
       "0               0.875              0.125000               0.065104   \n",
       "1               1.000              0.777778               0.317130   \n",
       "2               1.000              0.700000               0.420833   \n",
       "3               1.000              0.200000               0.116667   \n",
       "4               0.900              0.500000               0.333333   \n",
       "..                ...                   ...                    ...   \n",
       "461             1.000              0.500000               0.268750   \n",
       "462             0.500              0.333333               0.190972   \n",
       "463             1.000              0.727273               0.359848   \n",
       "464             1.000              0.500000               0.411458   \n",
       "465             1.000              0.625000               0.281250   \n",
       "\n",
       "     Hamming Feature Distance  Weighted Feature Distance  \\\n",
       "0                    0.072917                   0.671875   \n",
       "1                    0.365741                   4.222222   \n",
       "2                    0.466667                   3.512500   \n",
       "3                    0.150000                   2.325000   \n",
       "4                    0.383333                   3.312500   \n",
       "..                        ...                        ...   \n",
       "461                  0.304167                   3.112500   \n",
       "462                  0.215278                   2.145833   \n",
       "463                  0.401515                   4.261364   \n",
       "464                  0.453125                   4.000000   \n",
       "465                  0.322917                   3.718750   \n",
       "\n",
       "     Fast Levenshtein Distance Div Maxlen  label_bin  mbert_cos_similarity  \n",
       "0                                   0.875          0              0.488776  \n",
       "1                                   1.000          0              0.384518  \n",
       "2                                   1.000          0              0.365239  \n",
       "3                                   1.000          0              0.440511  \n",
       "4                                   0.900          0              0.359298  \n",
       "..                                    ...        ...                   ...  \n",
       "461                                 1.000          0              0.368733  \n",
       "462                                 0.500          1              0.616410  \n",
       "463                                 1.000          0              0.411980  \n",
       "464                                 1.000          0              0.402701  \n",
       "465                                 1.000          0              0.523436  \n",
       "\n",
       "[466 rows x 16 columns]"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f165db8",
   "metadata": {},
   "source": [
    "# get cos sim from xlm now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "6c49bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([8, 11, 200000])\n",
      "torch.Size([2, 11, 200000])\n",
      "466\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l1, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(l2, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l1_encodings = tokenizer(text =l5,text_pair = l6 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l2_encodings = tokenizer(text =l6,text_pair = l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    \n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        print(l1_vector.shape)\n",
    "        #print(l1_vector[0][0].shape,l2_vector[1].shape )\n",
    "        sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi.extend(list(sims))\n",
    "print(len(xlm_sim_lst_equi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ded6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['mbert_cos_similarity'] = sim_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "b4eaec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['xlm_cos_similarity'] =xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "1b1a2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['xlm_cos_similarity'] =xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "5a0840c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812</td>\n",
       "      <td>क़र्ज़</td>\n",
       "      <td>چاروا</td>\n",
       "      <td>qərzə</td>\n",
       "      <td>t͡ʃɒrvɒ</td>\n",
       "      <td>debt</td>\n",
       "      <td>Charwa</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488776</td>\n",
       "      <td>0.607569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5527</td>\n",
       "      <td>प्रमोद</td>\n",
       "      <td>لذت بسیار</td>\n",
       "      <td>prəmod</td>\n",
       "      <td>lzt bsjɒr</td>\n",
       "      <td>Pramod</td>\n",
       "      <td>So much fun</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384518</td>\n",
       "      <td>0.475078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246</td>\n",
       "      <td>अधीनस्थ</td>\n",
       "      <td>کمک</td>\n",
       "      <td>ad̤iːnstʰə</td>\n",
       "      <td>kmk</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>Help</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>3.512500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365239</td>\n",
       "      <td>0.594256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>آرزو</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ɒrzv</td>\n",
       "      <td>position</td>\n",
       "      <td>Wish</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440511</td>\n",
       "      <td>0.695991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>746</td>\n",
       "      <td>ढुलमुल</td>\n",
       "      <td>اسرار امیز</td>\n",
       "      <td>ɖ̤ulmul</td>\n",
       "      <td>ɒsrɒr ɒmjz</td>\n",
       "      <td>wavering</td>\n",
       "      <td>Mysterious</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359298</td>\n",
       "      <td>0.718769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>490</td>\n",
       "      <td>तजुर्बा</td>\n",
       "      <td>بهائی</td>\n",
       "      <td>təd͡ʒurbaː</td>\n",
       "      <td>bhɒيʔj</td>\n",
       "      <td>experience</td>\n",
       "      <td>Baha'i</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368733</td>\n",
       "      <td>0.732720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1118</td>\n",
       "      <td>वरना</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>vərnaː</td>\n",
       "      <td>vrnh</td>\n",
       "      <td>Otherwise</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616410</td>\n",
       "      <td>0.478068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>811</td>\n",
       "      <td>लवेबल</td>\n",
       "      <td>دوست داشتنی</td>\n",
       "      <td>ləvebəl</td>\n",
       "      <td>dvst dɒʃtnj</td>\n",
       "      <td>lovable</td>\n",
       "      <td>Lovely</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.359848</td>\n",
       "      <td>0.401515</td>\n",
       "      <td>4.261364</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411980</td>\n",
       "      <td>0.567750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1661</td>\n",
       "      <td>तेज़ी</td>\n",
       "      <td>فحش دادن</td>\n",
       "      <td>teziː</td>\n",
       "      <td>fhʃ dɒdn</td>\n",
       "      <td>swiftness</td>\n",
       "      <td>Swearing</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411458</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402701</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>252</td>\n",
       "      <td>ख़ानदान</td>\n",
       "      <td>شکاری</td>\n",
       "      <td>xaːndaːn</td>\n",
       "      <td>ʃkɒrj</td>\n",
       "      <td>family</td>\n",
       "      <td>Hunting</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523436</td>\n",
       "      <td>0.573659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 loan_word original_word loan_word_epitran  \\\n",
       "0           812    क़र्ज़        چاروا              qərzə   \n",
       "1          5527    प्रमोद     لذت بسیار            prəmod   \n",
       "2          1246   अधीनस्थ           کمک        ad̤iːnstʰə   \n",
       "3           137      ओहदा          آرزو             oɦdaː   \n",
       "4           746    ढुलमुल    اسرار امیز           ɖ̤ulmul   \n",
       "..          ...       ...           ...               ...   \n",
       "461         490   तजुर्बा         بهائی        təd͡ʒurbaː   \n",
       "462        1118      वरना          ورنه            vərnaː   \n",
       "463         811     लवेबल   دوست داشتنی           ləvebəl   \n",
       "464        1661     तेज़ी      فحش دادن             teziː   \n",
       "465         252   ख़ानदान         شکاری          xaːndaːn   \n",
       "\n",
       "    original_word_epitran loan_english original_english          label  \\\n",
       "0                t͡ʃɒrvɒ          debt           Charwa  hard_negative   \n",
       "1               lzt bsjɒr       Pramod      So much fun        synonym   \n",
       "2                     kmk  subordinate             Help        synonym   \n",
       "3                    ɒrzv     position             Wish         random   \n",
       "4              ɒsrɒr ɒmjz     wavering       Mysterious        synonym   \n",
       "..                    ...          ...              ...            ...   \n",
       "461                bhɒيʔj   experience           Baha'i         random   \n",
       "462                  vrnh    Otherwise             ورنه           loan   \n",
       "463           dvst dɒʃtnj      lovable           Lovely        synonym   \n",
       "464              fhʃ dɒdn    swiftness         Swearing        synonym   \n",
       "465                 ʃkɒrj       family          Hunting         random   \n",
       "\n",
       "     Fast Levenshtein  Dolgo Prime Distance  Feature Edit Distance  \\\n",
       "0               0.875              0.125000               0.065104   \n",
       "1               1.000              0.777778               0.317130   \n",
       "2               1.000              0.700000               0.420833   \n",
       "3               1.000              0.200000               0.116667   \n",
       "4               0.900              0.500000               0.333333   \n",
       "..                ...                   ...                    ...   \n",
       "461             1.000              0.500000               0.268750   \n",
       "462             0.500              0.333333               0.190972   \n",
       "463             1.000              0.727273               0.359848   \n",
       "464             1.000              0.500000               0.411458   \n",
       "465             1.000              0.625000               0.281250   \n",
       "\n",
       "     Hamming Feature Distance  Weighted Feature Distance  \\\n",
       "0                    0.072917                   0.671875   \n",
       "1                    0.365741                   4.222222   \n",
       "2                    0.466667                   3.512500   \n",
       "3                    0.150000                   2.325000   \n",
       "4                    0.383333                   3.312500   \n",
       "..                        ...                        ...   \n",
       "461                  0.304167                   3.112500   \n",
       "462                  0.215278                   2.145833   \n",
       "463                  0.401515                   4.261364   \n",
       "464                  0.453125                   4.000000   \n",
       "465                  0.322917                   3.718750   \n",
       "\n",
       "     Fast Levenshtein Distance Div Maxlen  label_bin  mbert_cos_similarity  \\\n",
       "0                                   0.875          0              0.488776   \n",
       "1                                   1.000          0              0.384518   \n",
       "2                                   1.000          0              0.365239   \n",
       "3                                   1.000          0              0.440511   \n",
       "4                                   0.900          0              0.359298   \n",
       "..                                    ...        ...                   ...   \n",
       "461                                 1.000          0              0.368733   \n",
       "462                                 0.500          1              0.616410   \n",
       "463                                 1.000          0              0.411980   \n",
       "464                                 1.000          0              0.402701   \n",
       "465                                 1.000          0              0.523436   \n",
       "\n",
       "     xlm_cos_similarity  \n",
       "0              0.607569  \n",
       "1              0.475078  \n",
       "2              0.594256  \n",
       "3              0.695991  \n",
       "4              0.718769  \n",
       "..                  ...  \n",
       "461            0.732720  \n",
       "462            0.478068  \n",
       "463            0.567750  \n",
       "464            0.740800  \n",
       "465            0.573659  \n",
       "\n",
       "[466 rows x 17 columns]"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['mbert_cos_similarity'] = sim_lst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca90418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['xlm_cos_similarity'] = xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['xlm_cos_similarity'] =xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7758fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['xlm_cos_similarity'] =xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "0f2a1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import csv\n",
    " \n",
    "# # s=requests.get(url).content\n",
    "# # c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "# #equiv = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/datasets/Hindi-Persian-Synonyms.csv\")\n",
    "# # rand = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/datasets/Hindi-Persian-Randoms.csv\")\n",
    "# # train = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/train_final.csv\")\n",
    "# # test =pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/test_final.csv\")\n",
    "# url = 'https://raw.githubusercontent.com/csu-signal/loan-word-detection/master/Datasets/Synonyms/Hindi-Persian-Synonyms.csv'\n",
    "# s = requests.get(url).content\n",
    "# #c = pd.read_csv(s)\n",
    "# #df_1 = pd.read_csv(url, header=None, sep='\\n')\n",
    "# c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    " \n",
    "# equiv = pd.read_csv(\"https://github.com/csu-signal/loan-word-detection/blob/main/Datasets/Synonyms/Hindi-Persian-Synonyms.csv\")\n",
    "# rand = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/datasets/Hindi-Persian-Randoms.csv\")\n",
    "# train = pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/train_final.csv\")\n",
    "# test =pd.read_csv(\"/s/chopin/d/proj/ramfis-aida/MachineTranslationIPA/test_final.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "514094c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>छोड़ देना</td>\n",
       "      <td>دست برداشتن از</td>\n",
       "      <td>t͡ʃʰoɽə denaː</td>\n",
       "      <td>dst brdɒʃtn ɒz</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.352679</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Abandon</td>\n",
       "      <td>give up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>छोड़ देना</td>\n",
       "      <td>رد کردن</td>\n",
       "      <td>t͡ʃʰoɽə denaː</td>\n",
       "      <td>rd krdn</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.232372</td>\n",
       "      <td>0.275641</td>\n",
       "      <td>3.048077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Abandon</td>\n",
       "      <td>reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>छोड़ देना</td>\n",
       "      <td>دست بردارید</td>\n",
       "      <td>t͡ʃʰoɽə denaː</td>\n",
       "      <td>dst brdɒrjd</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.258013</td>\n",
       "      <td>0.301282</td>\n",
       "      <td>3.317308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Abandon</td>\n",
       "      <td>Give up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>छोड़ देना</td>\n",
       "      <td>رها کردن</td>\n",
       "      <td>t͡ʃʰoɽə denaː</td>\n",
       "      <td>rhɒ krdn</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.190705</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.615385</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Abandon</td>\n",
       "      <td>to release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>छोड़ देना</td>\n",
       "      <td>کویر</td>\n",
       "      <td>t͡ʃʰoɽə denaː</td>\n",
       "      <td>kvjr</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.323718</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Abandon</td>\n",
       "      <td>desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8445</th>\n",
       "      <td>उमंग</td>\n",
       "      <td>اشتیاق</td>\n",
       "      <td>uməŋɡə</td>\n",
       "      <td>ɒʃtjɒɣ</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>5.270833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>synonym</td>\n",
       "      <td>exultation</td>\n",
       "      <td>Desire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8446</th>\n",
       "      <td>परोक्ष</td>\n",
       "      <td>مورب</td>\n",
       "      <td>pərokʂə</td>\n",
       "      <td>mvrb</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>4.892857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>synonym</td>\n",
       "      <td>indirect</td>\n",
       "      <td>Oblique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8447</th>\n",
       "      <td>परोक्ष</td>\n",
       "      <td>خودسر</td>\n",
       "      <td>pərokʂə</td>\n",
       "      <td>xvdsr</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>synonym</td>\n",
       "      <td>indirect</td>\n",
       "      <td>خودسر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8448</th>\n",
       "      <td>स्वच्छंद</td>\n",
       "      <td>مورب</td>\n",
       "      <td>svət͡ʃt͡ʃʰəndə</td>\n",
       "      <td>mvrb</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.361607</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>3.196429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>synonym</td>\n",
       "      <td>wayward</td>\n",
       "      <td>Oblique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>स्वच्छंद</td>\n",
       "      <td>خودسر</td>\n",
       "      <td>svət͡ʃt͡ʃʰəndə</td>\n",
       "      <td>xvdsr</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.342262</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>synonym</td>\n",
       "      <td>wayward</td>\n",
       "      <td>خودسر</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8450 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loan_word   original_word loan_word_epitran original_word_epitran  \\\n",
       "0     छोड़ देना  دست برداشتن از     t͡ʃʰoɽə denaː        dst brdɒʃtn ɒz   \n",
       "1     छोड़ देना         رد کردن     t͡ʃʰoɽə denaː               rd krdn   \n",
       "2     छोड़ देना     دست بردارید     t͡ʃʰoɽə denaː           dst brdɒrjd   \n",
       "3     छोड़ देना        رها کردن     t͡ʃʰoɽə denaː              rhɒ krdn   \n",
       "4     छोड़ देना            کویر     t͡ʃʰoɽə denaː                  kvjr   \n",
       "...         ...             ...               ...                   ...   \n",
       "8445       उमंग          اشتیاق            uməŋɡə                ɒʃtjɒɣ   \n",
       "8446     परोक्ष            مورب           pərokʂə                  mvrb   \n",
       "8447     परोक्ष           خودسر           pərokʂə                 xvdsr   \n",
       "8448   स्वच्छंद            مورب    svət͡ʃt͡ʃʰəndə                  mvrb   \n",
       "8449   स्वच्छंद           خودسر    svət͡ʃt͡ʃʰəndə                 xvdsr   \n",
       "\n",
       "      Fast Levenshtein  Dolgo Prime Distance  Feature Edit Distance  \\\n",
       "0             0.928571              0.500000               0.352679   \n",
       "1             0.846154              0.461538               0.232372   \n",
       "2             0.923077              0.615385               0.258013   \n",
       "3             0.846154              0.461538               0.190705   \n",
       "4             1.000000              0.461538               0.323718   \n",
       "...                ...                   ...                    ...   \n",
       "8445          1.000000              0.833333               0.250000   \n",
       "8446          0.857143              0.857143               0.473214   \n",
       "8447          1.000000              0.857143               0.392857   \n",
       "8448          0.928571              0.571429               0.361607   \n",
       "8449          0.928571              0.500000               0.303571   \n",
       "\n",
       "      Hamming Feature Distance  Weighted Feature Distance  \\\n",
       "0                     0.383929                   3.750000   \n",
       "1                     0.275641                   3.048077   \n",
       "2                     0.301282                   3.317308   \n",
       "3                     0.230769                   2.615385   \n",
       "4                     0.375000                   3.000000   \n",
       "...                        ...                        ...   \n",
       "8445                  0.277778                   5.270833   \n",
       "8446                  0.523810                   4.892857   \n",
       "8447                  0.464286                   4.375000   \n",
       "8448                  0.404762                   3.196429   \n",
       "8449                  0.342262                   2.687500   \n",
       "\n",
       "      Fast Levenshtein Distance Div Maxlen    label loan_english  \\\n",
       "0                                 0.928571  synonym      Abandon   \n",
       "1                                 0.846154  synonym      Abandon   \n",
       "2                                 0.923077  synonym      Abandon   \n",
       "3                                 0.846154  synonym      Abandon   \n",
       "4                                 1.000000  synonym      Abandon   \n",
       "...                                    ...      ...          ...   \n",
       "8445                              1.000000  synonym   exultation   \n",
       "8446                              0.857143  synonym     indirect   \n",
       "8447                              1.000000  synonym     indirect   \n",
       "8448                              0.928571  synonym      wayward   \n",
       "8449                              0.928571  synonym      wayward   \n",
       "\n",
       "     original_english  \n",
       "0             give up  \n",
       "1              reject  \n",
       "2             Give up  \n",
       "3          to release  \n",
       "4              desert  \n",
       "...               ...  \n",
       "8445           Desire  \n",
       "8446          Oblique  \n",
       "8447            خودسر  \n",
       "8448          Oblique  \n",
       "8449            خودسر  \n",
       "\n",
       "[8450 rows x 13 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the English word 'Refulgent out of train set'\n",
    "\n",
    "train = train.loc[train['original_word'] !='Refulgent'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d91d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the list of orig and loan words for getting the embeddings from mbert, and XLM for train and test sets\n",
    "\n",
    "l1 = list(train[\"loan_word\"])\n",
    "l2 = list(train[\"original_word\"])\n",
    "\n",
    "#l1 = list(test[\"loan_word\"])\n",
    "#l2 = list(test[\"original_word\"])\n",
    "len(l1), len(l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749adecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l5 = list(train['loan_word'])\n",
    "l6=list(train['original_word'])\n",
    "len(l5), len(l6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de006f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "l7 = l5[0:7]\n",
    "l8=l6[0:7]\n",
    "l7,l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "l1_encodings = tokenizer(text =l7,text_pair = l8 , truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "l2_encodings = tokenizer(text =l8,text_pair = l7, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "l2_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[0, 47790, 7657, 1, 179243, 167068, 1, 2]\n",
    "tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l7, l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    #l1_encodings = tokenizer(l7, truncation=True, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\")\n",
    "    #l2_encodings = tokenizer(l8, truncation=True, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\")\n",
    "    l1_encodings = tokenizer(text =l7,text_pair = l8 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(text =l8,text_pair = l7, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi_exp = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0), output_hidden_states =True)[0]\n",
    "        print(len(l1_vector))\n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True)[0] \n",
    "        #print(l1_vector.hidden_states,l2_vector.hidden_states )\n",
    "        sims = cos_s(l1_vector[:,4,:],l2_vector[:,4,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi_exp.extend(list(sims))\n",
    "#print(len(sim_lst_equi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65389f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91fc77bc",
   "metadata": {},
   "source": [
    "# Cosine similarities using XLM layers, middle and last. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    #model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "    \n",
    "    tokenizer.model_max_length = MAXTOKENS\n",
    "    l1_encodings = tokenizer(l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    l2_encodings = tokenizer(l6, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l1_encodings = tokenizer(text =l5,text_pair = l6 , truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask =True)\n",
    "    #l2_encodings = tokenizer(text =l6,text_pair = l5, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "    \n",
    "    dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "    data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "    base_model = XLMWithLMHeadModel.from_pretrained(PRE_TRAINED_MODEL).to(CUDA_0)\n",
    "    base_model.eval()\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    xlm_sim_lst_equi = []\n",
    "    for step, batch in enumerate(data_loader):\n",
    "#         l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l1_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states =True) \n",
    "        l1_vector = base_model(batch['l1_input_ids'].to(CUDA_0),output_hidden_states =True )[0] \n",
    "#         l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0),\n",
    "#                                       attention_mask=batch['l2_attention_mask'].to(CUDA_0),\n",
    "#                                       return_dict=True, output_hidden_states=True) \n",
    "        l2_vector = base_model(batch['l2_input_ids'].to(CUDA_0), output_hidden_states =True) [0]\n",
    "        #print(l1_vector[0][0].shape,l2_vector[1].shape )\n",
    "        sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "        xlm_sim_lst_equi.extend(list(sims))\n",
    "#print(len(sim_lst_equi))\n",
    "      # print(\"Similarities: \")\n",
    "      # for i in range(len(sims)):\n",
    "      #   print(l1[i], ' and ', l2[i], ' : ', sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39847392",
   "metadata": {},
   "source": [
    "# Random words Cosine sim using SOS token no clear threshold/separation between equivalent words, no truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e1acf",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using BOS/classification  token no clear threshold/separation between equivalent words, no truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4c805",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using middle token, some threshold emerges between cosine sims , no truncation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03bba5",
   "metadata": {},
   "source": [
    "# Equivalent words Cosine sim using EOS token, huge peak with cos sim of 1 , no truncation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(xlm_sim_lst_equi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"xlm_cos_similarity\"] = xlm_sim_lst_equi\n",
    "equiv[\"mbert_cos_similarity\"] =sim_lst\n",
    "equiv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2ea28",
   "metadata": {},
   "source": [
    "# Mean and Std for XLM cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce18395",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(xlm_sim_lst_equi)\n",
    " \n",
    "ordered_std = np.std(xlm_sim_lst_equi)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51726e",
   "metadata": {},
   "source": [
    "# Mean and Std for mbert cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91418d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(sim_lst)\n",
    " \n",
    "ordered_std = np.std(sim_lst)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f4d1c",
   "metadata": {},
   "source": [
    "# Outliers from Z score for XLM cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "xlm_ordered_outlier_idx = []\n",
    "for i in range(len(xlm_sim_lst_equi)):\n",
    "    z = abs(xlm_sim_lst_equi[i]-ordered_mean)/ordered_std\n",
    "    if z > threshold:\n",
    "        xlm_ordered_outlier_idx.append(i)\n",
    "print('outlier indices are', xlm_ordered_outlier_idx, len(xlm_ordered_outlier_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier = equiv.loc[xlm_ordered_outlier_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446db8b",
   "metadata": {},
   "source": [
    "# Outliers with low cos sim from XLM and Mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae002676",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos = xlm_equiv_outlier.loc[(xlm_equiv_outlier['xlm_cos_similarity'] <0.35) & (xlm_equiv_outlier['mbert_cos_similarity']<0.35)]\n",
    "equiv_outlier_lowcos.to_csv('Outliers_XLM_Mbert_lowcosim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_lowcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c8b14",
   "metadata": {},
   "source": [
    "# Outliers with high cos sim from XLM and Mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1a86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f456dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equiv.loc[(equiv['xlm_cos_similarity'] >0.68) & (equiv['mbert_cos_similarity']>0.6)]\n",
    "equiv_outlier_highcos.to_csv('Outliers_XLM_Mbert_highcosim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a88a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(equiv_outlier_highcos['xlm_cos_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904579b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_highcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44384552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d130653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00eeae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier_lowcos = equiv.loc[equiv['xlm_cos_similarity'] <0.23]\n",
    "xlm_equiv_outlier_lowcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70426825",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_equiv_outlier_highcos = equiv.loc[equiv['xlm_cos_similarity'] ==1]\n",
    "xlm_equiv_outlier_highcos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a9572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02558969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c898d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ead755",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"xlm_cos_similarity\"] = xlm_sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bf60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7fe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3ff39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34efa010",
   "metadata": {},
   "source": [
    "# Cosine similarity and Outliers using M-bert cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(sim_lst_equi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv[\"cos_similarity\"] = sim_lst_equi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mean = np.mean(sim_lst_equi)\n",
    " \n",
    "ordered_std = np.std(sim_lst_equi)\n",
    " \n",
    "print(\"ordered_mean: \", ordered_mean)\n",
    " \n",
    "print(\"ordered_std: \", ordered_std)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "ordered_outlier_idx = []\n",
    "for i in range(len(sim_lst_equi)):\n",
    "    z = abs(sim_lst_equi[i]-ordered_mean)/ordered_std\n",
    "    if z > threshold:\n",
    "        ordered_outlier_idx.append(i)\n",
    "print('outlier indice are', ordered_outlier_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_outlier_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75182721",
   "metadata": {},
   "outputs": [],
   "source": [
    "equov_outlier = equiv.loc[ordered_outlier_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740be8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "equov_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(equov_outlier['cos_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos = equov_outlier.loc[equov_outlier['cos_similarity'] <0.23]\n",
    "equiv_outlier_lowcos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_lowcos[[c for c in equiv_outlier_lowcos.columns if c in edit_dists_names]]\n",
    "equiv_outlier_lowcos = equiv_outlier_lowcos[[*edit_dists_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_dists_names = ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen',\n",
    "        ]\n",
    "dist_arr = np.array(edit_dists_names).reshape(-1,3)\n",
    "dist_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_lowcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace94efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc061faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equov_outlier.loc[equov_outlier['cos_similarity'] >0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_outlier_highcos = equiv_outlier_highcos[[*edit_dists_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30caae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(dist_arr), len(dist_arr[0]), figsize=(17,10))\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        sns.kdeplot(data= equiv_outlier_highcos, x=dist_arr[i,j], shade=1, ax=axs[i][j], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22372aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644f639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de597f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b23a95",
   "metadata": {},
   "source": [
    "# Get the logits for the train from the cnn/dnn for training the logistic regressor after padding to max length and concatenating loan and original word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "7655d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "6e31cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "abeb2c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 3.9 GB\n",
      "Cached:    6.7 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "id": "d2536f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "2a13f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get phonetic features using panPhon\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "c840ffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812</td>\n",
       "      <td>क़र्ज़</td>\n",
       "      <td>چاروا</td>\n",
       "      <td>qərzə</td>\n",
       "      <td>t͡ʃɒrvɒ</td>\n",
       "      <td>debt</td>\n",
       "      <td>Charwa</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488776</td>\n",
       "      <td>0.607569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5527</td>\n",
       "      <td>प्रमोद</td>\n",
       "      <td>لذت بسیار</td>\n",
       "      <td>prəmod</td>\n",
       "      <td>lzt bsjɒr</td>\n",
       "      <td>Pramod</td>\n",
       "      <td>So much fun</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384518</td>\n",
       "      <td>0.475078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246</td>\n",
       "      <td>अधीनस्थ</td>\n",
       "      <td>کمک</td>\n",
       "      <td>ad̤iːnstʰə</td>\n",
       "      <td>kmk</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>Help</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>3.512500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365239</td>\n",
       "      <td>0.594256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>آرزو</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ɒrzv</td>\n",
       "      <td>position</td>\n",
       "      <td>Wish</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440511</td>\n",
       "      <td>0.695991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>746</td>\n",
       "      <td>ढुलमुल</td>\n",
       "      <td>اسرار امیز</td>\n",
       "      <td>ɖ̤ulmul</td>\n",
       "      <td>ɒsrɒr ɒmjz</td>\n",
       "      <td>wavering</td>\n",
       "      <td>Mysterious</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359298</td>\n",
       "      <td>0.718769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>490</td>\n",
       "      <td>तजुर्बा</td>\n",
       "      <td>بهائی</td>\n",
       "      <td>təd͡ʒurbaː</td>\n",
       "      <td>bhɒيʔj</td>\n",
       "      <td>experience</td>\n",
       "      <td>Baha'i</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368733</td>\n",
       "      <td>0.732720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1118</td>\n",
       "      <td>वरना</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>vərnaː</td>\n",
       "      <td>vrnh</td>\n",
       "      <td>Otherwise</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616410</td>\n",
       "      <td>0.478068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>811</td>\n",
       "      <td>लवेबल</td>\n",
       "      <td>دوست داشتنی</td>\n",
       "      <td>ləvebəl</td>\n",
       "      <td>dvst dɒʃtnj</td>\n",
       "      <td>lovable</td>\n",
       "      <td>Lovely</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.359848</td>\n",
       "      <td>0.401515</td>\n",
       "      <td>4.261364</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411980</td>\n",
       "      <td>0.567750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1661</td>\n",
       "      <td>तेज़ी</td>\n",
       "      <td>فحش دادن</td>\n",
       "      <td>teziː</td>\n",
       "      <td>fhʃ dɒdn</td>\n",
       "      <td>swiftness</td>\n",
       "      <td>Swearing</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411458</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402701</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>252</td>\n",
       "      <td>ख़ानदान</td>\n",
       "      <td>شکاری</td>\n",
       "      <td>xaːndaːn</td>\n",
       "      <td>ʃkɒrj</td>\n",
       "      <td>family</td>\n",
       "      <td>Hunting</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523436</td>\n",
       "      <td>0.573659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 loan_word original_word loan_word_epitran  \\\n",
       "0           812    क़र्ज़        چاروا              qərzə   \n",
       "1          5527    प्रमोद     لذت بسیار            prəmod   \n",
       "2          1246   अधीनस्थ           کمک        ad̤iːnstʰə   \n",
       "3           137      ओहदा          آرزو             oɦdaː   \n",
       "4           746    ढुलमुल    اسرار امیز           ɖ̤ulmul   \n",
       "..          ...       ...           ...               ...   \n",
       "461         490   तजुर्बा         بهائی        təd͡ʒurbaː   \n",
       "462        1118      वरना          ورنه            vərnaː   \n",
       "463         811     लवेबल   دوست داشتنی           ləvebəl   \n",
       "464        1661     तेज़ी      فحش دادن             teziː   \n",
       "465         252   ख़ानदान         شکاری          xaːndaːn   \n",
       "\n",
       "    original_word_epitran loan_english original_english          label  \\\n",
       "0                t͡ʃɒrvɒ          debt           Charwa  hard_negative   \n",
       "1               lzt bsjɒr       Pramod      So much fun        synonym   \n",
       "2                     kmk  subordinate             Help        synonym   \n",
       "3                    ɒrzv     position             Wish         random   \n",
       "4              ɒsrɒr ɒmjz     wavering       Mysterious        synonym   \n",
       "..                    ...          ...              ...            ...   \n",
       "461                bhɒيʔj   experience           Baha'i         random   \n",
       "462                  vrnh    Otherwise             ورنه           loan   \n",
       "463           dvst dɒʃtnj      lovable           Lovely        synonym   \n",
       "464              fhʃ dɒdn    swiftness         Swearing        synonym   \n",
       "465                 ʃkɒrj       family          Hunting         random   \n",
       "\n",
       "     Fast Levenshtein  Dolgo Prime Distance  Feature Edit Distance  \\\n",
       "0               0.875              0.125000               0.065104   \n",
       "1               1.000              0.777778               0.317130   \n",
       "2               1.000              0.700000               0.420833   \n",
       "3               1.000              0.200000               0.116667   \n",
       "4               0.900              0.500000               0.333333   \n",
       "..                ...                   ...                    ...   \n",
       "461             1.000              0.500000               0.268750   \n",
       "462             0.500              0.333333               0.190972   \n",
       "463             1.000              0.727273               0.359848   \n",
       "464             1.000              0.500000               0.411458   \n",
       "465             1.000              0.625000               0.281250   \n",
       "\n",
       "     Hamming Feature Distance  Weighted Feature Distance  \\\n",
       "0                    0.072917                   0.671875   \n",
       "1                    0.365741                   4.222222   \n",
       "2                    0.466667                   3.512500   \n",
       "3                    0.150000                   2.325000   \n",
       "4                    0.383333                   3.312500   \n",
       "..                        ...                        ...   \n",
       "461                  0.304167                   3.112500   \n",
       "462                  0.215278                   2.145833   \n",
       "463                  0.401515                   4.261364   \n",
       "464                  0.453125                   4.000000   \n",
       "465                  0.322917                   3.718750   \n",
       "\n",
       "     Fast Levenshtein Distance Div Maxlen  label_bin  mbert_cos_similarity  \\\n",
       "0                                   0.875          0              0.488776   \n",
       "1                                   1.000          0              0.384518   \n",
       "2                                   1.000          0              0.365239   \n",
       "3                                   1.000          0              0.440511   \n",
       "4                                   0.900          0              0.359298   \n",
       "..                                    ...        ...                   ...   \n",
       "461                                 1.000          0              0.368733   \n",
       "462                                 0.500          1              0.616410   \n",
       "463                                 1.000          0              0.411980   \n",
       "464                                 1.000          0              0.402701   \n",
       "465                                 1.000          0              0.523436   \n",
       "\n",
       "     xlm_cos_similarity  \n",
       "0              0.607569  \n",
       "1              0.475078  \n",
       "2              0.594256  \n",
       "3              0.695991  \n",
       "4              0.718769  \n",
       "..                  ...  \n",
       "461            0.732720  \n",
       "462            0.478068  \n",
       "463            0.567750  \n",
       "464            0.740800  \n",
       "465            0.573659  \n",
       "\n",
       "[466 rows x 17 columns]"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "a358195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['features_loan'] = train.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "train['features_orig'] = train.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "id": "03ca4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features for test set \n",
    "test['features_loan'] = test.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "test['features_orig'] = test.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "05ee7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "3cf2d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a flat list of the features for both orig and loan words, run one at a time and comment the other out\n",
    "train['features_loan'] = train['features_loan'].apply(lambda x:sum(x, []))\n",
    "#train['features_orig'] = train['features_orig'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "aaa24e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "9a26ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['features_orig'] = test['features_orig'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "0fcad17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['features_loan'] = test['features_loan'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f02bba",
   "metadata": {},
   "source": [
    "# get padded vector separately for train and test, first train and then test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "accd7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_loan = train.loan_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()\n",
    "max_len_orig = train.original_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "053ce027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padarray(A, size):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "98d68bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train set \n",
    "pad_idx_loan = train['features_loan'].apply(len)\n",
    "pad_idx_orig = train['features_orig'].apply(len)\n",
    "pad_idx_loan = np.array(pad_idx_loan)\n",
    "pad_idx_orig = np.array(pad_idx_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "d979d0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 24, 144,  72, ..., 192, 168, 120]),\n",
       " array([120,  96,  72, ..., 120, 120, 240]))"
      ]
     },
     "execution_count": 977,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx_loan, pad_idx_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "b30f4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_loan = np.asarray(train['features_loan'])\n",
    "arr_orig = np.asarray(train['features_orig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "65ccb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_loan = []\n",
    "for i in range(len(arr_loan)):\n",
    "    a = padarray(arr_loan[i], max_len_loan*24)\n",
    "    l_loan.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "c30419bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_orig = []\n",
    "for i in range(len(arr_orig)):\n",
    "    a = padarray(arr_orig[i], max_len_orig*24)\n",
    "    l_orig.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "7d62b08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 1032)"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_orig)\n",
    "l_comb = np.concatenate((l_loan, l_orig), axis=1)\n",
    "l_comb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "e1115c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 1)"
      ]
     },
     "execution_count": 982,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables = np.array(train['label_bin']).reshape((-1,1))\n",
    "lables.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "a88daa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(l_comb, lables, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "57f66beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'x_train' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "x.shape\n",
    "%store x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "b0f1f1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 1033)"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c92be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "f0bed7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "b4c274bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_loan = test.loan_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()\n",
    "max_len_orig = test.original_word_epitran.map(lambda x: len(ft.word_to_vector_list(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "9adf650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 20)"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_loan, max_len_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "id": "69e75732",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# pad_idx_loan = test['features_loan'].apply(len)\n",
    "# pad_idx_orig = test['features_orig'].apply(len)\n",
    "# pad_idx_loan = np.array(pad_idx_loan)\n",
    "# pad_idx_orig = np.array(pad_idx_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "0171ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_idx_loan, pad_idx_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "a501ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for test set\n",
    "pad_idx_loan = test['features_loan'].apply(len)\n",
    "pad_idx_orig = test['features_orig'].apply(len)\n",
    "pad_idx_loan = np.array(pad_idx_loan)\n",
    "pad_idx_orig = np.array(pad_idx_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "7faaac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_idx_loan, pad_idx_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "id": "cf108cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train \n",
    "# arr_loan = np.asarray(original_df['features_loan'])\n",
    "# arr_orig = np.asarray(original_df['features_orig'])\n",
    "#for test \n",
    "arr_loan = np.asarray(test['features_loan'])\n",
    "arr_orig = np.asarray(test['features_orig'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "6ed14212",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_loan = []\n",
    "for i in range(len(arr_loan)):\n",
    "    a = padarray(arr_loan[i], max_len_loan*24)\n",
    "    l_loan.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "038dc584",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_orig = []\n",
    "for i in range(len(arr_orig)):\n",
    "    a = padarray(arr_orig[i], max_len_orig*24)\n",
    "    l_orig.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "05210ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "584ce273",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_comb = np.concatenate((l_loan, l_orig), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "8863e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 864)"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "id": "4f534639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 1)"
      ]
     },
     "execution_count": 999,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables = np.array(test['label_bin']).reshape((-1,1))\n",
    "lables.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "6918d42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 865)"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = np.append(l_comb, lables, axis=1) \n",
    "x_test_prod = np.append(l_comb, lables, axis=1) \n",
    "x_test_prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "05ab3676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'x_test_prod' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "x_test.shape\n",
    "%store x_test_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "43795d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_prod\n",
    "%store -r x_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "143ec396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "35ae66f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 1004,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[:,-1]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "a88ce65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "d16bfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "9a300629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits\n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b6be8",
   "metadata": {},
   "source": [
    "# get logits for DNN separately for train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "577d2e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466, 865), (4194, 1033))"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_prod.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "id": "6d3e5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train[:,0:1032]\n",
    "Y_train = x_train[:,1032] \n",
    "# Y_train\n",
    "X_test = x_test_prod[:,0:864]\n",
    "Y_test = x_test_prod[:,864] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "be1f37f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 1032), (4194,), (466, 864), (466,))"
      ]
     },
     "execution_count": 1057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "id": "51e92dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=864, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# do it seprately for train and test\n",
    "#model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "model = NeuralNetwork(X_test.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "id": "02aaa6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=864, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "id": "db46b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "id": "d15bb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "id": "a5c286e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "X_test= torch.tensor(X_test).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "id": "a5efb7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4194, 1032]),\n",
       " torch.Size([4194, 1]),\n",
       " torch.Size([466, 864]),\n",
       " torch.Size([466, 1]))"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "id": "c82760ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "id": "ca15cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "id": "46339fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "id": "a870444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0Test set - loss: 0.698 \n",
      "epoch 100Test set - loss: 0.245 \n",
      "epoch 200Test set - loss: 0.015 \n",
      "epoch 300Test set - loss: 0.006 \n",
      "epoch 400Test set - loss: 0.005 \n",
      "epoch 500Test set - loss: 0.004 \n",
      "epoch 600Test set - loss: 0.004 \n",
      "epoch 700Test set - loss: 0.003 \n",
      "epoch 800Test set - loss: 0.003 \n",
      "epoch 900Test set - loss: 0.003 \n"
     ]
    }
   ],
   "source": [
    "#train for 10000 epochs and get the logits \n",
    "test_losses = []\n",
    "train_losses = []\n",
    "test_accur = []\n",
    "train_accur = []\n",
    "logits = []\n",
    "for epoch in range(1000):\n",
    "\n",
    "#     y_pred = model(X_train.float())[0]\n",
    "#     logits = model(X_train.float())[1]\n",
    "    #getting logits for test set \n",
    "    y_pred = model(X_test.float())[0]\n",
    "    logits = model(X_test.float())[1]\n",
    "    #y_pred = model(X_train) \n",
    "    #print(y_pred)\n",
    "\n",
    "    #y_pred = torch.squeeze(y_pred)\n",
    "    #train_loss = criterion(y_pred, Y_train.float())\n",
    "    \n",
    "    test_loss = criterion(y_pred, Y_test.float())\n",
    "    #train_loss = criterion(y_pred, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        #train_acc,_ = calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "        #y_test_pred = model(X_test.float())\n",
    "        #y_test_pred = torch.squeeze(y_test_pred)\n",
    "         \n",
    "\n",
    "        #test_loss = criterion(y_test_pred, Y_test.float())\n",
    "\n",
    "        #test_acc, total_corr = calculate_accuracy(Y_test, y_test_pred)\n",
    "        #print(total_corr)\n",
    "        \n",
    "#         print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}Test  set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}\n",
    "# ''')\n",
    "        #print(f'''epoch {epoch}Train set - loss: {round_tensor(train_loss)} ''')\n",
    "        print(f'''epoch {epoch}Test set - loss: {round_tensor(test_loss)} ''')\n",
    "        #train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #test_accur.append(test_acc)\n",
    "        #train_accur.append(train_acc)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #train_loss.backward()\n",
    "    test_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "id": "30cfe588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "id": "ae825112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8173805da0>]"
      ]
     },
     "execution_count": 1069,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAblUlEQVR4nO3de3Qc5Znn8e/T3bpY8kWyWuYiX2RLMowDBIgwIJEsAZw1m6zJbmYSsyc5YTY7zmZjwgJ7Ibt7OHPYfzYzgQxn4s3GSzI7M5uMQxjOjGfGM4ZwmQ22AQtiLraxkeWLZAOWbck32dbt2T+6ZbeEZLXtlqqr+vc5R0dVb71d9bjBvy6/XVWvuTsiIhJ+saALEBGR3FCgi4hEhAJdRCQiFOgiIhGhQBcRiYhEUAdOJpNeW1sb1OFFRELpjTfeOOTu1aNtCyzQa2traWlpCerwIiKhZGZ7x9qmIRcRkYhQoIuIRIQCXUQkIhToIiIRkVWgm9lSM9thZq1m9sgo239gZlvSPzvNrDvnlYqIyHmNe5WLmcWBVcASoAPYbGZr3X3bUB93fzCj//3ADRNQq4iInEc2Z+iLgVZ3b3P3XmANcM95+t8L/EUuihMRkexlE+g1QHvGeke67WPMbB4wH3jx0ksb3Rt7u/jeP7yHHvsrIjJcrr8UXQ484+4Do200sxVm1mJmLZ2dnRd1gK0HjvKjl3ex70jPpdQpIhI52QT6fmBOxvrsdNtolnOe4RZ3X+3uje7eWF096p2r42quTwKwofXwRb1eRCSqsgn0zUCDmc03s2JSob12ZCczuxqoBDbltsThFiTLuXx6KRt2HZrIw4iIhM64ge7u/cBKYD2wHXja3bea2WNmtiyj63JgjU/w4LaZ0VRfxaZdhxkc1Di6iMiQrB7O5e7rgHUj2h4dsf77uSvr/Jrrkjz75n7e+/A4i66cPlmHFRHJa6G8U/TcOLqGXUREhoQy0C+fUUpddbnG0UVEMoQy0CF1lv767iP09g8GXYqISF4IbaA31SXp6R3grY7uoEsREckLoQ30WxdUETN45X0Nu4iIQIgDfUZZEdfUzGCjxtFFRIAQBzqkxtF/s6+bk2f6gy5FRCRw4Q70uiT9g87re44EXYqISOBCHeiNtZUUJ2Js1PXoIiLhDvTSojifmlvJK3pQl4hIuAMdoLm+iu0fHOPwiTNBlyIiEqgIBHrqMQCb2nSWLiKFLfSBfm3NDKaVJPR8dBEpeKEP9EQ8xs0LqnQ9uogUvNAHOqTG0fce7qFd09KJSAGLRKDflh5H11m6iBSySAR6/aypzJpWonF0ESlokQh0M6OproqNuw4zwTPgiYjkrUgEOkBTfZJDJ86w86MTQZciIhKIyAT60PXor+gxACJSoLIKdDNbamY7zKzVzB4Zo8+XzWybmW01s5/ntszx1VRMYX6yXM91EZGClRivg5nFgVXAEqAD2Gxma919W0afBuC7QLO7d5nZrIkq+Hya6qr46y0H6B8YJBGPzD8+RESykk3qLQZa3b3N3XuBNcA9I/r8HrDK3bsA3P1gbsvMTnN9khNn+nmr42gQhxcRCVQ2gV4DtGesd6TbMi0EFprZBjN71cyWjrYjM1thZi1m1tLZ2XlxFZ/HrQuqMIMNGnYRkQKUq3GJBNAA3A7cC/xvM6sY2cndV7t7o7s3VldX5+jQ51SWF7PoiukKdBEpSNkE+n5gTsb67HRbpg5grbv3uftuYCepgJ90t6WnpTvVOxDE4UVEApNNoG8GGsxsvpkVA8uBtSP6/BWps3PMLElqCKYtd2Vmr6k+Se/AIJs1LZ2IFJhxA93d+4GVwHpgO/C0u281s8fMbFm623rgsJltA14C/qO7B3If/k21lRTFjQ16rouIFJhxL1sEcPd1wLoRbY9mLDvwUPonUGXFCW6YW6lxdBEpOJG8WLu5LsnWA8fo7ukNuhQRkUkTyUC/raEKd9i0S09fFJHCEclAv252BeXFcY2ji0hBiWSgFw1NS6fno4tIAYlkoEPquS5th05yoPtU0KWIiEyKyAb60ON0dbWLiBSKyAb6VZdNIzm1mI36YlRECkRkAz0WM26tS7Kh9ZCmpRORghDZQAdorqvi4PEz7OrUtHQiEn3RDvShaene1zi6iERfpAN9zswy5sycwgaNo4tIAYh0oEPqcbqvth2mf2Aw6FJERCZU5AO9qS7J8dP9vHvgWNCliIhMqAII9CpA16OLSPRFPtCrppZw9eXTFOgiEnmRD3RIXe3SsreL032alk5EoqsgAv22+iS9/YO8sbcr6FJERCZMQQT64vkzScRMwy4iEmkFEejlJQmun1OhQBeRSMsq0M1sqZntMLNWM3tklO33mVmnmW1J//yb3Jd6aZrqk7yz/yhHT/UFXYqIyIQYN9DNLA6sAu4GFgH3mtmiUbr+wt2vT/88leM6L1lzXRWDDq+26a5REYmmbM7QFwOt7t7m7r3AGuCeiS0r926YW8mUojgbNewiIhGVTaDXAO0Z6x3ptpG+ZGZvm9kzZjYnJ9XlUHEixuL5M/VcFxGJrFx9Kfo3QK27Xwc8D/zpaJ3MbIWZtZhZS2dnZ44Onb3m+ipaD57gw6OnJ/3YIiITLZtA3w9knnHPTred5e6H3f1MevUp4FOj7cjdV7t7o7s3VldXX0y9l6SpLvU43Y27NOwiItGTTaBvBhrMbL6ZFQPLgbWZHczsiozVZcD23JWYO4uumE5lWREbWjXsIiLRkxivg7v3m9lKYD0QB37q7lvN7DGgxd3XAt8xs2VAP3AEuG8Ca75osZjRVJdk467UtHRmFnRJIiI5M26gA7j7OmDdiLZHM5a/C3w3t6VNjKb6Kv7unQ/YfegkC6qnBl2OiEjOFMSdopma0+PoumtURKKm4AJ9XlUZNRVTNI4uIpFTcIFuZjTXV7Gp7TADgx50OSIiOVNwgQ6p56MfPdXHNk1LJyIRUpCBfuvQtHS6Hl1EIqQgA33WtFIWXjZVX4yKSKQUZKBD6q7RzXuOcKZf09KJSDQUbKDfVp/kdN8gb+7tDroUEZGcKNhAv3nBTOIx03NdRCQyCjbQp5UWcd3sGbyicXQRiYiCDXRI3TX6dsdRjp/WtHQiEn4FHehN9VUMDDqvtR0JuhQRkUtW0IF+49xKSotiuh5dRCKhoAO9tCjOTbUz2ajnuohIBBR0oEPqevQdHx3n4HFNSyci4Vbwgd5cn3oMwCZNHi0iIVfwgf6JK2cwvTShxwCISOgVfKDH09PSbWg9jLsepysi4VXwgQ6pYZf93afYd6Qn6FJERC6aAh1oqk9NS6e7RkUkzLIKdDNbamY7zKzVzB45T78vmZmbWWPuSpx4C5LlXD69VJcvikiojRvoZhYHVgF3A4uAe81s0Sj9pgEPAK/lusiJZmY01VexcdchBjUtnYiEVDZn6IuBVndvc/deYA1wzyj9/jvwPSCUF3TfVp+kq6eP7R9qWjoRCadsAr0GaM9Y70i3nWVmNwJz3P3vzrcjM1thZi1m1tLZ2XnBxU6k5vQ4uoZdRCSsLvlLUTOLAU8AD4/X191Xu3ujuzdWV1df6qFz6rLppdRVl+uLUREJrWwCfT8wJ2N9drptyDTgGuBlM9sD3AKsDdsXo5A6S3999xF6+weDLkVE5IJlE+ibgQYzm29mxcByYO3QRnc/6u5Jd69191rgVWCZu7dMSMUTqKkuyam+Aba0dwddiojIBRs30N29H1gJrAe2A0+7+1Yze8zMlk10gZPp1gVVxAw9BkBEQimRTSd3XwesG9H26Bh9b7/0soIxo6yIa2tmsKH1EA8uWRh0OSIiF0R3io7QVJ9kS3s3J8/0B12KiMgFUaCP0FyXpH/QeX23pqUTkXBRoI/QWFtJcSKmcXQRCR0F+gilRXEa51WyQRNeiEjIKNBH0VyfZPsHxzh04kzQpYiIZE2BPoqmOk1LJyLho0AfxbU1M5hWkmDjLo2ji0h4KNBHkYjHuKWuig16UJeIhIgCfQzNdVXsO9JDu6alE5GQUKCPYehxurp8UUTCQoE+hvpZU5k1rUSXL4pIaCjQx2BmNNVVsWnXIdw1LZ2I5D8F+nk01yc5dKKXHR8dD7oUEZFxKdDP49w4uoZdRCT/KdDP48qKKcxPluuLUREJBQX6OJrqqnit7TB9A5qWTkTymwJ9HM31SU72DvB2R3fQpYiInJcCfRy3LqjCTOPoIpL/FOjjqCwv5hNXTtc4uojkvawC3cyWmtkOM2s1s0dG2f5vzewdM9tiZq+Y2aLclxqc5rokb+7roqdX09KJSP4aN9DNLA6sAu4GFgH3jhLYP3f3a939euAPgCdyXWiQmuqT9A04m/d0BV2KiMiYsjlDXwy0unubu/cCa4B7Mju4+7GM1XIgUrdW3lRbSVHc2KhhFxHJY4ks+tQA7RnrHcDNIzuZ2beBh4Bi4I7RdmRmK4AVAHPnzr3QWgNTVpzgxrmVbNDz0UUkj+XsS1F3X+XudcB/Bv7bGH1Wu3ujuzdWV1fn6tCTork+ydYDx+g62Rt0KSIio8om0PcDczLWZ6fbxrIG+OIl1JSXmuurcIdNbbp8UUTyUzaBvhloMLP5ZlYMLAfWZnYws4aM1c8D7+euxPxw3ewKyovjunxRRPLWuGPo7t5vZiuB9UAc+Km7bzWzx4AWd18LrDSzu4A+oAv4+kQWHYSieIybF1SxUc9HF5E8lc2Xorj7OmDdiLZHM5YfyHFdeam5PsmL7x1kf/cpaiqmBF2OiMgwulP0AjTXVwGalk5E8pMC/QJcddk0klOLdT26iOQlBfoFMDNurUuyYddhTUsnInlHgX6Bmuuq6Dx+htaDJ4IuRURkGAX6BTo3LZ2GXUQkvyjQL9CcmWXMnVnGK3o+uojkGQX6RWiuT01L169p6UQkjyjQL0JTXZLjZ/p5Z//RoEsRETlLgX4RmupS16PrrlERyScK9ItQNbWE37piOr9+vzPoUkREzlKgX6Sln7icV9uO8K6GXUQkTyjQL9J9zbVML03wg+d3Bl2KiAigQL9oM6YU8c1/UscL7x3kzX2aa1REgqdAvwT3NdVSVV7M48/tCLoUEREF+qUoL0nwrdvr2NB6mI2ab1REAqZAv0RfvWUel00v4YnnduqBXSISKAX6JSotinP/HQ207O3i5Z26jFFEgqNAz4EvN85hduUUHn9uh87SRSQwCvQcKE7EeODOBt7df4z1Wz8KuhwRKVAK9Bz5FzfUsKC6nCee38HAoM7SRWTyZRXoZrbUzHaYWauZPTLK9ofMbJuZvW1mL5jZvNyXmt8S8RgP3rWQnR+d4G/fPhB0OSJSgMYNdDOLA6uAu4FFwL1mtmhEt98Aje5+HfAM8Ae5LjQMPn/tFVx9+TR+8PxOPVpXRCZdNmfoi4FWd29z915gDXBPZgd3f8nde9KrrwKzc1tmOMRixkNLFrLncA/Pvrk/6HJEpMBkE+g1QHvGeke6bSzfAP5+tA1mtsLMWsyspbMzmpf4LVl0GZ+cPYMnX3ifM/0DQZcjIgUkp1+KmtlXgUbgD0fb7u6r3b3R3Rurq6tzeei8YWY8/Lmr2N99il9sbh//BSIiOZJNoO8H5mSsz063DWNmdwH/FVjm7mdyU144fbohyeLamfzxi62c6tVZuohMjmwCfTPQYGbzzawYWA6szexgZjcAPyYV5gdzX2a4pM7SF9J5/Ax//uqeoMsRkQIxbqC7ez+wElgPbAeedvetZvaYmS1Ld/tDYCrwSzPbYmZrx9hdwbh5QRWfbkjyo5d3ceJMf9DliEgBsKBuVW9sbPSWlpZAjj1ZtrR388VVG3h4yULuv7Mh6HJEJALM7A13bxxtm+4UnUDXz6lgyaLLWP3rNo729AVdjohEnAJ9gj20ZCHHT/ez+te7gi5FRCJOgT7BfuuK6Xzhuiv4kw17OHSioC/+EZEJpkCfBA8uWcjpvgH+18s6SxeRiaNAnwR11VP5lzfO5s9e3cuHR08HXY6IRJQCfZI8cGcDg4POD196P+hSRCSiFOiTZM7MMr5y0xx+sbmd9iM9479AROQCKdAn0f13NGBmPPmCztJFJPcU6JPo8hmlfO2WeTz7Zge7Ok8EXY6IRIwCfZJ96/Y6Sovi/NGvdJYuIrmlQJ9kyakl/G5zLX/z1gG2f3As6HJEJEIU6AFY8ek6ppUmeOL5nUGXIiIRokAPwIyyIn7v0wt4fttHvNXeHXQ5IhIRCvSA/Ovb5lNZVsTjOksXkRxRoAdkakmCb91ex//b2cnru48EXY6IRIACPUBfu6WW6mklfH/9DoJ6Lr2IRIcCPUBTiuPcf0c9r+85wiuth4IuR0RCToEesK/cNIeaiik6SxeRS6ZAD1hJIs537qznrY6j/Gp7wc+vLSKXIKtAN7OlZrbDzFrN7JFRtn/GzN40s34z++3clxltX7pxNrVVZTz+3A4GB3WWLiIXZ9xAN7M4sAq4G1gE3Gtmi0Z02wfcB/w81wUWgkQ8xoNLFvLeh8dZ9+4HQZcjIiGVzRn6YqDV3dvcvRdYA9yT2cHd97j728DgBNRYEL5w3ZUsvGwqTzy/k/4BvY0icuGyCfQaoD1jvSPddsHMbIWZtZhZS2dn58XsIrLiMeOhJQtp6zzJX205EHQ5IhJCk/qlqLuvdvdGd2+srq6ezEOHwj/9xOVcUzOdJ1/YSW+/ztJF5MJkE+j7gTkZ67PTbZJjZsbDn7uK9iOneLqlffwXiIhkyCbQNwMNZjbfzIqB5cDaiS2rcN2+sJpPzavkj198n9N9A0GXIyIhMm6gu3s/sBJYD2wHnnb3rWb2mJktAzCzm8ysA/gd4MdmtnUii44yM+M/fO4qPjp2hp+9ti/ockQkRBLZdHL3dcC6EW2PZixvJjUUIzlwa10VzfVV/M+XWll+0xzKS7L6zyQiBU53iuaphz93FYdP9vJ/Nu4JuhQRCQkFep66cW4ld149ix//4y6OnuoLuhwRCQEFeh57cMlCjp3u5yev7A66FBEJAQV6HrumZgb/7NrL+cmv2zhysjfockQkzynQ89xDSxZyqm+AH//jrqBLEZE8p0DPc/WzpvHF62v40017OHjsdNDliEgeU6CHwAN3NdA34Kx6qTXoUkQkjynQQ2BeVTlfbpzNz1/fR0dXT9DliEieUqCHxP13NGAYP3xRZ+kiMjoFekhcWTGFf3XzXH75Rge7D50MuhwRyUMK9BD5d5+toyhuPPmrnUGXIiJ5SIEeIrOmlXJf03z++q0D7PzoeNDliEieUaCHzDc/s4Dy4gQ/eF5n6SIynAI9ZCrLi/nGbfP5+3c/5N39R4MuR0TyiAI9hL7x6flUlBXx+HM7gi5FRPKIAj2EppcW8c3P1PHSjk7e2Hsk6HJEJE8o0EPq603zSE4t4fvrNZYuIikK9JAqK07w7c/WsantMH+Zvja9u6eXwUEPujQRCYjmNguxexfP5Sev7ObhX751ti1mUFFWTEVZEZVlxVSWFVFRVszM8o+3VZYVU1leRMWUYooT+mwXCTsFeoiVFsVZu/I23mrvpqunl66ePrp7ejlyspfunj66enrZ332arQeO0dXTy+m+wTH3NbUkcTbwK8qKmFlefHY583fmB0NZcRwzm8Q/sYicT1aBbmZLgSeBOPCUu/+PEdtLgD8DPgUcBr7i7ntyW6qMZmZ5MZ+9elZWfU/1DqSD/1zgd/X00X0y9XtoW1dPH/uO9NB1spdjp/vH3F9xPHY2/CvKiihJxEnEjHjMSMSNmFl6PZb6HU+tn22Pn9set9Rr4jE7u4+hn2H7+Fi7kYjFiMUgEYsRjxkxAzPDgJgZZqR+MGKx9O+htnQ/s3Qb5/oPvTaW7kPG8rn9Du8ztAznjnlumfSyZSyjD0XJmXED3cziwCpgCdABbDazte6+LaPbN4Aud683s+XA94CvTETBcvGmFMeZUjyFKyumZP2a/oFBuk+lzvy7evroOjn8w6Dr5LkPiO5TfQwOOv2DzsDgYPq30z/gDLpnrA+mfg+ta9z/rKFsH/qQObd87sOBYR8O439oZLws4ziWsTz29uFtwyr9WJuN0i+jCkZ+bo38GBvvg220zR/fp42zfax9j75lzIpGq+UC9v3AnQ38809eOdbeL1o2Z+iLgVZ3bwMwszXAPUBmoN8D/H56+Rngh2Zm7q6/qSGXiMdITi0hObVkQo8zOCzgU4E/kBH4AxkfFAODnO2T+aEx9FoH3B13cIdB92Ftgw6Op34P9SNjW0bb4Ih94Odel9pP5n6HjpNaHjK0HTi7PbU8vH1oZbQ+fnZfqbahjufaxz5Gxq4/xjOOea5t+GtHvt5HaWOU4wx/zfACRtYzsryPbx/lDzDuPvy828c61vj9P75lzKAbY8OMKUVjveKSZBPoNUB7xnoHcPNYfdy938yOAlXAocxOZrYCWAEwd+7ciyxZoigWM4pjQ2cy8UBrEQmrSb20wd1Xu3ujuzdWV1dP5qFFRCIvm0DfD8zJWJ+dbhu1j5klgBmkvhwVEZFJkk2gbwYazGy+mRUDy4G1I/qsBb6eXv5t4EWNn4uITK5xx9DTY+IrgfWkBjd/6u5bzewxoMXd1wI/Af7czFqBI6RCX0REJlFW16G7+zpg3Yi2RzOWTwO/k9vSRETkQuh+bxGRiFCgi4hEhAJdRCQiLKiLUcysE9h7kS9PMuKmpQKn92M4vR/n6L0YLgrvxzx3H/VGnsAC/VKYWYu7NwZdR77Q+zGc3o9z9F4MF/X3Q0MuIiIRoUAXEYmIsAb66qALyDN6P4bT+3GO3ovhIv1+hHIMXUREPi6sZ+giIjKCAl1EJCJCF+hmttTMdphZq5k9EnQ9QTGzOWb2kpltM7OtZvZA0DXlAzOLm9lvzOxvg64laGZWYWbPmNl7ZrbdzG4NuqagmNmD6b8n75rZX5hZadA1TYRQBXrG/KZ3A4uAe81sUbBVBaYfeNjdFwG3AN8u4Pci0wPA9qCLyBNPAv/g7lcDn6RA3xczqwG+AzS6+zWknhobySfChirQyZjf1N17gaH5TQuOu3/g7m+ml4+T+staE2xVwTKz2cDngaeCriVoZjYD+AypR1vj7r3u3h1oUcFKAFPSE/CUAQcCrmdChC3QR5vftKBDDMDMaoEbgNcCLiVofwT8J2Aw4DrywXygE/iT9BDUU2ZWHnRRQXD3/cD3gX3AB8BRd38u2KomRtgCXUYws6nAXwL/3t2PBV1PUMzsC8BBd38j6FryRAK4EfiRu98AnAQK8jsnM6sk9S/5+cCVQLmZfTXYqiZG2AI9m/lNC4aZFZEK85+5+7NB1xOwZmCZme0hNRR3h5n932BLClQH0OHuQ/9qe4ZUwBeiu4Dd7t7p7n3As0BTwDVNiLAFejbzmxYEMzNS46Pb3f2JoOsJmrt/191nu3stqf8vXnT3SJ6FZcPdPwTazeyqdNOdwLYASwrSPuAWMytL/725k4h+QZzVFHT5Yqz5TQMuKyjNwNeAd8xsS7rtv6SnCxQBuB/4Wfrkpw343YDrCYS7v2ZmzwBvkro67DdE9BEAuvVfRCQiwjbkIiIiY1Cgi4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQi4v8D1FriLVCd7twAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "84d7c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DNN_logits'] = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "cc4e1395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 20)"
      ]
     },
     "execution_count": 1044,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "id": "baf84eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this after getting logits for test set separately from the top\n",
    "test['DNN_logits'] = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(120, 1056, device=device)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "id": "78b6fb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812</td>\n",
       "      <td>क़र्ज़</td>\n",
       "      <td>چاروا</td>\n",
       "      <td>qərzə</td>\n",
       "      <td>t͡ʃɒrvɒ</td>\n",
       "      <td>debt</td>\n",
       "      <td>Charwa</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488776</td>\n",
       "      <td>0.607569</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>-9.210550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5527</td>\n",
       "      <td>प्रमोद</td>\n",
       "      <td>لذت بسیار</td>\n",
       "      <td>prəmod</td>\n",
       "      <td>lzt bsjɒr</td>\n",
       "      <td>Pramod</td>\n",
       "      <td>So much fun</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384518</td>\n",
       "      <td>0.475078</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, 1, 1, 1, -1, 1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>-11.989496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246</td>\n",
       "      <td>अधीनस्थ</td>\n",
       "      <td>کمک</td>\n",
       "      <td>ad̤iːnstʰə</td>\n",
       "      <td>kmk</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>Help</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>3.512500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365239</td>\n",
       "      <td>0.594256</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>-15.287443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>آرزو</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ɒrzv</td>\n",
       "      <td>position</td>\n",
       "      <td>Wish</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440511</td>\n",
       "      <td>0.695991</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-7.222701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>746</td>\n",
       "      <td>ढुलमुल</td>\n",
       "      <td>اسرار امیز</td>\n",
       "      <td>ɖ̤ulmul</td>\n",
       "      <td>ɒsrɒr ɒmjz</td>\n",
       "      <td>wavering</td>\n",
       "      <td>Mysterious</td>\n",
       "      <td>synonym</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359298</td>\n",
       "      <td>0.718769</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, 1, -1, -1, 1...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-20.950424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>490</td>\n",
       "      <td>तजुर्बा</td>\n",
       "      <td>بهائی</td>\n",
       "      <td>təd͡ʒurbaː</td>\n",
       "      <td>bhɒيʔj</td>\n",
       "      <td>experience</td>\n",
       "      <td>Baha'i</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368733</td>\n",
       "      <td>0.732720</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-9.158766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1118</td>\n",
       "      <td>वरना</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>vərnaː</td>\n",
       "      <td>vrnh</td>\n",
       "      <td>Otherwise</td>\n",
       "      <td>ورنه</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616410</td>\n",
       "      <td>0.478068</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1...</td>\n",
       "      <td>8.675792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>811</td>\n",
       "      <td>लवेबल</td>\n",
       "      <td>دوست داشتنی</td>\n",
       "      <td>ləvebəl</td>\n",
       "      <td>dvst dɒʃtnj</td>\n",
       "      <td>lovable</td>\n",
       "      <td>Lovely</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.359848</td>\n",
       "      <td>0.401515</td>\n",
       "      <td>4.261364</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411980</td>\n",
       "      <td>0.567750</td>\n",
       "      <td>[-1, 1, 1, 1, -1, 1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>-24.917213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1661</td>\n",
       "      <td>तेज़ी</td>\n",
       "      <td>فحش دادن</td>\n",
       "      <td>teziː</td>\n",
       "      <td>fhʃ dɒdn</td>\n",
       "      <td>swiftness</td>\n",
       "      <td>Swearing</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411458</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402701</td>\n",
       "      <td>0.740800</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...</td>\n",
       "      <td>-17.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>252</td>\n",
       "      <td>ख़ानदान</td>\n",
       "      <td>شکاری</td>\n",
       "      <td>xaːndaːn</td>\n",
       "      <td>ʃkɒrj</td>\n",
       "      <td>family</td>\n",
       "      <td>Hunting</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.322917</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523436</td>\n",
       "      <td>0.573659</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>-9.400351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 loan_word original_word loan_word_epitran  \\\n",
       "0           812    क़र्ज़        چاروا              qərzə   \n",
       "1          5527    प्रमोद     لذت بسیار            prəmod   \n",
       "2          1246   अधीनस्थ           کمک        ad̤iːnstʰə   \n",
       "3           137      ओहदा          آرزو             oɦdaː   \n",
       "4           746    ढुलमुल    اسرار امیز           ɖ̤ulmul   \n",
       "..          ...       ...           ...               ...   \n",
       "461         490   तजुर्बा         بهائی        təd͡ʒurbaː   \n",
       "462        1118      वरना          ورنه            vərnaː   \n",
       "463         811     लवेबल   دوست داشتنی           ləvebəl   \n",
       "464        1661     तेज़ी      فحش دادن             teziː   \n",
       "465         252   ख़ानदान         شکاری          xaːndaːn   \n",
       "\n",
       "    original_word_epitran loan_english original_english          label  \\\n",
       "0                t͡ʃɒrvɒ          debt           Charwa  hard_negative   \n",
       "1               lzt bsjɒr       Pramod      So much fun        synonym   \n",
       "2                     kmk  subordinate             Help        synonym   \n",
       "3                    ɒrzv     position             Wish         random   \n",
       "4              ɒsrɒr ɒmjz     wavering       Mysterious        synonym   \n",
       "..                    ...          ...              ...            ...   \n",
       "461                bhɒيʔj   experience           Baha'i         random   \n",
       "462                  vrnh    Otherwise             ورنه           loan   \n",
       "463           dvst dɒʃtnj      lovable           Lovely        synonym   \n",
       "464              fhʃ dɒdn    swiftness         Swearing        synonym   \n",
       "465                 ʃkɒrj       family          Hunting         random   \n",
       "\n",
       "     Fast Levenshtein  Dolgo Prime Distance  Feature Edit Distance  \\\n",
       "0               0.875              0.125000               0.065104   \n",
       "1               1.000              0.777778               0.317130   \n",
       "2               1.000              0.700000               0.420833   \n",
       "3               1.000              0.200000               0.116667   \n",
       "4               0.900              0.500000               0.333333   \n",
       "..                ...                   ...                    ...   \n",
       "461             1.000              0.500000               0.268750   \n",
       "462             0.500              0.333333               0.190972   \n",
       "463             1.000              0.727273               0.359848   \n",
       "464             1.000              0.500000               0.411458   \n",
       "465             1.000              0.625000               0.281250   \n",
       "\n",
       "     Hamming Feature Distance  Weighted Feature Distance  \\\n",
       "0                    0.072917                   0.671875   \n",
       "1                    0.365741                   4.222222   \n",
       "2                    0.466667                   3.512500   \n",
       "3                    0.150000                   2.325000   \n",
       "4                    0.383333                   3.312500   \n",
       "..                        ...                        ...   \n",
       "461                  0.304167                   3.112500   \n",
       "462                  0.215278                   2.145833   \n",
       "463                  0.401515                   4.261364   \n",
       "464                  0.453125                   4.000000   \n",
       "465                  0.322917                   3.718750   \n",
       "\n",
       "     Fast Levenshtein Distance Div Maxlen  label_bin  mbert_cos_similarity  \\\n",
       "0                                   0.875          0              0.488776   \n",
       "1                                   1.000          0              0.384518   \n",
       "2                                   1.000          0              0.365239   \n",
       "3                                   1.000          0              0.440511   \n",
       "4                                   0.900          0              0.359298   \n",
       "..                                    ...        ...                   ...   \n",
       "461                                 1.000          0              0.368733   \n",
       "462                                 0.500          1              0.616410   \n",
       "463                                 1.000          0              0.411980   \n",
       "464                                 1.000          0              0.402701   \n",
       "465                                 1.000          0              0.523436   \n",
       "\n",
       "     xlm_cos_similarity                                      features_loan  \\\n",
       "0              0.607569  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "1              0.475078  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "2              0.594256  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "3              0.695991  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "4              0.718769  [-1, -1, 1, -1, -1, -1, -1, 0, 1, 1, -1, -1, 1...   \n",
       "..                  ...                                                ...   \n",
       "461            0.732720  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "462            0.478068  [-1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1...   \n",
       "463            0.567750  [-1, 1, 1, 1, -1, 1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "464            0.740800  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "465            0.573659  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "\n",
       "                                         features_orig  DNN_logits  \n",
       "0    [-1, -1, 1, -1, 1, -1, -1, 0, -1, -1, -1, -1, ...   -9.210550  \n",
       "1    [-1, 1, 1, 1, -1, 1, -1, 0, 1, -1, -1, 1, 1, -...  -11.989496  \n",
       "2    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...  -15.287443  \n",
       "3    [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   -7.222701  \n",
       "4    [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...  -20.950424  \n",
       "..                                                 ...         ...  \n",
       "461  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   -9.158766  \n",
       "462  [-1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1...    8.675792  \n",
       "463  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...  -24.917213  \n",
       "464  [-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...  -17.452015  \n",
       "465  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   -9.400351  \n",
       "\n",
       "[466 rows x 20 columns]"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4160d",
   "metadata": {},
   "source": [
    "# Pad features to max length with zeros where max length is extended to closest perfect square to the original max length of IPA characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "id": "3600315e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032, 864)"
      ]
     },
     "execution_count": 1072,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1], X_test.shape[1]  # these are original max lengths, so pad them on the right with torch zeros until closest perfect square so that CNNs can work \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "id": "0a49cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CNN = F.pad(X_train,pad =(0, 1089-X_train.shape[1]), value=0)\n",
    "Y_train_CNN = Y_train\n",
    "\n",
    "X_test_CNN = F.pad(X_test,pad =(0, 900-X_test.shape[1]), value=0)\n",
    "Y_test_CNN = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "f74a16fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4194, 1089]),\n",
       " torch.Size([4194, 1]),\n",
       " torch.Size([466, 900]),\n",
       " torch.Size([466, 1]))"
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CNN.shape, Y_train_CNN.shape, X_test_CNN.shape, Y_test_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2154d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db601f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "id": "e7d6a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib64/python3.6/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 2) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128,2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        x = torch.randn(33,33).view(-1,1,33,33) #33 because its the square root of 1089\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.tanh(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "id": "53bd359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "id": "8d641b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#X_train = torch.tensor(X_train).view(-1,23,23).to(device)\n",
    "X_train_CNN = torch.tensor(X_train_CNN).view(-1,33,33).to(device)\n",
    "#X_train = torch.tensor(X_train).to(device)\n",
    "#Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_train_CNN = torch.tensor(Y_train_CNN).to(device)\n",
    "#X_test = torch.tensor(X_test).view(-1,23,23).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "id": "5113feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.tensor(X_train)\n",
    "# Y_train = torch.tensor(Y_train)\n",
    "# X_test = torch.tensor(X_test)\n",
    "# Y_test = torch.tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "ed1e4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_CNN = F.pad(X_train,pad =(0, 1089-X_train.shape[1]), value=0)\n",
    "# Y_train_CNN = Y_train\n",
    "\n",
    "# X_test_CNN = F.pad(X_test,pad =(0, 900-X_test.shape[1]), value=0)\n",
    "# Y_test_CNN = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t4d = torch.ones(3, 3)\n",
    "# p1d = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t4d = F.pad(t4d,pad =(0, 10-t4d.shape[1]), value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t4d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "c9633f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4194, 33, 33]),\n",
       " torch.Size([4194, 1]),\n",
       " torch.Size([466, 900]),\n",
       " torch.Size([466, 1]))"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CNN.shape, Y_train_CNN.shape, X_test_CNN.shape, Y_test_CNN.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "af017f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "f26351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "id": "bf978d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib64/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.7009398341178894\n",
      "Epoch: 1. Loss: 0.6385523676872253\n",
      "Epoch: 2. Loss: 0.6022046804428101\n",
      "Epoch: 3. Loss: 0.6147534251213074\n",
      "Epoch: 4. Loss: 0.6018617153167725\n",
      "Epoch: 5. Loss: 0.5835819244384766\n",
      "Epoch: 6. Loss: 0.5941641926765442\n",
      "Epoch: 7. Loss: 0.5903205871582031\n",
      "Epoch: 8. Loss: 0.5762678384780884\n",
      "Epoch: 9. Loss: 0.5780029892921448\n",
      "Epoch: 10. Loss: 0.5842543840408325\n",
      "Epoch: 11. Loss: 0.5758993029594421\n",
      "Epoch: 12. Loss: 0.5709930062294006\n",
      "Epoch: 13. Loss: 0.5770629644393921\n",
      "Epoch: 14. Loss: 0.577378511428833\n",
      "Epoch: 15. Loss: 0.5711982846260071\n",
      "Epoch: 16. Loss: 0.5716713070869446\n",
      "Epoch: 17. Loss: 0.5757595896720886\n",
      "Epoch: 18. Loss: 0.5729579329490662\n",
      "Epoch: 19. Loss: 0.5694700479507446\n",
      "Epoch: 20. Loss: 0.5713174939155579\n",
      "Epoch: 21. Loss: 0.5725302696228027\n",
      "Epoch: 22. Loss: 0.5698596239089966\n",
      "Epoch: 23. Loss: 0.5682770609855652\n",
      "Epoch: 24. Loss: 0.5697313547134399\n",
      "Epoch: 25. Loss: 0.5701640248298645\n",
      "Epoch: 26. Loss: 0.5683649778366089\n",
      "Epoch: 27. Loss: 0.5675744414329529\n",
      "Epoch: 28. Loss: 0.5685878396034241\n",
      "Epoch: 29. Loss: 0.5686842203140259\n",
      "Epoch: 30. Loss: 0.5673206448554993\n",
      "Epoch: 31. Loss: 0.5668126940727234\n",
      "Epoch: 32. Loss: 0.5674243569374084\n",
      "Epoch: 33. Loss: 0.567097544670105\n",
      "Epoch: 34. Loss: 0.5659469962120056\n",
      "Epoch: 35. Loss: 0.5658115744590759\n",
      "Epoch: 36. Loss: 0.5661112070083618\n",
      "Epoch: 37. Loss: 0.5653679966926575\n",
      "Epoch: 38. Loss: 0.5647262930870056\n",
      "Epoch: 39. Loss: 0.5650064945220947\n",
      "Epoch: 40. Loss: 0.5646767616271973\n",
      "Epoch: 41. Loss: 0.5639352798461914\n",
      "Epoch: 42. Loss: 0.5640195608139038\n",
      "Epoch: 43. Loss: 0.5637918710708618\n",
      "Epoch: 44. Loss: 0.5630765557289124\n",
      "Epoch: 45. Loss: 0.5630409121513367\n",
      "Epoch: 46. Loss: 0.5627355575561523\n",
      "Epoch: 47. Loss: 0.5621106028556824\n",
      "Epoch: 48. Loss: 0.56206214427948\n",
      "Epoch: 49. Loss: 0.5616269111633301\n",
      "Epoch: 50. Loss: 0.5611578822135925\n",
      "Epoch: 51. Loss: 0.561050295829773\n",
      "Epoch: 52. Loss: 0.5604808926582336\n",
      "Epoch: 53. Loss: 0.5602107644081116\n",
      "Epoch: 54. Loss: 0.5598617196083069\n",
      "Epoch: 55. Loss: 0.5593398809432983\n",
      "Epoch: 56. Loss: 0.5591058731079102\n",
      "Epoch: 57. Loss: 0.5585219264030457\n",
      "Epoch: 58. Loss: 0.5582415461540222\n",
      "Epoch: 59. Loss: 0.5576909184455872\n",
      "Epoch: 60. Loss: 0.5573331117630005\n",
      "Epoch: 61. Loss: 0.5568026304244995\n",
      "Epoch: 62. Loss: 0.5564004182815552\n",
      "Epoch: 63. Loss: 0.5558456778526306\n",
      "Epoch: 64. Loss: 0.5554303526878357\n",
      "Epoch: 65. Loss: 0.5548270344734192\n",
      "Epoch: 66. Loss: 0.5544033050537109\n",
      "Epoch: 67. Loss: 0.553780734539032\n",
      "Epoch: 68. Loss: 0.5532658100128174\n",
      "Epoch: 69. Loss: 0.5527194142341614\n",
      "Epoch: 70. Loss: 0.5520493388175964\n",
      "Epoch: 71. Loss: 0.5514874458312988\n",
      "Epoch: 72. Loss: 0.5508875846862793\n",
      "Epoch: 73. Loss: 0.5501687526702881\n",
      "Epoch: 74. Loss: 0.5494429469108582\n",
      "Epoch: 75. Loss: 0.5487598180770874\n",
      "Epoch: 76. Loss: 0.5481080412864685\n",
      "Epoch: 77. Loss: 0.547537088394165\n",
      "Epoch: 78. Loss: 0.547278881072998\n",
      "Epoch: 79. Loss: 0.5481871366500854\n",
      "Epoch: 80. Loss: 0.5504355430603027\n",
      "Epoch: 81. Loss: 0.5516301393508911\n",
      "Epoch: 82. Loss: 0.5448452234268188\n",
      "Epoch: 83. Loss: 0.543630063533783\n",
      "Epoch: 84. Loss: 0.5469936728477478\n",
      "Epoch: 85. Loss: 0.5426623225212097\n",
      "Epoch: 86. Loss: 0.5407614707946777\n",
      "Epoch: 87. Loss: 0.5429439544677734\n",
      "Epoch: 88. Loss: 0.5395327210426331\n",
      "Epoch: 89. Loss: 0.5381669998168945\n",
      "Epoch: 90. Loss: 0.5393946766853333\n",
      "Epoch: 91. Loss: 0.5364596247673035\n",
      "Epoch: 92. Loss: 0.535178542137146\n",
      "Epoch: 93. Loss: 0.5358688831329346\n",
      "Epoch: 94. Loss: 0.5335754752159119\n",
      "Epoch: 95. Loss: 0.5316805243492126\n",
      "Epoch: 96. Loss: 0.5318275094032288\n",
      "Epoch: 97. Loss: 0.5308707356452942\n",
      "Epoch: 98. Loss: 0.5286419987678528\n",
      "Epoch: 99. Loss: 0.5269269347190857\n",
      "Epoch: 100. Loss: 0.5263956785202026\n",
      "Epoch: 101. Loss: 0.5262897610664368\n",
      "Epoch: 102. Loss: 0.5256208181381226\n",
      "Epoch: 103. Loss: 0.5249254703521729\n",
      "Epoch: 104. Loss: 0.5232208371162415\n",
      "Epoch: 105. Loss: 0.5216444134712219\n",
      "Epoch: 106. Loss: 0.5193954110145569\n",
      "Epoch: 107. Loss: 0.5174460411071777\n",
      "Epoch: 108. Loss: 0.5154637694358826\n",
      "Epoch: 109. Loss: 0.5138857960700989\n",
      "Epoch: 110. Loss: 0.5126256942749023\n",
      "Epoch: 111. Loss: 0.5126734375953674\n",
      "Epoch: 112. Loss: 0.5152580142021179\n",
      "Epoch: 113. Loss: 0.5258864760398865\n",
      "Epoch: 114. Loss: 0.5254701375961304\n",
      "Epoch: 115. Loss: 0.5139485597610474\n",
      "Epoch: 116. Loss: 0.5020277500152588\n",
      "Epoch: 117. Loss: 0.5136536359786987\n",
      "Epoch: 118. Loss: 0.5084352493286133\n",
      "Epoch: 119. Loss: 0.49860990047454834\n",
      "Epoch: 120. Loss: 0.5088686943054199\n",
      "Epoch: 121. Loss: 0.49666261672973633\n",
      "Epoch: 122. Loss: 0.49815911054611206\n",
      "Epoch: 123. Loss: 0.4969969689846039\n",
      "Epoch: 124. Loss: 0.4895194172859192\n",
      "Epoch: 125. Loss: 0.4938705861568451\n",
      "Epoch: 126. Loss: 0.48547303676605225\n",
      "Epoch: 127. Loss: 0.4878942668437958\n",
      "Epoch: 128. Loss: 0.48445090651512146\n",
      "Epoch: 129. Loss: 0.47988274693489075\n",
      "Epoch: 130. Loss: 0.4821320176124573\n",
      "Epoch: 131. Loss: 0.47535911202430725\n",
      "Epoch: 132. Loss: 0.4738054871559143\n",
      "Epoch: 133. Loss: 0.47373875975608826\n",
      "Epoch: 134. Loss: 0.4675290584564209\n",
      "Epoch: 135. Loss: 0.46517911553382874\n",
      "Epoch: 136. Loss: 0.4652077853679657\n",
      "Epoch: 137. Loss: 0.46099403500556946\n",
      "Epoch: 138. Loss: 0.4553772509098053\n",
      "Epoch: 139. Loss: 0.45361462235450745\n",
      "Epoch: 140. Loss: 0.45359694957733154\n",
      "Epoch: 141. Loss: 0.45130065083503723\n",
      "Epoch: 142. Loss: 0.44832998514175415\n",
      "Epoch: 143. Loss: 0.44326579570770264\n",
      "Epoch: 144. Loss: 0.4390627145767212\n",
      "Epoch: 145. Loss: 0.43471917510032654\n",
      "Epoch: 146. Loss: 0.4320465922355652\n",
      "Epoch: 147. Loss: 0.4312949776649475\n",
      "Epoch: 148. Loss: 0.4398595690727234\n",
      "Epoch: 149. Loss: 0.4556315541267395\n",
      "Epoch: 150. Loss: 0.4821292757987976\n",
      "Epoch: 151. Loss: 0.4190883934497833\n",
      "Epoch: 152. Loss: 0.42397913336753845\n",
      "Epoch: 153. Loss: 0.4509273171424866\n",
      "Epoch: 154. Loss: 0.4024040400981903\n",
      "Epoch: 155. Loss: 0.43499842286109924\n",
      "Epoch: 156. Loss: 0.42502182722091675\n",
      "Epoch: 157. Loss: 0.40021371841430664\n",
      "Epoch: 158. Loss: 0.42196881771087646\n",
      "Epoch: 159. Loss: 0.39432308077812195\n",
      "Epoch: 160. Loss: 0.40187162160873413\n",
      "Epoch: 161. Loss: 0.3870149254798889\n",
      "Epoch: 162. Loss: 0.3860621750354767\n",
      "Epoch: 163. Loss: 0.3893771767616272\n",
      "Epoch: 164. Loss: 0.3732694387435913\n",
      "Epoch: 165. Loss: 0.37502214312553406\n",
      "Epoch: 166. Loss: 0.3704792559146881\n",
      "Epoch: 167. Loss: 0.36208030581474304\n",
      "Epoch: 168. Loss: 0.3634663224220276\n",
      "Epoch: 169. Loss: 0.3549162447452545\n",
      "Epoch: 170. Loss: 0.3495362401008606\n",
      "Epoch: 171. Loss: 0.34917163848876953\n",
      "Epoch: 172. Loss: 0.341578871011734\n",
      "Epoch: 173. Loss: 0.33467185497283936\n",
      "Epoch: 174. Loss: 0.3337777256965637\n",
      "Epoch: 175. Loss: 0.33045288920402527\n",
      "Epoch: 176. Loss: 0.320818156003952\n",
      "Epoch: 177. Loss: 0.31476303935050964\n",
      "Epoch: 178. Loss: 0.3141733407974243\n",
      "Epoch: 179. Loss: 0.31256818771362305\n",
      "Epoch: 180. Loss: 0.31097957491874695\n",
      "Epoch: 181. Loss: 0.30589839816093445\n",
      "Epoch: 182. Loss: 0.30164021253585815\n",
      "Epoch: 183. Loss: 0.294196218252182\n",
      "Epoch: 184. Loss: 0.28873926401138306\n",
      "Epoch: 185. Loss: 0.2825879156589508\n",
      "Epoch: 186. Loss: 0.27895626425743103\n",
      "Epoch: 187. Loss: 0.27596983313560486\n",
      "Epoch: 188. Loss: 0.27810633182525635\n",
      "Epoch: 189. Loss: 0.2823939025402069\n",
      "Epoch: 190. Loss: 0.295362651348114\n",
      "Epoch: 191. Loss: 0.2880657911300659\n",
      "Epoch: 192. Loss: 0.27033981680870056\n",
      "Epoch: 193. Loss: 0.243282288312912\n",
      "Epoch: 194. Loss: 0.2425740361213684\n",
      "Epoch: 195. Loss: 0.2568652629852295\n",
      "Epoch: 196. Loss: 0.2512173354625702\n",
      "Epoch: 197. Loss: 0.23308467864990234\n",
      "Epoch: 198. Loss: 0.22162559628486633\n",
      "Epoch: 199. Loss: 0.22728055715560913\n",
      "Epoch: 200. Loss: 0.232781782746315\n",
      "Epoch: 201. Loss: 0.2213621884584427\n",
      "Epoch: 202. Loss: 0.2081892192363739\n",
      "Epoch: 203. Loss: 0.20285163819789886\n",
      "Epoch: 204. Loss: 0.2061546891927719\n",
      "Epoch: 205. Loss: 0.2083500325679779\n",
      "Epoch: 206. Loss: 0.20234596729278564\n",
      "Epoch: 207. Loss: 0.19373755156993866\n",
      "Epoch: 208. Loss: 0.18390533328056335\n",
      "Epoch: 209. Loss: 0.1799636036157608\n",
      "Epoch: 210. Loss: 0.17823278903961182\n",
      "Epoch: 211. Loss: 0.17855988442897797\n",
      "Epoch: 212. Loss: 0.1819889396429062\n",
      "Epoch: 213. Loss: 0.18521186709403992\n",
      "Epoch: 214. Loss: 0.19357948005199432\n",
      "Epoch: 215. Loss: 0.19326874613761902\n",
      "Epoch: 216. Loss: 0.18736092746257782\n",
      "Epoch: 217. Loss: 0.1640724241733551\n",
      "Epoch: 218. Loss: 0.1505158245563507\n",
      "Epoch: 219. Loss: 0.150746151804924\n",
      "Epoch: 220. Loss: 0.15746188163757324\n",
      "Epoch: 221. Loss: 0.15861740708351135\n",
      "Epoch: 222. Loss: 0.14544863998889923\n",
      "Epoch: 223. Loss: 0.13634778559207916\n",
      "Epoch: 224. Loss: 0.13220909237861633\n",
      "Epoch: 225. Loss: 0.13495637476444244\n",
      "Epoch: 226. Loss: 0.1367960125207901\n",
      "Epoch: 227. Loss: 0.1312090903520584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228. Loss: 0.12489423155784607\n",
      "Epoch: 229. Loss: 0.117103211581707\n",
      "Epoch: 230. Loss: 0.11482753604650497\n",
      "Epoch: 231. Loss: 0.11459778994321823\n",
      "Epoch: 232. Loss: 0.1143447682261467\n",
      "Epoch: 233. Loss: 0.1144733726978302\n",
      "Epoch: 234. Loss: 0.11110281199216843\n",
      "Epoch: 235. Loss: 0.10820706188678741\n",
      "Epoch: 236. Loss: 0.10286073386669159\n",
      "Epoch: 237. Loss: 0.09829135984182358\n",
      "Epoch: 238. Loss: 0.0944618433713913\n",
      "Epoch: 239. Loss: 0.09122738987207413\n",
      "Epoch: 240. Loss: 0.0895959734916687\n",
      "Epoch: 241. Loss: 0.08811209350824356\n",
      "Epoch: 242. Loss: 0.08797844499349594\n",
      "Epoch: 243. Loss: 0.08901543915271759\n",
      "Epoch: 244. Loss: 0.09330421686172485\n",
      "Epoch: 245. Loss: 0.10340698808431625\n",
      "Epoch: 246. Loss: 0.12280011922121048\n",
      "Epoch: 247. Loss: 0.1472439169883728\n",
      "Epoch: 248. Loss: 0.1557733714580536\n",
      "Epoch: 249. Loss: 0.1088499128818512\n",
      "Epoch: 250. Loss: 0.07551449537277222\n",
      "Epoch: 251. Loss: 0.1008945181965828\n",
      "Epoch: 252. Loss: 0.10211849957704544\n",
      "Epoch: 253. Loss: 0.07745905965566635\n",
      "Epoch: 254. Loss: 0.07688957452774048\n",
      "Epoch: 255. Loss: 0.0822545513510704\n",
      "Epoch: 256. Loss: 0.07285107672214508\n",
      "Epoch: 257. Loss: 0.06800958514213562\n",
      "Epoch: 258. Loss: 0.06919160485267639\n",
      "Epoch: 259. Loss: 0.06699598580598831\n",
      "Epoch: 260. Loss: 0.06227537617087364\n",
      "Epoch: 261. Loss: 0.060268428176641464\n",
      "Epoch: 262. Loss: 0.0610027052462101\n",
      "Epoch: 263. Loss: 0.05839317664504051\n",
      "Epoch: 264. Loss: 0.054745662957429886\n",
      "Epoch: 265. Loss: 0.05484682321548462\n",
      "Epoch: 266. Loss: 0.05411502346396446\n",
      "Epoch: 267. Loss: 0.05130905285477638\n",
      "Epoch: 268. Loss: 0.049750715494155884\n",
      "Epoch: 269. Loss: 0.04975137487053871\n",
      "Epoch: 270. Loss: 0.04816560819745064\n",
      "Epoch: 271. Loss: 0.04571269452571869\n",
      "Epoch: 272. Loss: 0.045768365263938904\n",
      "Epoch: 273. Loss: 0.044842250645160675\n",
      "Epoch: 274. Loss: 0.042584411799907684\n",
      "Epoch: 275. Loss: 0.04235060140490532\n",
      "Epoch: 276. Loss: 0.041448961943387985\n",
      "Epoch: 277. Loss: 0.03994635120034218\n",
      "Epoch: 278. Loss: 0.03940705582499504\n",
      "Epoch: 279. Loss: 0.03816886246204376\n",
      "Epoch: 280. Loss: 0.037492625415325165\n",
      "Epoch: 281. Loss: 0.036896444857120514\n",
      "Epoch: 282. Loss: 0.03544607013463974\n",
      "Epoch: 283. Loss: 0.03501937910914421\n",
      "Epoch: 284. Loss: 0.03452295809984207\n",
      "Epoch: 285. Loss: 0.0332471989095211\n",
      "Epoch: 286. Loss: 0.03271672874689102\n",
      "Epoch: 287. Loss: 0.03223223239183426\n",
      "Epoch: 288. Loss: 0.03134362772107124\n",
      "Epoch: 289. Loss: 0.030714986845850945\n",
      "Epoch: 290. Loss: 0.030111217871308327\n",
      "Epoch: 291. Loss: 0.029513176530599594\n",
      "Epoch: 292. Loss: 0.0289263017475605\n",
      "Epoch: 293. Loss: 0.028260504826903343\n",
      "Epoch: 294. Loss: 0.02780897356569767\n",
      "Epoch: 295. Loss: 0.02726273611187935\n",
      "Epoch: 296. Loss: 0.026616768911480904\n",
      "Epoch: 297. Loss: 0.02621701918542385\n",
      "Epoch: 298. Loss: 0.025717509910464287\n",
      "Epoch: 299. Loss: 0.025157425552606583\n",
      "Epoch: 300. Loss: 0.02474994957447052\n",
      "Epoch: 301. Loss: 0.024281790480017662\n",
      "Epoch: 302. Loss: 0.02382594905793667\n",
      "Epoch: 303. Loss: 0.023412715643644333\n",
      "Epoch: 304. Loss: 0.02296658605337143\n",
      "Epoch: 305. Loss: 0.022585827857255936\n",
      "Epoch: 306. Loss: 0.022183513268828392\n",
      "Epoch: 307. Loss: 0.021778130903840065\n",
      "Epoch: 308. Loss: 0.021434087306261063\n",
      "Epoch: 309. Loss: 0.021055154502391815\n",
      "Epoch: 310. Loss: 0.020689714699983597\n",
      "Epoch: 311. Loss: 0.02036597579717636\n",
      "Epoch: 312. Loss: 0.02001257985830307\n",
      "Epoch: 313. Loss: 0.019688423722982407\n",
      "Epoch: 314. Loss: 0.019376322627067566\n",
      "Epoch: 315. Loss: 0.019055968150496483\n",
      "Epoch: 316. Loss: 0.01875772327184677\n",
      "Epoch: 317. Loss: 0.01845920830965042\n",
      "Epoch: 318. Loss: 0.0181745532900095\n",
      "Epoch: 319. Loss: 0.0178950447589159\n",
      "Epoch: 320. Loss: 0.017614144831895828\n",
      "Epoch: 321. Loss: 0.017354847863316536\n",
      "Epoch: 322. Loss: 0.017090823501348495\n",
      "Epoch: 323. Loss: 0.016834432259202003\n",
      "Epoch: 324. Loss: 0.01659328304231167\n",
      "Epoch: 325. Loss: 0.01634838618338108\n",
      "Epoch: 326. Loss: 0.016113879159092903\n",
      "Epoch: 327. Loss: 0.015886392444372177\n",
      "Epoch: 328. Loss: 0.015658920630812645\n",
      "Epoch: 329. Loss: 0.015443411655724049\n",
      "Epoch: 330. Loss: 0.015230845659971237\n",
      "Epoch: 331. Loss: 0.015021413564682007\n",
      "Epoch: 332. Loss: 0.01481928862631321\n",
      "Epoch: 333. Loss: 0.014620170928537846\n",
      "Epoch: 334. Loss: 0.01442739088088274\n",
      "Epoch: 335. Loss: 0.01423696894198656\n",
      "Epoch: 336. Loss: 0.014052055776119232\n",
      "Epoch: 337. Loss: 0.013870878145098686\n",
      "Epoch: 338. Loss: 0.013694164343178272\n",
      "Epoch: 339. Loss: 0.01352070551365614\n",
      "Epoch: 340. Loss: 0.013350949622690678\n",
      "Epoch: 341. Loss: 0.013185745105147362\n",
      "Epoch: 342. Loss: 0.013023328967392445\n",
      "Epoch: 343. Loss: 0.01286399643868208\n",
      "Epoch: 344. Loss: 0.012709733098745346\n",
      "Epoch: 345. Loss: 0.012557423673570156\n",
      "Epoch: 346. Loss: 0.012408643029630184\n",
      "Epoch: 347. Loss: 0.012263115495443344\n",
      "Epoch: 348. Loss: 0.012120746076107025\n",
      "Epoch: 349. Loss: 0.011982564814388752\n",
      "Epoch: 350. Loss: 0.011845484375953674\n",
      "Epoch: 351. Loss: 0.011712566018104553\n",
      "Epoch: 352. Loss: 0.011581676080822945\n",
      "Epoch: 353. Loss: 0.011453140527009964\n",
      "Epoch: 354. Loss: 0.011329102329909801\n",
      "Epoch: 355. Loss: 0.011205008253455162\n",
      "Epoch: 356. Loss: 0.011083998717367649\n",
      "Epoch: 357. Loss: 0.010964500717818737\n",
      "Epoch: 358. Loss: 0.010850075632333755\n",
      "Epoch: 359. Loss: 0.010736479423940182\n",
      "Epoch: 360. Loss: 0.010625405237078667\n",
      "Epoch: 361. Loss: 0.010515219531953335\n",
      "Epoch: 362. Loss: 0.01040853001177311\n",
      "Epoch: 363. Loss: 0.010303977876901627\n",
      "Epoch: 364. Loss: 0.010199840180575848\n",
      "Epoch: 365. Loss: 0.010100041516125202\n",
      "Epoch: 366. Loss: 0.010000807233154774\n",
      "Epoch: 367. Loss: 0.009903186932206154\n",
      "Epoch: 368. Loss: 0.00980701856315136\n",
      "Epoch: 369. Loss: 0.009713663719594479\n",
      "Epoch: 370. Loss: 0.009621855802834034\n",
      "Epoch: 371. Loss: 0.009531555697321892\n",
      "Epoch: 372. Loss: 0.009443525224924088\n",
      "Epoch: 373. Loss: 0.009355451911687851\n",
      "Epoch: 374. Loss: 0.009270178154110909\n",
      "Epoch: 375. Loss: 0.009186490438878536\n",
      "Epoch: 376. Loss: 0.009104195982217789\n",
      "Epoch: 377. Loss: 0.009023717604577541\n",
      "Epoch: 378. Loss: 0.008942835964262486\n",
      "Epoch: 379. Loss: 0.008865040726959705\n",
      "Epoch: 380. Loss: 0.008788295090198517\n",
      "Epoch: 381. Loss: 0.008713705465197563\n",
      "Epoch: 382. Loss: 0.008638684637844563\n",
      "Epoch: 383. Loss: 0.008565204218029976\n",
      "Epoch: 384. Loss: 0.008494032546877861\n",
      "Epoch: 385. Loss: 0.008423754014074802\n",
      "Epoch: 386. Loss: 0.008353296667337418\n",
      "Epoch: 387. Loss: 0.008285867981612682\n",
      "Epoch: 388. Loss: 0.00821857899427414\n",
      "Epoch: 389. Loss: 0.008153066970407963\n",
      "Epoch: 390. Loss: 0.00808732956647873\n",
      "Epoch: 391. Loss: 0.00802334863692522\n",
      "Epoch: 392. Loss: 0.00796104222536087\n",
      "Epoch: 393. Loss: 0.007898678071796894\n",
      "Epoch: 394. Loss: 0.007837595418095589\n",
      "Epoch: 395. Loss: 0.00777754932641983\n",
      "Epoch: 396. Loss: 0.007718037813901901\n",
      "Epoch: 397. Loss: 0.0076592485420405865\n",
      "Epoch: 398. Loss: 0.007602438796311617\n",
      "Epoch: 399. Loss: 0.007546873297542334\n",
      "Epoch: 400. Loss: 0.007490704767405987\n",
      "Epoch: 401. Loss: 0.007436071988195181\n",
      "Epoch: 402. Loss: 0.007382179144769907\n",
      "Epoch: 403. Loss: 0.00732998363673687\n",
      "Epoch: 404. Loss: 0.007276230026036501\n",
      "Epoch: 405. Loss: 0.007225595414638519\n",
      "Epoch: 406. Loss: 0.007174620404839516\n",
      "Epoch: 407. Loss: 0.007124618627130985\n",
      "Epoch: 408. Loss: 0.007076180074363947\n",
      "Epoch: 409. Loss: 0.00702670169994235\n",
      "Epoch: 410. Loss: 0.006978850346058607\n",
      "Epoch: 411. Loss: 0.006932088173925877\n",
      "Epoch: 412. Loss: 0.006885858252644539\n",
      "Epoch: 413. Loss: 0.006839812733232975\n",
      "Epoch: 414. Loss: 0.006794464308768511\n",
      "Epoch: 415. Loss: 0.0067503186874091625\n",
      "Epoch: 416. Loss: 0.006705991458147764\n",
      "Epoch: 417. Loss: 0.006663098465651274\n",
      "Epoch: 418. Loss: 0.006620955653488636\n",
      "Epoch: 419. Loss: 0.006577763240784407\n",
      "Epoch: 420. Loss: 0.006537487264722586\n",
      "Epoch: 421. Loss: 0.006496493238955736\n",
      "Epoch: 422. Loss: 0.006456377916038036\n",
      "Epoch: 423. Loss: 0.006416608579456806\n",
      "Epoch: 424. Loss: 0.006377832032740116\n",
      "Epoch: 425. Loss: 0.006339550483971834\n",
      "Epoch: 426. Loss: 0.006301289889961481\n",
      "Epoch: 427. Loss: 0.006263545714318752\n",
      "Epoch: 428. Loss: 0.006226561032235622\n",
      "Epoch: 429. Loss: 0.006190267391502857\n",
      "Epoch: 430. Loss: 0.006153668276965618\n",
      "Epoch: 431. Loss: 0.006117712706327438\n",
      "Epoch: 432. Loss: 0.006083282642066479\n",
      "Epoch: 433. Loss: 0.00604831101372838\n",
      "Epoch: 434. Loss: 0.006014019250869751\n",
      "Epoch: 435. Loss: 0.005980602465569973\n",
      "Epoch: 436. Loss: 0.00594667112454772\n",
      "Epoch: 437. Loss: 0.0059144869446754456\n",
      "Epoch: 438. Loss: 0.0058813090436160564\n",
      "Epoch: 439. Loss: 0.0058499351143836975\n",
      "Epoch: 440. Loss: 0.00581769784912467\n",
      "Epoch: 441. Loss: 0.005786503665149212\n",
      "Epoch: 442. Loss: 0.0057558538392186165\n",
      "Epoch: 443. Loss: 0.005725063383579254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444. Loss: 0.005694434512406588\n",
      "Epoch: 445. Loss: 0.005664716009050608\n",
      "Epoch: 446. Loss: 0.005635578650981188\n",
      "Epoch: 447. Loss: 0.00560604827478528\n",
      "Epoch: 448. Loss: 0.005577745847404003\n",
      "Epoch: 449. Loss: 0.005548801738768816\n",
      "Epoch: 450. Loss: 0.005521005485206842\n",
      "Epoch: 451. Loss: 0.005493107717484236\n",
      "Epoch: 452. Loss: 0.005465690512210131\n",
      "Epoch: 453. Loss: 0.00543940020725131\n",
      "Epoch: 454. Loss: 0.005412394180893898\n",
      "Epoch: 455. Loss: 0.005386040546000004\n",
      "Epoch: 456. Loss: 0.00536027317866683\n",
      "Epoch: 457. Loss: 0.00533439964056015\n",
      "Epoch: 458. Loss: 0.0053087081760168076\n",
      "Epoch: 459. Loss: 0.005284029990434647\n",
      "Epoch: 460. Loss: 0.005258786026388407\n",
      "Epoch: 461. Loss: 0.005234213080257177\n",
      "Epoch: 462. Loss: 0.0052101644687354565\n",
      "Epoch: 463. Loss: 0.005185751244425774\n",
      "Epoch: 464. Loss: 0.00516217527911067\n",
      "Epoch: 465. Loss: 0.005138570908457041\n",
      "Epoch: 466. Loss: 0.005115796346217394\n",
      "Epoch: 467. Loss: 0.005092400126159191\n",
      "Epoch: 468. Loss: 0.005069897044450045\n",
      "Epoch: 469. Loss: 0.005047587212175131\n",
      "Epoch: 470. Loss: 0.005025025922805071\n",
      "Epoch: 471. Loss: 0.005003392230719328\n",
      "Epoch: 472. Loss: 0.004981302656233311\n",
      "Epoch: 473. Loss: 0.004960567224770784\n",
      "Epoch: 474. Loss: 0.0049386839382350445\n",
      "Epoch: 475. Loss: 0.004917907994240522\n",
      "Epoch: 476. Loss: 0.004896966274827719\n",
      "Epoch: 477. Loss: 0.004876658320426941\n",
      "Epoch: 478. Loss: 0.004856377840042114\n",
      "Epoch: 479. Loss: 0.004836178384721279\n",
      "Epoch: 480. Loss: 0.004816174507141113\n",
      "Epoch: 481. Loss: 0.004796421620994806\n",
      "Epoch: 482. Loss: 0.004776834975928068\n",
      "Epoch: 483. Loss: 0.00475796265527606\n",
      "Epoch: 484. Loss: 0.004738690797239542\n",
      "Epoch: 485. Loss: 0.004720212426036596\n",
      "Epoch: 486. Loss: 0.004701338242739439\n",
      "Epoch: 487. Loss: 0.004682658240199089\n",
      "Epoch: 488. Loss: 0.004664532840251923\n",
      "Epoch: 489. Loss: 0.004646370653063059\n",
      "Epoch: 490. Loss: 0.004628426861017942\n",
      "Epoch: 491. Loss: 0.004610865842550993\n",
      "Epoch: 492. Loss: 0.0045934212394058704\n",
      "Epoch: 493. Loss: 0.00457592960447073\n",
      "Epoch: 494. Loss: 0.0045584156177937984\n",
      "Epoch: 495. Loss: 0.004541915841400623\n",
      "Epoch: 496. Loss: 0.004525274503976107\n",
      "Epoch: 497. Loss: 0.0045081861317157745\n",
      "Epoch: 498. Loss: 0.004491896368563175\n",
      "Epoch: 499. Loss: 0.004475802183151245\n",
      "Epoch: 500. Loss: 0.004459411837160587\n",
      "Epoch: 501. Loss: 0.004443361423909664\n",
      "Epoch: 502. Loss: 0.004427679348737001\n",
      "Epoch: 503. Loss: 0.004411653149873018\n",
      "Epoch: 504. Loss: 0.004396582487970591\n",
      "Epoch: 505. Loss: 0.004380884114652872\n",
      "Epoch: 506. Loss: 0.004366166889667511\n",
      "Epoch: 507. Loss: 0.004351019859313965\n",
      "Epoch: 508. Loss: 0.004336347803473473\n",
      "Epoch: 509. Loss: 0.004321059677749872\n",
      "Epoch: 510. Loss: 0.004306831397116184\n",
      "Epoch: 511. Loss: 0.00429221848025918\n",
      "Epoch: 512. Loss: 0.004278112202882767\n",
      "Epoch: 513. Loss: 0.004263357724994421\n",
      "Epoch: 514. Loss: 0.0042499215342104435\n",
      "Epoch: 515. Loss: 0.004235646687448025\n",
      "Epoch: 516. Loss: 0.00422185193747282\n",
      "Epoch: 517. Loss: 0.004208208527415991\n",
      "Epoch: 518. Loss: 0.004194540902972221\n",
      "Epoch: 519. Loss: 0.004181428346782923\n",
      "Epoch: 520. Loss: 0.004167983774095774\n",
      "Epoch: 521. Loss: 0.004154858645051718\n",
      "Epoch: 522. Loss: 0.004141626879572868\n",
      "Epoch: 523. Loss: 0.00412908336147666\n",
      "Epoch: 524. Loss: 0.004115895833820105\n",
      "Epoch: 525. Loss: 0.004103537183254957\n",
      "Epoch: 526. Loss: 0.004090697038918734\n",
      "Epoch: 527. Loss: 0.004078363534063101\n",
      "Epoch: 528. Loss: 0.004066157154738903\n",
      "Epoch: 529. Loss: 0.004053959157317877\n",
      "Epoch: 530. Loss: 0.004041584208607674\n",
      "Epoch: 531. Loss: 0.004029645584523678\n",
      "Epoch: 532. Loss: 0.004017682746052742\n",
      "Epoch: 533. Loss: 0.004006208851933479\n",
      "Epoch: 534. Loss: 0.00399411004036665\n",
      "Epoch: 535. Loss: 0.003982784226536751\n",
      "Epoch: 536. Loss: 0.003971257247030735\n",
      "Epoch: 537. Loss: 0.0039601195603609085\n",
      "Epoch: 538. Loss: 0.0039484635926783085\n",
      "Epoch: 539. Loss: 0.003937509376555681\n",
      "Epoch: 540. Loss: 0.003926231525838375\n",
      "Epoch: 541. Loss: 0.003915294073522091\n",
      "Epoch: 542. Loss: 0.003904357785359025\n",
      "Epoch: 543. Loss: 0.003893404733389616\n",
      "Epoch: 544. Loss: 0.0038826889358460903\n",
      "Epoch: 545. Loss: 0.003872206201776862\n",
      "Epoch: 546. Loss: 0.0038618457037955523\n",
      "Epoch: 547. Loss: 0.0038509033620357513\n",
      "Epoch: 548. Loss: 0.0038406644016504288\n",
      "Epoch: 549. Loss: 0.0038305865600705147\n",
      "Epoch: 550. Loss: 0.0038201056886464357\n",
      "Epoch: 551. Loss: 0.0038102874532341957\n",
      "Epoch: 552. Loss: 0.0038000938948243856\n",
      "Epoch: 553. Loss: 0.003790049348026514\n",
      "Epoch: 554. Loss: 0.003780474653467536\n",
      "Epoch: 555. Loss: 0.003770574228838086\n",
      "Epoch: 556. Loss: 0.0037607515696436167\n",
      "Epoch: 557. Loss: 0.0037513987626880407\n",
      "Epoch: 558. Loss: 0.0037415847182273865\n",
      "Epoch: 559. Loss: 0.003732457058504224\n",
      "Epoch: 560. Loss: 0.0037227764260023832\n",
      "Epoch: 561. Loss: 0.0037137791514396667\n",
      "Epoch: 562. Loss: 0.0037044265773147345\n",
      "Epoch: 563. Loss: 0.003695333143696189\n",
      "Epoch: 564. Loss: 0.0036863212008029222\n",
      "Epoch: 565. Loss: 0.0036772426683455706\n",
      "Epoch: 566. Loss: 0.003668334102258086\n",
      "Epoch: 567. Loss: 0.0036593570839613676\n",
      "Epoch: 568. Loss: 0.0036507199984043837\n",
      "Epoch: 569. Loss: 0.0036419436801224947\n",
      "Epoch: 570. Loss: 0.003633294953033328\n",
      "Epoch: 571. Loss: 0.003624462755396962\n",
      "Epoch: 572. Loss: 0.0036163812037557364\n",
      "Epoch: 573. Loss: 0.0036076984833925962\n",
      "Epoch: 574. Loss: 0.003599669551476836\n",
      "Epoch: 575. Loss: 0.0035909186117351055\n",
      "Epoch: 576. Loss: 0.0035829481203109026\n",
      "Epoch: 577. Loss: 0.0035746865905821323\n",
      "Epoch: 578. Loss: 0.0035666264593601227\n",
      "Epoch: 579. Loss: 0.003558551426976919\n",
      "Epoch: 580. Loss: 0.003550552763044834\n",
      "Epoch: 581. Loss: 0.0035425974056124687\n",
      "Epoch: 582. Loss: 0.003534823190420866\n",
      "Epoch: 583. Loss: 0.0035268361680209637\n",
      "Epoch: 584. Loss: 0.0035194293595850468\n",
      "Epoch: 585. Loss: 0.0035116239450871944\n",
      "Epoch: 586. Loss: 0.0035040914081037045\n",
      "Epoch: 587. Loss: 0.003496210090816021\n",
      "Epoch: 588. Loss: 0.003489135531708598\n",
      "Epoch: 589. Loss: 0.0034813934471458197\n",
      "Epoch: 590. Loss: 0.003473825752735138\n",
      "Epoch: 591. Loss: 0.0034666024148464203\n",
      "Epoch: 592. Loss: 0.003459549741819501\n",
      "Epoch: 593. Loss: 0.0034522325731813908\n",
      "Epoch: 594. Loss: 0.0034451319370418787\n",
      "Epoch: 595. Loss: 0.003437824547290802\n",
      "Epoch: 596. Loss: 0.0034306864254176617\n",
      "Epoch: 597. Loss: 0.0034237708896398544\n",
      "Epoch: 598. Loss: 0.00341684278100729\n",
      "Epoch: 599. Loss: 0.0034101118799299\n",
      "Epoch: 600. Loss: 0.0034029821399599314\n",
      "Epoch: 601. Loss: 0.0033962067682296038\n",
      "Epoch: 602. Loss: 0.003389426739886403\n",
      "Epoch: 603. Loss: 0.0033827670849859715\n",
      "Epoch: 604. Loss: 0.0033760727383196354\n",
      "Epoch: 605. Loss: 0.003369556739926338\n",
      "Epoch: 606. Loss: 0.003362837713211775\n",
      "Epoch: 607. Loss: 0.0033564988989382982\n",
      "Epoch: 608. Loss: 0.0033499712590128183\n",
      "Epoch: 609. Loss: 0.0033435607329010963\n",
      "Epoch: 610. Loss: 0.0033371862955391407\n",
      "Epoch: 611. Loss: 0.003330677282065153\n",
      "Epoch: 612. Loss: 0.0033243130892515182\n",
      "Epoch: 613. Loss: 0.0033181076869368553\n",
      "Epoch: 614. Loss: 0.0033120918087661266\n",
      "Epoch: 615. Loss: 0.0033057895489037037\n",
      "Epoch: 616. Loss: 0.0032996146474033594\n",
      "Epoch: 617. Loss: 0.003293519141152501\n",
      "Epoch: 618. Loss: 0.0032875980250537395\n",
      "Epoch: 619. Loss: 0.0032816228922456503\n",
      "Epoch: 620. Loss: 0.0032755567226558924\n",
      "Epoch: 621. Loss: 0.003269600449129939\n",
      "Epoch: 622. Loss: 0.0032637629192322493\n",
      "Epoch: 623. Loss: 0.0032580934930592775\n",
      "Epoch: 624. Loss: 0.0032523502595722675\n",
      "Epoch: 625. Loss: 0.0032465485855937004\n",
      "Epoch: 626. Loss: 0.003240786725655198\n",
      "Epoch: 627. Loss: 0.003235133830457926\n",
      "Epoch: 628. Loss: 0.003229440888389945\n",
      "Epoch: 629. Loss: 0.0032237654086202383\n",
      "Epoch: 630. Loss: 0.00321844476275146\n",
      "Epoch: 631. Loss: 0.003212900133803487\n",
      "Epoch: 632. Loss: 0.003207325004041195\n",
      "Epoch: 633. Loss: 0.0032019419595599174\n",
      "Epoch: 634. Loss: 0.003196420380845666\n",
      "Epoch: 635. Loss: 0.0031911483965814114\n",
      "Epoch: 636. Loss: 0.0031857085414230824\n",
      "Epoch: 637. Loss: 0.0031805806793272495\n",
      "Epoch: 638. Loss: 0.0031750942580401897\n",
      "Epoch: 639. Loss: 0.0031698395032435656\n",
      "Epoch: 640. Loss: 0.0031647835858166218\n",
      "Epoch: 641. Loss: 0.0031597048509866\n",
      "Epoch: 642. Loss: 0.0031545651145279408\n",
      "Epoch: 643. Loss: 0.0031493925489485264\n",
      "Epoch: 644. Loss: 0.003144396934658289\n",
      "Epoch: 645. Loss: 0.003139348467811942\n",
      "Epoch: 646. Loss: 0.0031342478469014168\n",
      "Epoch: 647. Loss: 0.003129424760118127\n",
      "Epoch: 648. Loss: 0.003124316455796361\n",
      "Epoch: 649. Loss: 0.003119585569947958\n",
      "Epoch: 650. Loss: 0.0031146970577538013\n",
      "Epoch: 651. Loss: 0.0031098329927772284\n",
      "Epoch: 652. Loss: 0.0031051950063556433\n",
      "Epoch: 653. Loss: 0.0031002170871943235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 654. Loss: 0.003095675492659211\n",
      "Epoch: 655. Loss: 0.00309099187143147\n",
      "Epoch: 656. Loss: 0.0030862255953252316\n",
      "Epoch: 657. Loss: 0.0030816735234111547\n",
      "Epoch: 658. Loss: 0.0030769656877964735\n",
      "Epoch: 659. Loss: 0.0030724876560270786\n",
      "Epoch: 660. Loss: 0.003067800309509039\n",
      "Epoch: 661. Loss: 0.0030633171554654837\n",
      "Epoch: 662. Loss: 0.0030589255038648844\n",
      "Epoch: 663. Loss: 0.003054528497159481\n",
      "Epoch: 664. Loss: 0.003050039289519191\n",
      "Epoch: 665. Loss: 0.003045563120394945\n",
      "Epoch: 666. Loss: 0.003041235962882638\n",
      "Epoch: 667. Loss: 0.003036926733329892\n",
      "Epoch: 668. Loss: 0.0030325488187372684\n",
      "Epoch: 669. Loss: 0.003028411651030183\n",
      "Epoch: 670. Loss: 0.0030238579493016005\n",
      "Epoch: 671. Loss: 0.0030197016894817352\n",
      "Epoch: 672. Loss: 0.00301562063395977\n",
      "Epoch: 673. Loss: 0.0030113684479147196\n",
      "Epoch: 674. Loss: 0.003007130231708288\n",
      "Epoch: 675. Loss: 0.003002996789291501\n",
      "Epoch: 676. Loss: 0.002998931799083948\n",
      "Epoch: 677. Loss: 0.0029948714654892683\n",
      "Epoch: 678. Loss: 0.0029906746931374073\n",
      "Epoch: 679. Loss: 0.0029866411350667477\n",
      "Epoch: 680. Loss: 0.00298270839266479\n",
      "Epoch: 681. Loss: 0.0029786857776343822\n",
      "Epoch: 682. Loss: 0.002974736038595438\n",
      "Epoch: 683. Loss: 0.0029707057401537895\n",
      "Epoch: 684. Loss: 0.002966768341138959\n",
      "Epoch: 685. Loss: 0.002962915226817131\n",
      "Epoch: 686. Loss: 0.0029590530321002007\n",
      "Epoch: 687. Loss: 0.002955189673230052\n",
      "Epoch: 688. Loss: 0.0029512280598282814\n",
      "Epoch: 689. Loss: 0.0029474622569978237\n",
      "Epoch: 690. Loss: 0.002943889470770955\n",
      "Epoch: 691. Loss: 0.002940082922577858\n",
      "Epoch: 692. Loss: 0.0029361569322645664\n",
      "Epoch: 693. Loss: 0.0029324449133127928\n",
      "Epoch: 694. Loss: 0.002928866073489189\n",
      "Epoch: 695. Loss: 0.0029252266976982355\n",
      "Epoch: 696. Loss: 0.0029215121176093817\n",
      "Epoch: 697. Loss: 0.0029178301338106394\n",
      "Epoch: 698. Loss: 0.0029142324347049\n",
      "Epoch: 699. Loss: 0.0029105704743415117\n",
      "Epoch: 700. Loss: 0.002907038200646639\n",
      "Epoch: 701. Loss: 0.0029034940525889397\n",
      "Epoch: 702. Loss: 0.002899955026805401\n",
      "Epoch: 703. Loss: 0.0028965782839804888\n",
      "Epoch: 704. Loss: 0.0028929898981004953\n",
      "Epoch: 705. Loss: 0.0028894622810184956\n",
      "Epoch: 706. Loss: 0.0028861667960882187\n",
      "Epoch: 707. Loss: 0.00288255512714386\n",
      "Epoch: 708. Loss: 0.0028791408985853195\n",
      "Epoch: 709. Loss: 0.0028759075794368982\n",
      "Epoch: 710. Loss: 0.0028724921867251396\n",
      "Epoch: 711. Loss: 0.0028690630570054054\n",
      "Epoch: 712. Loss: 0.002865731483325362\n",
      "Epoch: 713. Loss: 0.002862496068701148\n",
      "Epoch: 714. Loss: 0.0028590173460543156\n",
      "Epoch: 715. Loss: 0.0028559197671711445\n",
      "Epoch: 716. Loss: 0.002852502977475524\n",
      "Epoch: 717. Loss: 0.00284935487434268\n",
      "Epoch: 718. Loss: 0.002846125280484557\n",
      "Epoch: 719. Loss: 0.0028430584352463484\n",
      "Epoch: 720. Loss: 0.00283976667560637\n",
      "Epoch: 721. Loss: 0.0028364728204905987\n",
      "Epoch: 722. Loss: 0.0028334700036793947\n",
      "Epoch: 723. Loss: 0.002830266021192074\n",
      "Epoch: 724. Loss: 0.0028272371273487806\n",
      "Epoch: 725. Loss: 0.002823957009240985\n",
      "Epoch: 726. Loss: 0.002820960246026516\n",
      "Epoch: 727. Loss: 0.002817797474563122\n",
      "Epoch: 728. Loss: 0.0028148554265499115\n",
      "Epoch: 729. Loss: 0.002811931073665619\n",
      "Epoch: 730. Loss: 0.0028087832033634186\n",
      "Epoch: 731. Loss: 0.0028057785239070654\n",
      "Epoch: 732. Loss: 0.0028027265798300505\n",
      "Epoch: 733. Loss: 0.00279984506778419\n",
      "Epoch: 734. Loss: 0.002796925837174058\n",
      "Epoch: 735. Loss: 0.002794004511088133\n",
      "Epoch: 736. Loss: 0.002791021019220352\n",
      "Epoch: 737. Loss: 0.002788013545796275\n",
      "Epoch: 738. Loss: 0.0027852184139192104\n",
      "Epoch: 739. Loss: 0.002782331081107259\n",
      "Epoch: 740. Loss: 0.002779407659545541\n",
      "Epoch: 741. Loss: 0.0027767280116677284\n",
      "Epoch: 742. Loss: 0.0027737771160900593\n",
      "Epoch: 743. Loss: 0.002771099330857396\n",
      "Epoch: 744. Loss: 0.002768098609521985\n",
      "Epoch: 745. Loss: 0.0027652743738144636\n",
      "Epoch: 746. Loss: 0.002762662945315242\n",
      "Epoch: 747. Loss: 0.0027599961031228304\n",
      "Epoch: 748. Loss: 0.002757241250947118\n",
      "Epoch: 749. Loss: 0.002754399785771966\n",
      "Epoch: 750. Loss: 0.0027516065165400505\n",
      "Epoch: 751. Loss: 0.0027489839121699333\n",
      "Epoch: 752. Loss: 0.0027462379075586796\n",
      "Epoch: 753. Loss: 0.0027435352094471455\n",
      "Epoch: 754. Loss: 0.002741072792559862\n",
      "Epoch: 755. Loss: 0.002738439943641424\n",
      "Epoch: 756. Loss: 0.0027358357328921556\n",
      "Epoch: 757. Loss: 0.0027331083547323942\n",
      "Epoch: 758. Loss: 0.002730535576120019\n",
      "Epoch: 759. Loss: 0.002727908780798316\n",
      "Epoch: 760. Loss: 0.0027254626620560884\n",
      "Epoch: 761. Loss: 0.0027229248080402613\n",
      "Epoch: 762. Loss: 0.002720259828492999\n",
      "Epoch: 763. Loss: 0.0027177275624126196\n",
      "Epoch: 764. Loss: 0.0027151580434292555\n",
      "Epoch: 765. Loss: 0.002712798770517111\n",
      "Epoch: 766. Loss: 0.0027102974709123373\n",
      "Epoch: 767. Loss: 0.0027077440172433853\n",
      "Epoch: 768. Loss: 0.0027052315417677164\n",
      "Epoch: 769. Loss: 0.0027027628384530544\n",
      "Epoch: 770. Loss: 0.002700346289202571\n",
      "Epoch: 771. Loss: 0.0026978992391377687\n",
      "Epoch: 772. Loss: 0.002695581642910838\n",
      "Epoch: 773. Loss: 0.00269313040189445\n",
      "Epoch: 774. Loss: 0.002690749242901802\n",
      "Epoch: 775. Loss: 0.0026883636601269245\n",
      "Epoch: 776. Loss: 0.0026860134676098824\n",
      "Epoch: 777. Loss: 0.0026836541946977377\n",
      "Epoch: 778. Loss: 0.0026811682619154453\n",
      "Epoch: 779. Loss: 0.002678923076018691\n",
      "Epoch: 780. Loss: 0.002676560077816248\n",
      "Epoch: 781. Loss: 0.0026741856709122658\n",
      "Epoch: 782. Loss: 0.0026719518937170506\n",
      "Epoch: 783. Loss: 0.0026696650311350822\n",
      "Epoch: 784. Loss: 0.0026674785185605288\n",
      "Epoch: 785. Loss: 0.002665196545422077\n",
      "Epoch: 786. Loss: 0.002662901533767581\n",
      "Epoch: 787. Loss: 0.0026606679894030094\n",
      "Epoch: 788. Loss: 0.0026584186125546694\n",
      "Epoch: 789. Loss: 0.0026562423445284367\n",
      "Epoch: 790. Loss: 0.0026540406979620457\n",
      "Epoch: 791. Loss: 0.002651641145348549\n",
      "Epoch: 792. Loss: 0.002649628324434161\n",
      "Epoch: 793. Loss: 0.002647373592481017\n",
      "Epoch: 794. Loss: 0.0026451542507857084\n",
      "Epoch: 795. Loss: 0.0026431172154843807\n",
      "Epoch: 796. Loss: 0.0026410326827317476\n",
      "Epoch: 797. Loss: 0.002638617530465126\n",
      "Epoch: 798. Loss: 0.0026366643141955137\n",
      "Epoch: 799. Loss: 0.002634542528539896\n",
      "Epoch: 800. Loss: 0.0026323955971747637\n",
      "Epoch: 801. Loss: 0.0026303669437766075\n",
      "Epoch: 802. Loss: 0.0026283240877091885\n",
      "Epoch: 803. Loss: 0.0026262367609888315\n",
      "Epoch: 804. Loss: 0.0026240141596645117\n",
      "Epoch: 805. Loss: 0.002621930791065097\n",
      "Epoch: 806. Loss: 0.002619924722239375\n",
      "Epoch: 807. Loss: 0.0026180099230259657\n",
      "Epoch: 808. Loss: 0.0026159430854022503\n",
      "Epoch: 809. Loss: 0.002613925840705633\n",
      "Epoch: 810. Loss: 0.0026119754184037447\n",
      "Epoch: 811. Loss: 0.002609889255836606\n",
      "Epoch: 812. Loss: 0.0026078899390995502\n",
      "Epoch: 813. Loss: 0.002605914371088147\n",
      "Epoch: 814. Loss: 0.0026040314696729183\n",
      "Epoch: 815. Loss: 0.002602015156298876\n",
      "Epoch: 816. Loss: 0.0026001909282058477\n",
      "Epoch: 817. Loss: 0.002598135732114315\n",
      "Epoch: 818. Loss: 0.0025961820501834154\n",
      "Epoch: 819. Loss: 0.0025943138170987368\n",
      "Epoch: 820. Loss: 0.002592288190498948\n",
      "Epoch: 821. Loss: 0.0025905421935021877\n",
      "Epoch: 822. Loss: 0.002588464878499508\n",
      "Epoch: 823. Loss: 0.0025866427458822727\n",
      "Epoch: 824. Loss: 0.002584905596449971\n",
      "Epoch: 825. Loss: 0.0025830105878412724\n",
      "Epoch: 826. Loss: 0.0025810380466282368\n",
      "Epoch: 827. Loss: 0.00257932860404253\n",
      "Epoch: 828. Loss: 0.002577277133241296\n",
      "Epoch: 829. Loss: 0.002575451973825693\n",
      "Epoch: 830. Loss: 0.0025737457908689976\n",
      "Epoch: 831. Loss: 0.0025718670804053545\n",
      "Epoch: 832. Loss: 0.0025699473917484283\n",
      "Epoch: 833. Loss: 0.0025681874249130487\n",
      "Epoch: 834. Loss: 0.0025664037093520164\n",
      "Epoch: 835. Loss: 0.002564643044024706\n",
      "Epoch: 836. Loss: 0.0025629254523664713\n",
      "Epoch: 837. Loss: 0.002561147091910243\n",
      "Epoch: 838. Loss: 0.0025593398604542017\n",
      "Epoch: 839. Loss: 0.002557543106377125\n",
      "Epoch: 840. Loss: 0.0025559058412909508\n",
      "Epoch: 841. Loss: 0.002554110949859023\n",
      "Epoch: 842. Loss: 0.0025523726362735033\n",
      "Epoch: 843. Loss: 0.0025505563244223595\n",
      "Epoch: 844. Loss: 0.002548894612118602\n",
      "Epoch: 845. Loss: 0.0025472906418144703\n",
      "Epoch: 846. Loss: 0.0025456282310187817\n",
      "Epoch: 847. Loss: 0.0025439728051424026\n",
      "Epoch: 848. Loss: 0.0025421984028071165\n",
      "Epoch: 849. Loss: 0.0025405571796000004\n",
      "Epoch: 850. Loss: 0.0025388216599822044\n",
      "Epoch: 851. Loss: 0.0025372812524437904\n",
      "Epoch: 852. Loss: 0.002535603241994977\n",
      "Epoch: 853. Loss: 0.002533895894885063\n",
      "Epoch: 854. Loss: 0.0025323019362986088\n",
      "Epoch: 855. Loss: 0.002530645113438368\n",
      "Epoch: 856. Loss: 0.002529062097892165\n",
      "Epoch: 857. Loss: 0.002527303993701935\n",
      "Epoch: 858. Loss: 0.0025258094538003206\n",
      "Epoch: 859. Loss: 0.002524170558899641\n",
      "Epoch: 860. Loss: 0.0025225800927728415\n",
      "Epoch: 861. Loss: 0.002521040616557002\n",
      "Epoch: 862. Loss: 0.0025193416513502598\n",
      "Epoch: 863. Loss: 0.002517877845093608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 864. Loss: 0.0025163362734019756\n",
      "Epoch: 865. Loss: 0.0025147292762994766\n",
      "Epoch: 866. Loss: 0.0025131129659712315\n",
      "Epoch: 867. Loss: 0.002511483384296298\n",
      "Epoch: 868. Loss: 0.002509992802515626\n",
      "Epoch: 869. Loss: 0.0025085255037993193\n",
      "Epoch: 870. Loss: 0.0025068637914955616\n",
      "Epoch: 871. Loss: 0.0025054444558918476\n",
      "Epoch: 872. Loss: 0.0025039527099579573\n",
      "Epoch: 873. Loss: 0.0025025333743542433\n",
      "Epoch: 874. Loss: 0.002500935224816203\n",
      "Epoch: 875. Loss: 0.002499425783753395\n",
      "Epoch: 876. Loss: 0.0024978886358439922\n",
      "Epoch: 877. Loss: 0.002496450673788786\n",
      "Epoch: 878. Loss: 0.002495007123798132\n",
      "Epoch: 879. Loss: 0.0024934690445661545\n",
      "Epoch: 880. Loss: 0.002492049243301153\n",
      "Epoch: 881. Loss: 0.0024906389880925417\n",
      "Epoch: 882. Loss: 0.0024891821667551994\n",
      "Epoch: 883. Loss: 0.0024876180104911327\n",
      "Epoch: 884. Loss: 0.0024862876161932945\n",
      "Epoch: 885. Loss: 0.0024848610628396273\n",
      "Epoch: 886. Loss: 0.00248348293825984\n",
      "Epoch: 887. Loss: 0.00248200842179358\n",
      "Epoch: 888. Loss: 0.002480624243617058\n",
      "Epoch: 889. Loss: 0.0024791962932795286\n",
      "Epoch: 890. Loss: 0.002477763220667839\n",
      "Epoch: 891. Loss: 0.002476336667314172\n",
      "Epoch: 892. Loss: 0.0024750318843871355\n",
      "Epoch: 893. Loss: 0.002473593922331929\n",
      "Epoch: 894. Loss: 0.0024721648078411818\n",
      "Epoch: 895. Loss: 0.002470903331413865\n",
      "Epoch: 896. Loss: 0.0024694676976650953\n",
      "Epoch: 897. Loss: 0.0024680537171661854\n",
      "Epoch: 898. Loss: 0.0024666355457156897\n",
      "Epoch: 899. Loss: 0.0024653251748532057\n",
      "Epoch: 900. Loss: 0.002464097924530506\n",
      "Epoch: 901. Loss: 0.002462740521878004\n",
      "Epoch: 902. Loss: 0.0024614285212010145\n",
      "Epoch: 903. Loss: 0.002460006857290864\n",
      "Epoch: 904. Loss: 0.0024587453808635473\n",
      "Epoch: 905. Loss: 0.0024573272094130516\n",
      "Epoch: 906. Loss: 0.0024560505989938974\n",
      "Epoch: 907. Loss: 0.0024548012297600508\n",
      "Epoch: 908. Loss: 0.0024535167030990124\n",
      "Epoch: 909. Loss: 0.0024521537125110626\n",
      "Epoch: 910. Loss: 0.002450775820761919\n",
      "Epoch: 911. Loss: 0.0024495290126651525\n",
      "Epoch: 912. Loss: 0.002448249841108918\n",
      "Epoch: 913. Loss: 0.002447070088237524\n",
      "Epoch: 914. Loss: 0.0024458018597215414\n",
      "Epoch: 915. Loss: 0.0024445594754070044\n",
      "Epoch: 916. Loss: 0.0024432321079075336\n",
      "Epoch: 917. Loss: 0.00244203582406044\n",
      "Epoch: 918. Loss: 0.0024407159071415663\n",
      "Epoch: 919. Loss: 0.0024395370855927467\n",
      "Epoch: 920. Loss: 0.002438307972624898\n",
      "Epoch: 921. Loss: 0.00243701902218163\n",
      "Epoch: 922. Loss: 0.0024357729125767946\n",
      "Epoch: 923. Loss: 0.002434564521536231\n",
      "Epoch: 924. Loss: 0.0024333782494068146\n",
      "Epoch: 925. Loss: 0.0024321649689227343\n",
      "Epoch: 926. Loss: 0.0024309263098984957\n",
      "Epoch: 927. Loss: 0.002429672982543707\n",
      "Epoch: 928. Loss: 0.0024285679683089256\n",
      "Epoch: 929. Loss: 0.0024274124298244715\n",
      "Epoch: 930. Loss: 0.0024261760991066694\n",
      "Epoch: 931. Loss: 0.0024249046109616756\n",
      "Epoch: 932. Loss: 0.0024238149635493755\n",
      "Epoch: 933. Loss: 0.0024225707165896893\n",
      "Epoch: 934. Loss: 0.002421364886686206\n",
      "Epoch: 935. Loss: 0.0024203453212976456\n",
      "Epoch: 936. Loss: 0.002419121563434601\n",
      "Epoch: 937. Loss: 0.0024179588072001934\n",
      "Epoch: 938. Loss: 0.0024167271330952644\n",
      "Epoch: 939. Loss: 0.002415579976513982\n",
      "Epoch: 940. Loss: 0.0024144165217876434\n",
      "Epoch: 941. Loss: 0.0024133664555847645\n",
      "Epoch: 942. Loss: 0.002412268426269293\n",
      "Epoch: 943. Loss: 0.0024110504891723394\n",
      "Epoch: 944. Loss: 0.0024099654983729124\n",
      "Epoch: 945. Loss: 0.0024088197387754917\n",
      "Epoch: 946. Loss: 0.0024077161215245724\n",
      "Epoch: 947. Loss: 0.00240653776563704\n",
      "Epoch: 948. Loss: 0.0024054767563939095\n",
      "Epoch: 949. Loss: 0.002404388738796115\n",
      "Epoch: 950. Loss: 0.00240326882340014\n",
      "Epoch: 951. Loss: 0.002402142621576786\n",
      "Epoch: 952. Loss: 0.002401087200269103\n",
      "Epoch: 953. Loss: 0.0023999917320907116\n",
      "Epoch: 954. Loss: 0.0023988564498722553\n",
      "Epoch: 955. Loss: 0.002397854346781969\n",
      "Epoch: 956. Loss: 0.002396743046119809\n",
      "Epoch: 957. Loss: 0.002395746298134327\n",
      "Epoch: 958. Loss: 0.0023946044966578484\n",
      "Epoch: 959. Loss: 0.002393547911196947\n",
      "Epoch: 960. Loss: 0.0023924887645989656\n",
      "Epoch: 961. Loss: 0.002391532529145479\n",
      "Epoch: 962. Loss: 0.002390397246927023\n",
      "Epoch: 963. Loss: 0.002389355329796672\n",
      "Epoch: 964. Loss: 0.0023882784880697727\n",
      "Epoch: 965. Loss: 0.0023872260935604572\n",
      "Epoch: 966. Loss: 0.0023861867375671864\n",
      "Epoch: 967. Loss: 0.0023851757869124413\n",
      "Epoch: 968. Loss: 0.0023841571528464556\n",
      "Epoch: 969. Loss: 0.0023831359576433897\n",
      "Epoch: 970. Loss: 0.002382106613367796\n",
      "Epoch: 971. Loss: 0.002381025580689311\n",
      "Epoch: 972. Loss: 0.0023800719063729048\n",
      "Epoch: 973. Loss: 0.0023790262639522552\n",
      "Epoch: 974. Loss: 0.0023780185729265213\n",
      "Epoch: 975. Loss: 0.002377084456384182\n",
      "Epoch: 976. Loss: 0.0023760716430842876\n",
      "Epoch: 977. Loss: 0.002375041600316763\n",
      "Epoch: 978. Loss: 0.0023740017786622047\n",
      "Epoch: 979. Loss: 0.0023730627726763487\n",
      "Epoch: 980. Loss: 0.0023721065372228622\n",
      "Epoch: 981. Loss: 0.0023710785899311304\n",
      "Epoch: 982. Loss: 0.0023701589088886976\n",
      "Epoch: 983. Loss: 0.002369226422160864\n",
      "Epoch: 984. Loss: 0.002368256449699402\n",
      "Epoch: 985. Loss: 0.0023671670351177454\n",
      "Epoch: 986. Loss: 0.0023662911262363195\n",
      "Epoch: 987. Loss: 0.00236528180539608\n",
      "Epoch: 988. Loss: 0.0023643202148377895\n",
      "Epoch: 989. Loss: 0.002363325795158744\n",
      "Epoch: 990. Loss: 0.002362472005188465\n",
      "Epoch: 991. Loss: 0.002361514139920473\n",
      "Epoch: 992. Loss: 0.002360549056902528\n",
      "Epoch: 993. Loss: 0.00235962076112628\n",
      "Epoch: 994. Loss: 0.0023586542811244726\n",
      "Epoch: 995. Loss: 0.002357790945097804\n",
      "Epoch: 996. Loss: 0.0023568945471197367\n",
      "Epoch: 997. Loss: 0.0023559900000691414\n",
      "Epoch: 998. Loss: 0.0023550402838736773\n",
      "Epoch: 999. Loss: 0.002354109426960349\n"
     ]
    }
   ],
   "source": [
    "loss_cnn_train = []\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 120\n",
    "cnn_logits_lst = []\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    #for i in tqdm(range(0, len(X_train_CNN), BATCH_SIZE)):\n",
    "    batch_X = X_train_CNN.view(-1, 1, 33,33)\n",
    "    batch_y = Y_train_CNN\n",
    "\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net(batch_X.float())[0]\n",
    "    cnn_logits = net(batch_X.float())[1]\n",
    "    #cnn_logits_lst.extend(cnn_logits)\n",
    "    #print(outputs)\n",
    "    loss = loss_function(outputs,  batch_y.float().reshape((-1,1)))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_cnn_train.append(loss)\n",
    "    if EPOCHS % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "id": "ae3f478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAe1klEQVR4nO3de3xcdZ3/8ddnJteGpuklLU3a0gJFKJRSCBUQQRG0CNvqKlp0XVS06tqF9ee6P9TfD7Xu/n6ou6irdQXRVVQsiK4bsFi5KXdsyrVJaZuW0nubpqUpbdPcPvvHnKSTdNJM20nOnJn38/GYR875nm9mPqcH3nPynTPna+6OiIhEXyzsAkREJDMU6CIiOUKBLiKSIxToIiI5QoEuIpIjCsJ64TFjxvjkyZPDenkRkUhavnz5TnevTLUttECfPHkydXV1Yb28iEgkmdlr/W3TkIuISI5QoIuI5AgFuohIjlCgi4jkCAW6iEiOSCvQzWy2ma0ys0YzuynF9m+b2QvBY7WZvZ7xSkVE5IgGvGzRzOLAIuAKYBOwzMxq3b2hu4+7fy6p/98DMwehVhEROYJ0ztBnAY3uvs7d24DFwNwj9L8W+FUmiktl2fpd/NsfV9He2TVYLyEiEknpBHo1sDFpfVPQdhgzOwmYAjxy/KWl9vyG3XzvkUbaOhToIiLJMv2h6DzgXnfvTLXRzOabWZ2Z1TU1NR3TC8RjiZI7OjUxh4hIsnQCfTMwMWl9QtCWyjyOMNzi7re7e42711RWprwVwYAKYgZAR5fO0EVEkqUT6MuAqWY2xcyKSIR2bd9OZnY6MBJ4OrMl9lYQTwR6Z5fO0EVEkg0Y6O7eASwAlgIrgXvcvd7MFprZnKSu84DFPsiTlHafobcr0EVEeknrbovuvgRY0qft5j7rX81cWf0rCMbQOzWGLiLSS+S+Kdo95NKuMXQRkV4iF+jxmMbQRURSiVygF+iyRRGRlCIY6LpsUUQklegFerw70HWGLiKSLHqBriEXEZGUIhfocQ25iIikFLlAL9Q3RUVEUopcoPecoWvIRUSkl8gFemE8GEPXGbqISC+RC/RDZ+gaQxcRSRa5QNfNuUREUotcoBcVdF+2qDN0EZFkkQ30g5qCTkSkl8gFenFBHICD7SlnuRMRyVuRC/TuM/Q2DbmIiPQSuUAv7h5yaVegi4gki1ygd1/lsq9NQy4iIskiF+hmiUD/4Z/XhlyJiEh2iVygJ2vVB6MiIj3SCnQzm21mq8ys0cxu6qfPB8yswczqzeyuzJaZ2q59bUPxMiIikVAwUAcziwOLgCuATcAyM6t194akPlOBLwJvcffdZjZ2sApO1vxGG1UVpUPxUiIiWS+dM/RZQKO7r3P3NmAxMLdPn08Ci9x9N4C778hsmb1VDCsEYNd+naGLiHRLJ9CrgY1J65uCtmSnAaeZ2ZNm9oyZzU71RGY238zqzKyuqanp2CoG7p5/IQCrtrUc83OIiOSaTH0oWgBMBd4GXAv8yMwq+nZy99vdvcbdayorK4/5xYaXJEaK/t+SV+jSTbpERID0An0zMDFpfULQlmwTUOvu7e7+KrCaRMAPiu5AB1i3c99gvYyISKSkE+jLgKlmNsXMioB5QG2fPr8jcXaOmY0hMQSzLnNl9lZWdCjQf/rUq4P1MiIikTJgoLt7B7AAWAqsBO5x93ozW2hmc4JuS4FmM2sAHgW+4O7Ng1Z08G1RgF88s4HXmnWWLiJi7uGMQdfU1HhdXd0x//7q7Xt557cfA+CS0yr5/odmUl5SmKnyRESykpktd/eaVNsi+03RyaPLepYfW93E2V/9I0817gyxIhGRcEU20IsKYjzy+Ut7tX3ojmeZu+hJnt+wWzMaiUjeieyQS7cfP/Eqv1m+iYatva9JH15SwNVnVzH3nCpmTR7Va9xdRCSqjjTkEvlA73agrZMNu/bzm+c20bClhVd37mPz6weAxD3UL582jnedeSKXnT6WE4oHvOOBiEhWyotAT2VD837+vHoHT61t5onGnext7aAoHuPSN1Vy1fTxzD7rREoK44Nag4hIJuVtoCfr6OziL+t38WDDdh54eRvbWloZVVbEvPMn8qlLTmHEMF0hIyLZT4Heh7vz1Npm7nx6PX9s2E55SSEL3n4qH7nwJEoK4zTtPUhBzBhZVhRKfSIi/VGgH0HDlhZu+cMrPLa6ieqKUuZfcjJfqa3ngpNHsTi4CZiISLbIyevQM2VaVTl3fnwWv7j+zVQMK+QrtfUAPLNuF7dpmjsRiZC8D/RuF08dw30LLuaHf3Mub39T4k6Q//+BV3iwYXvIlYmIpEeBniQWM2afNZ4ffuQ8ZkysAOCTd9bx3Ibd4RYmIpIGBXoKxQVxfvd3F/G3F54EwF//4ClWbN4TclUiIkemQO+HmbFw7ll879qZAFz7o2d0V0cRyWoK9AH81YwqvvPBc9jb2sHn7n6BTs2QJCJZSoGehvfMrOYb75vOcxte5/Jb/0z9lj10dHZx9fce5+v3N4RdnogIoEBP2wdqJvKDD5/L3tYOvvjbl9m+9yArNrfw4yc0Y5KIZAfdpSpNZsa7p4/nYEcnn7v7RX7waGPYJYmI9KIz9KM0d0Y1MydV8MtnN/S0vbDx9fAKEhEJKNCPUixm/PN7zurV9p5FT9KuCTVEJGRpBbqZzTazVWbWaGY3pdj+UTNrMrMXgscnMl9q9jizagSPfeHt3Hz1tJ62638W/n1pRCS/DTiGbmZxYBFwBbAJWGZmte7e9/KOu919wSDUmJUmjR7GGePLe9YfW90UYjUiIumdoc8CGt19nbu3AYuBuYNbVjSMH1ESdgkiIj3SCfRqYGPS+qagra/3mdlLZnavmU1M9URmNt/M6sysrqkp+me0k8eU8c33nd2z/pBu5CUiIcrUh6L3AZPd/WzgQeBnqTq5++3uXuPuNZWVlRl66XB94PyJfPndZwDwiTvr2HewI+SKRCRfpRPom4HkM+4JQVsPd29294PB6h3AeZkpLxo+ecnJPctPr20OsRIRyWfpBPoyYKqZTTGzImAeUJvcwczGJ63OAVZmrsRouP7iKUDiLF1EJAwDBrq7dwALgKUkgvoed683s4VmNifodoOZ1ZvZi8ANwEcHq+Bs9YV3valnOdUVL0827uSptTuHsiQRyTN5P6doJj2+pomP/PgvAKy/5ape2ybf9PuU7SIiR0Nzig6Ri04Z07Pc2t4ZYiUiko8U6BkUjxn/es0MAG59cHXI1YhIvlGgZ9j7z5tA1YgSXt6kKetEZGgp0AfBFdPG8fS6Zp5s1IegIjJ0FOiD4BNvTVyXftdfNgzQU0QkcxTog2DiqGG8/7wJPLFmJ51drlvrisiQUKAPkktOq2TPgXbuXraRA7riRUSGgAJ9kFw6NXGvmh/+eS179rf3tP9hxdawShKRHKdAHyQjhhXy3XnnsGHXfpbWb+tp//QvnguxKhHJZQr0QfSOM8YRM3hgxbaBO4uIHCcF+iA6obiAM6tGsPy13WGXIiJ5QIE+yM6fPCrsEkQkTyjQB9msKYcHeu2LW0KoRERynQJ9kF162uEzMz25Rt8gFZHMU6APstKiOM/93yu47PSxPW2797eFWJGI5CoF+hAYVVbECcUFPet/bNhOV1c496EXkdylQB8iW/cc6LX+4MrtIVUiIrlKgT5EFs49q9e6JsAQkUxToA+RM8aX91qPxyykSkQkV6UV6GY228xWmVmjmd10hH7vMzM3s5Tz3ckhMVOgi0hmDRjoZhYHFgFXAtOAa81sWop+w4EbgWczXWQu0i11RSTT0jlDnwU0uvs6d28DFgNzU/T7OvANoDWD9eWUceXFPcs3/3d9iJWISC5KJ9CrgY1J65uCth5mdi4w0d1/f6QnMrP5ZlZnZnVNTU1HXWzU1S64uGd5z4H2I/QUETl6x/2hqJnFgFuBzw/U191vd/cad6+prDz8G5S5blx5CQ0L3xV2GSKSo9IJ9M3AxKT1CUFbt+HAWcCfzGw9cAFQqw9GUystjPcs9702XUTkeKQT6MuAqWY2xcyKgHlAbfdGd9/j7mPcfbK7TwaeAea4e92gVBxxZsaX3n06ABfd8kjI1YhILhkw0N29A1gALAVWAve4e72ZLTSzOYNdYC5685TRALi+/S8iGVQwcBdw9yXAkj5tN/fT923HX1ZumzGxgivPOpFV2/aGXYqI5BB9UzQkI8uKWLdzn27SJSIZo0APybY9icv173tJk12ISGYo0EPy95edCkDdes03KiKZoUAPycxJI5lePYL1zfvCLkVEcoQCPUSt7Z08vmYnG5r3h12KiOQABXqIuqele3nznpArEZFcoEAP0WeDcfTNr+sMXUSOnwI9ROUlhYwuK2LN9jfCLkVEcoACPWTTJ4zg18s3sXtfW9iliEjEKdBD9t6ZiTsRr9zWEnIlIhJ1CvSQnT95FACv7tTliyJyfBToITuxvISSwhivNinQReT4KNBDFosZk0eX8eTaZly3XxSR46BAzwLvnVnNyq0tvLhJ16OLyLFToGeBueckPhjVF4xE5Hgo0LNA5fBiCmLG1tc1JZ2IHDsFehaIx4xx5SVsUaCLyHFQoGeJkyvLWK1vjIrIcVCgZ4kZEypo2NrCwyu3h12KiESUAj1LfOZtp3D6icP5+v0NYZciIhGVVqCb2WwzW2VmjWZ2U4rtnzazl83sBTN7wsymZb7U3FZWXMB7Z1azvnm/7usiIsdkwEA3sziwCLgSmAZcmyKw73L36e5+DvBN4NZMF5oPzqwaAUD9Ft3XRUSOXjpn6LOARndf5+5twGJgbnIHd09OoDJAX3k8BmdWlQNQv0XXo4vI0Usn0KuBjUnrm4K2Xszss2a2lsQZ+g2pnsjM5ptZnZnVNTU1HUu9OW1kWRHVFaXUb2nhqcadfOeh1WGXJCIRkrEPRd19kbufAvxv4P/00+d2d69x95rKyspMvXROObOqnBWb9/ChO57lOw+toa2jK+ySRCQi0gn0zcDEpPUJQVt/FgPvOY6a8tqMiRWsS7qV7oZdugujiKQnnUBfBkw1sylmVgTMA2qTO5jZ1KTVq4A1mSsxv5wzsaLX+vaWg+EUIiKRUzBQB3fvMLMFwFIgDvzE3evNbCFQ5+61wAIzuxxoB3YD1w1m0bls+oQRvdb3traHVImIRM2AgQ7g7kuAJX3abk5avjHDdeWt8pJCSgpjtLYnxs5bDnSEXJGIRIW+KZqFPlhz6COLFp2hi0iaFOhZ6MtXTePxf3o7ZtDSqjN0EUmPAj0LFRXEmDhqGMMK47yhQBeRNCnQs1hpUZzWjs6wyxCRiFCgZ7HSojgH2hToIpIeBXoWK4rH+K/nN/P02uawSxGRCFCgZ7F9BxNn5x+645mQKxGRKFCgZ7GDwfi5696VIpIGBXoWO6gbc4nIUVCgZ7FhRWl9kVdEBFCgZ7Wffux8AEoL4yFXIiJRoEDPYmdVj+CGy06ltaOTri4NpIvIkSnQs1x5aSHucPKXlgzcWUTymgI9y40+oahned9B3QZARPqnQM9ylSeU9Cw3v9EWYiUiku0U6FlueMmhK12a3tDsRSLSPwV6lptePYJ3ThsHQLMCXUSOQIGe5WIx42tzzwSgeZ+GXESkfwr0CBhdVgzA5t0HcN0HQET6kVagm9lsM1tlZo1mdlOK7f/LzBrM7CUze9jMTsp8qfmrqCBGPGZ8/9FGFj3aGHY5IpKlBgx0M4sDi4ArgWnAtWY2rU+354Eadz8buBf4ZqYLzXedwReL7np2Q8iViEi2SucMfRbQ6O7r3L0NWAzMTe7g7o+6+/5g9RlgQmbLlK/NSYyjlxXr/i4iklo6gV4NbExa3xS09ed64IFUG8xsvpnVmVldU1NT+lUK1100mY9eNJktr2scXURSy+iHomb2N0AN8K1U2939dnevcfeaysrKTL50XqiuKGVfWyf1W1q49FuPUvvilrBLEpEskk6gbwYmJq1PCNp6MbPLgS8Dc9xdF0wPgqqKUgDue2kLrzXv5wf6gFREkqQT6MuAqWY2xcyKgHlAbXIHM5sJ3EYizHdkvkwBqKpI3AbgL6/uAmBfm+7tIiKHDBjo7t4BLACWAiuBe9y93swWmtmcoNu3gBOAX5vZC2ZW28/TyXGYMHIYAM9veB2ANs1oJCJJ0rpkwt2XAEv6tN2ctHx5huuSFCqHFzNhZCmbdh8AYH8wibSICOibopHzhXe9iTPGl3P12ePZ19ahK15EpIcCPWLmnlPNAze+lWlV5XS5JpIWkUMU6BE1LJhndH+bhl1EJEGBHlHDgm+MahYjEemmQI+o8mDiiz0H2kOuRESyhQI9okYOS8w1evX3nuDxNf3fRmHnGwfZr+vVRfKCAj2iRpUdmjz6heC69FRq/vkhPnDb00NQkYiETYEeUcmBPtCwy4rNLYNdjohkAQV6RI0qK+LDb54EwI69qW+d09quK2BE8olurh1RZsa/vHc6q7btZcfe1pR99rZq7Fwkn+gMPeLGlhf3e4be0qorYETyiQI94sYOL2Fd0z6+8OsXD9umM3SR/KJAj7hJoxJ3YPz18k2HjZm3BB+Wmg15WSISAgV6xF05/cSe5dXb9/ba1j3kUlIQH9KaRCQcCvSIGz+ilD/949sAWLm19+WJLQcSQy4lhTrMIvlA/6fngEmjhlFcEGPN9jd6tfecoRfqDF0kHyjQc0AsZpw0ehiv7drfq31vEOjxmAbRRfKBAj1HnDr2BB5s2M6PHlvXM+lF95CLpqoTyQ8K9Bzx6UtP4aTRw/iXJStZFXw42j3k0t6pQBfJB2kFupnNNrNVZtZoZjel2H6JmT1nZh1m9v7MlykDOXtCBb/9zEXEY8Z9L24BDl22qDN0kfwwYKCbWRxYBFwJTAOuNbNpfbptAD4K3JXpAiV9o08oZubECp5e2wxAS/DFooMdXXR1ae5RkVyXzhn6LKDR3de5exuwGJib3MHd17v7S4BOBUM2c1IFK7a00NbRxe79bQB0dDkf/9mykCsTkcGWTqBXAxuT1jcFbUfNzOabWZ2Z1TU19T8pgxy7mZNG0tbRxYote9i+59BNu/60Sv/eIrluSD8Udffb3b3G3WsqKyuH8qXzxgUnjyZmcP+LW9nX1smwIl2DLpIv0gn0zcDEpPUJQZtkoVFlRZx30kh+8uSrAEwYWRpyRSIyVNIJ9GXAVDObYmZFwDygdnDLkuPx6UtPARI35Tp30sie9u7r00UkNw0Y6O7eASwAlgIrgXvcvd7MFprZHAAzO9/MNgHXALeZWf1gFi1H9o4zxvGfHzuf2s9eTHlpYU97/RZNRSeSyyyss7aamhqvq6sL5bXzycL7GnqGXwDW33JViNWIyPEys+XuXpNqm74pmuM6u3QlqUi+UKDnuGtqJvZa79QXjERylgI9x51VPYL1t1zFv10zA4DvPrwm5IpEZLAo0PPEBaeMBuDfH16j2wCI5CgFep6orijllr+eDsDiZRsH6C0iUaRAzyPvOGMcAF/6r5d542BHyNWISKYp0PNI5fDinrH0f7znRX3RSCTHKNDzzNxzqpgxsYI/1G9jaf32sMsRkQxSoOeZgniMuz7xZsYOL+bvfrmcFZv3hF2SiGSIAj0PlRUXcPenLiQeM67+3hOsCaasE5FoU6DnqSljyvjJR8+nKB7jmtueZrVCXSTyFOh57K1TK7n3MxcC8L4fPMVDDRpTF4kyBXqeO3tCBUtueCvVI0v5xJ11fP3+BlrbO8MuS0SOgQJdqKoo5e5PXcgHaibw4yde5Z3ffozfPb9Z930RiRgFugAworSQb75/Bj+/fhZlxQX8w90vMPs7j3Hfi1vo6NQdG0WiQPdDl8N0dTkPrNjGrQ+uYm3TPk4sL2HuOVXMOaeKaePLMbOwSxTJW0e6H7oCXfrV2eU88soO7nr2NR5fs5OOLueUyjIuO30sF50yhvOnjOKE4oKwyxTJKwp0OW679rXxwIqt/P6lrdSt301bZxfxmDG9egRnVZdzZtUIzqwq57RxwykpjIddrkjOUqBLRrW2d7L8td08tXYny9bvZuWWFvYGN/syg6oRpUwaNYzJY4YxaVQZVRUlVA4vZuzwEsaWFzO8uEDDNiLH6EiBntbfy2Y2G/guEAfucPdb+mwvBu4EzgOagQ+6+/rjKVqyV0lhnLecOoa3nDoGSIy5b9y9n4YtLbyybS8bdu3nteZ9/LF+O8372lL8foyxw0sYVVZEeWkhI0oLGVFaQHlJ93Ih5aWFDCuKU1oYZ1hRAaVFMUqLCoL1OMUFMb0piPQxYKCbWRxYBFwBbAKWmVmtuzckdbse2O3up5rZPOAbwAcHo2DJPrGYcdLoMk4aXcaV08f32ra3tZ3tLa3saDnIjr0H2bH30PLu/W3s2d/GhuZ9tLR2sOdAe9qXSppBaWEi8EsK4xQVxCiMW/Az8SiKx3rau9cLe9piFBYYcTMKYkYsduhn3Ix4rM+jn7bk3ysI2syMmNHrpxnEzDCCn8ahtqRtyX1jBkY/fYNtvV4Hw2KpXyfRklgGel6re7nXNr1RRlY6Z+izgEZ3XwdgZouBuUByoM8Fvhos3wt838zMdX/WvDe8pJDhJYWcOnb4gH3dnf1tnew50E5Lazv72zo5EDz2t3dyoK2jZ7m1rTOxvT3xaO902ju6aO/soq2zi7aOLva3dbDngPdqa+/s6ul7sLOLri6nQ9fb9yv5DSCxboe/AXCoU3/bzHo/R89zpnj+I/bvee7D36AO/V5/2w69UR168+qzD0ltPev0WU9+Hvpu7H81+fdufMdU/mpGVd/fPm7pBHo1kDzFzSbgzf31cfcOM9sDjAZ2ZqJIyQ9mRllxAWXFBVRROqSv3dXldLrT2RU83Ons7NOWtK37jSBVW5c7OHQ5OE6Xk9TmePdPEm9iXc5hbd3riW2J9e7n6ukb9O/qCn56799NtCXWCZ438RO638J6tnH4tu6NyX2T+/W3jcO2+RH79z3t696vgeo79HtJ21L07+7Rd1vfOrtfu1ctvUvrVevh247wu306jygtZDAM6TVnZjYfmA8wadKkoXxpkSOKxYwYhi7QkShL55uim4GJSesTgraUfcysABhB4sPRXtz9dnevcfeaysrKY6tYRERSSifQlwFTzWyKmRUB84DaPn1qgeuC5fcDj2j8XERkaA045BKMiS8AlpK4bPEn7l5vZguBOnevBX4M/NzMGoFdJEJfRESGUFpj6O6+BFjSp+3mpOVW4JrMliYiIkdDd1sUEckRCnQRkRyhQBcRyREKdBGRHBHa3RbNrAl47Rh/fQz59y1U7XN+0D7nh+PZ55PcPeUXeUIL9ONhZnX93T4yV2mf84P2OT8M1j5ryEVEJEco0EVEckRUA/32sAsIgfY5P2if88Og7HMkx9BFRORwUT1DFxGRPhToIiI5InKBbmazzWyVmTWa2U1h15MpZjbRzB41swYzqzezG4P2UWb2oJmtCX6ODNrNzP49+Hd4yczODXcPjo2Zxc3seTO7P1ifYmbPBvt1d3DLZsysOFhvDLZPDrXwY2RmFWZ2r5m9YmYrzezCPDjGnwv+m15hZr8ys5JcPM5m9hMz22FmK5LajvrYmtl1Qf81ZnZdqtfqT6QCPWnC6iuBacC1ZjYt3KoypgP4vLtPAy4APhvs203Aw+4+FXg4WIfEv8HU4DEf+I+hLzkjbgRWJq1/A/i2u58K7CYxATkkTUQOfDvoF0XfBf7g7qcDM0jse84eYzOrBm4Aatz9LBK34O6eSD7XjvNPgdl92o7q2JrZKOArJKb5nAV8pftNIC3ePTdhBB7AhcDSpPUvAl8Mu65B2tf/Bq4AVgHjg7bxwKpg+Tbg2qT+Pf2i8iAx+9XDwGXA/STm1N0JFPQ93iTux39hsFwQ9LOw9+Eo93cE8GrfunP8GHfPNzwqOG73A+/K1eMMTAZWHOuxBa4Fbktq79VvoEekztBJPWF1dUi1DJrgz8yZwLPAOHffGmzaBowLlnPh3+I7wD8BXcH6aOB1d+8I1pP3qddE5ED3RORRMgVoAv4zGGa6w8zKyOFj7O6bgX8FNgBbSRy35eT2cU52tMf2uI551AI955nZCcBvgH9w95bkbZ54y86J60zN7Gpgh7svD7uWIVQAnAv8h7vPBPZx6E9wILeOMUAwXDCXxJtZFVDG4cMSeWEojm3UAj2dCasjy8wKSYT5L939t0HzdjMbH2wfD+wI2qP+b/EWYI6ZrQcWkxh2+S5QEUw0Dr33Ka2JyLPcJmCTuz8brN9LIuBz9RgDXA686u5N7t4O/JbEsc/l45zsaI/tcR3zqAV6OhNWR5KZGYm5WVe6+61Jm5In4L6OxNh6d/vfBp+WXwDsSfrTLuu5+xfdfYK7TyZxHB9x9w8Dj5KYaBwO399IT0Tu7tuAjWb2pqDpHUADOXqMAxuAC8xsWPDfePc+5+xx7uNoj+1S4J1mNjL46+adQVt6wv4Q4Rg+dHg3sBpYC3w57HoyuF8Xk/hz7CXgheDxbhLjhw8Da4CHgFFBfyNxxc9a4GUSVxGEvh/HuO9vA+4Plk8G/gI0Ar8GioP2kmC9Mdh+cth1H+O+ngPUBcf5d8DIXD/GwNeAV4AVwM+B4lw8zsCvSHxO0E7ir7Hrj+XYAh8P9r8R+NjR1KCv/ouI5IioDbmIiEg/FOgiIjlCgS4ikiMU6CIiOUKBLiKSIxToIiI5QoEuIpIj/geA0bat2N6hugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_cnn_train)\n",
    "len(cnn_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "id": "22632efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CNN_logits'] =cnn_logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "id": "4dd57973",
   "metadata": {},
   "outputs": [],
   "source": [
    " #train = train.loc[train[\"original_word\"]!='Refulgent'].shape  #very dangerous line here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "id": "41acbc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136</td>\n",
       "      <td>ओ</td>\n",
       "      <td>بیتاب</td>\n",
       "      <td>o</td>\n",
       "      <td>bjtɒb</td>\n",
       "      <td>O</td>\n",
       "      <td>Impatient</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420495</td>\n",
       "      <td>0.697084</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-16.218758</td>\n",
       "      <td>-6.709172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>दुख़्तर</td>\n",
       "      <td>دختر</td>\n",
       "      <td>duxtər</td>\n",
       "      <td>dxtr</td>\n",
       "      <td>pain</td>\n",
       "      <td>Girl</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452070</td>\n",
       "      <td>0.516028</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.018238</td>\n",
       "      <td>7.578733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1079</td>\n",
       "      <td>रूह</td>\n",
       "      <td>روح</td>\n",
       "      <td>ruːɦ</td>\n",
       "      <td>rvh</td>\n",
       "      <td>spirit</td>\n",
       "      <td>Soul</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506079</td>\n",
       "      <td>0.719540</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>8.149307</td>\n",
       "      <td>13.127642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>976</td>\n",
       "      <td>मुअल्लिमा</td>\n",
       "      <td>معلمه</td>\n",
       "      <td>muallimaː</td>\n",
       "      <td>mʔlmh</td>\n",
       "      <td>Muallima</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>3.527778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436408</td>\n",
       "      <td>0.725925</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>7.360963</td>\n",
       "      <td>11.419708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4646</td>\n",
       "      <td>मलिका</td>\n",
       "      <td>ماجراجو</td>\n",
       "      <td>məlikaː</td>\n",
       "      <td>mɒd͡ʒrɒd͡ʒv</td>\n",
       "      <td>malika</td>\n",
       "      <td>Adventurer</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>1.864583</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697087</td>\n",
       "      <td>0.756841</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-16.619883</td>\n",
       "      <td>-9.829449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>260</td>\n",
       "      <td>ख़ालिक़</td>\n",
       "      <td>جاری</td>\n",
       "      <td>xaːliq</td>\n",
       "      <td>d͡ʒɒrj</td>\n",
       "      <td>pure</td>\n",
       "      <td>Current</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333789</td>\n",
       "      <td>0.746394</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>-7.594423</td>\n",
       "      <td>-20.254333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>2208</td>\n",
       "      <td>पकड़ना</td>\n",
       "      <td>فهم</td>\n",
       "      <td>pəkɽənaː</td>\n",
       "      <td>fhm</td>\n",
       "      <td>Catch</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>4.140625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493462</td>\n",
       "      <td>0.387989</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...</td>\n",
       "      <td>-7.256489</td>\n",
       "      <td>-12.875332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>4169</td>\n",
       "      <td>प्रशंसा</td>\n",
       "      <td>ستایش</td>\n",
       "      <td>prəʃənsaː</td>\n",
       "      <td>stɒjʃ</td>\n",
       "      <td>Praise</td>\n",
       "      <td>Praise</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421296</td>\n",
       "      <td>3.638889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537812</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>-20.811543</td>\n",
       "      <td>-18.016346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>1263</td>\n",
       "      <td>सिलसिला</td>\n",
       "      <td>سلسله</td>\n",
       "      <td>silsilaː</td>\n",
       "      <td>slslh</td>\n",
       "      <td>continuation</td>\n",
       "      <td>Series</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286458</td>\n",
       "      <td>2.515625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.469336</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>9.651114</td>\n",
       "      <td>10.320404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>323</td>\n",
       "      <td>गिरह</td>\n",
       "      <td>نیست و نابود</td>\n",
       "      <td>ɡirəɦ</td>\n",
       "      <td>njst v nɒbvd</td>\n",
       "      <td>gang</td>\n",
       "      <td>Not and destroyed</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496528</td>\n",
       "      <td>4.177083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329257</td>\n",
       "      <td>0.111304</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>-28.924120</td>\n",
       "      <td>-9.297581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4194 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  loan_word original_word loan_word_epitran  \\\n",
       "0            136          ओ         بیتاب                 o   \n",
       "1            624    दुख़्तर          دختر            duxtər   \n",
       "2           1079        रूह           روح              ruːɦ   \n",
       "3            976  मुअल्लिमा         معلمه         muallimaː   \n",
       "4           4646      मलिका      ماجراجو            məlikaː   \n",
       "...          ...        ...           ...               ...   \n",
       "4191         260    ख़ालिक़          جاری            xaːliq   \n",
       "4192        2208     पकड़ना           فهم          pəkɽənaː   \n",
       "4193        4169    प्रशंसा         ستایش         prəʃənsaː   \n",
       "4194        1263    सिलसिला         سلسله          silsilaː   \n",
       "4195         323       गिरह  نیست و نابود             ɡirəɦ   \n",
       "\n",
       "     original_word_epitran  loan_english   original_english          label  \\\n",
       "0                    bjtɒb             O          Impatient         random   \n",
       "1                     dxtr          pain               Girl           loan   \n",
       "2                      rvh        spirit               Soul           loan   \n",
       "3                    mʔlmh      Muallima            Teacher           loan   \n",
       "4             mɒd͡ʒrɒd͡ʒv         malika         Adventurer  hard_negative   \n",
       "...                    ...           ...                ...            ...   \n",
       "4191                d͡ʒɒrj          pure            Current         random   \n",
       "4192                   fhm         Catch      Understanding        synonym   \n",
       "4193                 stɒjʃ        Praise             Praise        synonym   \n",
       "4194                 slslh  continuation             Series           loan   \n",
       "4195          njst v nɒbvd          gang  Not and destroyed         random   \n",
       "\n",
       "      Fast Levenshtein  Dolgo Prime Distance  ...  Hamming Feature Distance  \\\n",
       "0             1.000000              0.800000  ...                  0.808333   \n",
       "1             0.333333              0.333333  ...                  0.333333   \n",
       "2             0.750000              0.250000  ...                  0.114583   \n",
       "3             0.666667              0.444444  ...                  0.398148   \n",
       "4             0.916667              0.166667  ...                  0.177083   \n",
       "...                ...                   ...  ...                       ...   \n",
       "4191          1.000000              0.333333  ...                  0.291667   \n",
       "4192          1.000000              0.625000  ...                  0.546875   \n",
       "4193          1.000000              0.555556  ...                  0.421296   \n",
       "4194          0.500000              0.375000  ...                  0.286458   \n",
       "4195          1.000000              0.750000  ...                  0.496528   \n",
       "\n",
       "      Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  \\\n",
       "0                      5.900000                              1.000000   \n",
       "1                      2.416667                              0.333333   \n",
       "2                      2.187500                              0.750000   \n",
       "3                      3.527778                              0.666667   \n",
       "4                      1.864583                              0.916667   \n",
       "...                         ...                                   ...   \n",
       "4191                   2.666667                              1.000000   \n",
       "4192                   4.140625                              1.000000   \n",
       "4193                   3.638889                              1.000000   \n",
       "4194                   2.515625                              0.500000   \n",
       "4195                   4.177083                              1.000000   \n",
       "\n",
       "      label_bin  mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0             0              0.420495            0.697084   \n",
       "1             1              0.452070            0.516028   \n",
       "2             1              0.506079            0.719540   \n",
       "3             1              0.436408            0.725925   \n",
       "4             0              0.697087            0.756841   \n",
       "...         ...                   ...                 ...   \n",
       "4191          0              0.333789            0.746394   \n",
       "4192          0              0.493462            0.387989   \n",
       "4193          0              0.537812            0.747160   \n",
       "4194          1              0.469336            0.535746   \n",
       "4195          0              0.329257            0.111304   \n",
       "\n",
       "                                          features_loan  \\\n",
       "0     [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "1     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "2     [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "3     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "4     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "...                                                 ...   \n",
       "4191  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "4192  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4193  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4194  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "4195  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, -1, ...   \n",
       "\n",
       "                                          features_orig DNN_logits  CNN_logits  \n",
       "0     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -... -16.218758   -6.709172  \n",
       "1     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   8.018238    7.578733  \n",
       "2     [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   8.149307   13.127642  \n",
       "3     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   7.360963   11.419708  \n",
       "4     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,... -16.619883   -9.829449  \n",
       "...                                                 ...        ...         ...  \n",
       "4191  [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...  -7.594423  -20.254333  \n",
       "4192  [-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...  -7.256489  -12.875332  \n",
       "4193  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1... -20.811543  -18.016346  \n",
       "4194  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   9.651114   10.320404  \n",
       "4195  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ... -28.924120   -9.297581  \n",
       "\n",
       "[4194 rows x 21 columns]"
      ]
     },
     "execution_count": 1087,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "id": "33ccf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_withlogits_prod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec7dab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv('test_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1e2ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('train_withlogits.csv')\n",
    "# test = pd.read_csv('test_withlogits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d300fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CNN_logits'] =cnn_logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "id": "bf00151e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 21)"
      ]
     },
     "execution_count": 1089,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb600b2",
   "metadata": {},
   "source": [
    "# Getting logits from CNN test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "52bf251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_test(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib64/python3.6/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "class Net_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 2) # input is 1 image, 32 output channels, 2X2 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128,2)\n",
    "        \n",
    "\n",
    "        #x = torch.randn(23,23).view(-1,1,23,23)\n",
    "        x = torch.randn(30,30).view(-1,1,30,30)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 1) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.tanh(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.tanh(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.sigmoid(x), x, #comment it out to get the logits in the return statement \n",
    "        #return x\n",
    "                         \n",
    "\n",
    "\n",
    "net = Net_test().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "id": "596febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "id": "37be4634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib64/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.6879889369010925\n",
      "Epoch: 1. Loss: 0.6125616431236267\n",
      "Epoch: 2. Loss: 0.6369106769561768\n",
      "Epoch: 3. Loss: 0.6110633611679077\n",
      "Epoch: 4. Loss: 0.6238124370574951\n",
      "Epoch: 5. Loss: 0.6186586022377014\n",
      "Epoch: 6. Loss: 0.6053056716918945\n",
      "Epoch: 7. Loss: 0.6099947690963745\n",
      "Epoch: 8. Loss: 0.6117966175079346\n",
      "Epoch: 9. Loss: 0.6031708717346191\n",
      "Epoch: 10. Loss: 0.5995726585388184\n",
      "Epoch: 11. Loss: 0.603376567363739\n",
      "Epoch: 12. Loss: 0.6022711992263794\n",
      "Epoch: 13. Loss: 0.5960303544998169\n",
      "Epoch: 14. Loss: 0.5943571925163269\n",
      "Epoch: 15. Loss: 0.5968614816665649\n",
      "Epoch: 16. Loss: 0.5945163369178772\n",
      "Epoch: 17. Loss: 0.5900189876556396\n",
      "Epoch: 18. Loss: 0.5904158353805542\n",
      "Epoch: 19. Loss: 0.5912238359451294\n",
      "Epoch: 20. Loss: 0.5874618291854858\n",
      "Epoch: 21. Loss: 0.5851168632507324\n",
      "Epoch: 22. Loss: 0.5859919786453247\n",
      "Epoch: 23. Loss: 0.5836492776870728\n",
      "Epoch: 24. Loss: 0.5802352428436279\n",
      "Epoch: 25. Loss: 0.5801452994346619\n",
      "Epoch: 26. Loss: 0.5783249139785767\n",
      "Epoch: 27. Loss: 0.5747095942497253\n",
      "Epoch: 28. Loss: 0.5740224123001099\n",
      "Epoch: 29. Loss: 0.5716999173164368\n",
      "Epoch: 30. Loss: 0.5681557655334473\n",
      "Epoch: 31. Loss: 0.5671631097793579\n",
      "Epoch: 32. Loss: 0.5632854700088501\n",
      "Epoch: 33. Loss: 0.5608202219009399\n",
      "Epoch: 34. Loss: 0.5577400326728821\n",
      "Epoch: 35. Loss: 0.5538179278373718\n",
      "Epoch: 36. Loss: 0.5507335662841797\n",
      "Epoch: 37. Loss: 0.5463398098945618\n",
      "Epoch: 38. Loss: 0.5420693755149841\n",
      "Epoch: 39. Loss: 0.5383880138397217\n",
      "Epoch: 40. Loss: 0.532175600528717\n",
      "Epoch: 41. Loss: 0.5275101661682129\n",
      "Epoch: 42. Loss: 0.523630678653717\n",
      "Epoch: 43. Loss: 0.5188924670219421\n",
      "Epoch: 44. Loss: 0.5118541717529297\n",
      "Epoch: 45. Loss: 0.5042048096656799\n",
      "Epoch: 46. Loss: 0.4966261386871338\n",
      "Epoch: 47. Loss: 0.48909619450569153\n",
      "Epoch: 48. Loss: 0.4812726080417633\n",
      "Epoch: 49. Loss: 0.4737747311592102\n",
      "Epoch: 50. Loss: 0.4773019254207611\n",
      "Epoch: 51. Loss: 0.5405400991439819\n",
      "Epoch: 52. Loss: 0.4663848280906677\n",
      "Epoch: 53. Loss: 0.47392529249191284\n",
      "Epoch: 54. Loss: 0.46739983558654785\n",
      "Epoch: 55. Loss: 0.451924204826355\n",
      "Epoch: 56. Loss: 0.44338053464889526\n",
      "Epoch: 57. Loss: 0.43465349078178406\n",
      "Epoch: 58. Loss: 0.4228760600090027\n",
      "Epoch: 59. Loss: 0.4235974848270416\n",
      "Epoch: 60. Loss: 0.39980974793434143\n",
      "Epoch: 61. Loss: 0.409302294254303\n",
      "Epoch: 62. Loss: 0.38241052627563477\n",
      "Epoch: 63. Loss: 0.39097562432289124\n",
      "Epoch: 64. Loss: 0.3644481301307678\n",
      "Epoch: 65. Loss: 0.3727366328239441\n",
      "Epoch: 66. Loss: 0.34658294916152954\n",
      "Epoch: 67. Loss: 0.3514973521232605\n",
      "Epoch: 68. Loss: 0.32760655879974365\n",
      "Epoch: 69. Loss: 0.3307628333568573\n",
      "Epoch: 70. Loss: 0.30991655588150024\n",
      "Epoch: 71. Loss: 0.30552345514297485\n",
      "Epoch: 72. Loss: 0.29339513182640076\n",
      "Epoch: 73. Loss: 0.2779139578342438\n",
      "Epoch: 74. Loss: 0.27579265832901\n",
      "Epoch: 75. Loss: 0.2559407651424408\n",
      "Epoch: 76. Loss: 0.24877452850341797\n",
      "Epoch: 77. Loss: 0.24328269064426422\n",
      "Epoch: 78. Loss: 0.22408153116703033\n",
      "Epoch: 79. Loss: 0.21438725292682648\n",
      "Epoch: 80. Loss: 0.21149441599845886\n",
      "Epoch: 81. Loss: 0.19873298704624176\n",
      "Epoch: 82. Loss: 0.18229828774929047\n",
      "Epoch: 83. Loss: 0.1713908165693283\n",
      "Epoch: 84. Loss: 0.16669148206710815\n",
      "Epoch: 85. Loss: 0.16371355950832367\n",
      "Epoch: 86. Loss: 0.15552176535129547\n",
      "Epoch: 87. Loss: 0.1429213583469391\n",
      "Epoch: 88. Loss: 0.12731729447841644\n",
      "Epoch: 89. Loss: 0.11716506630182266\n",
      "Epoch: 90. Loss: 0.1131560206413269\n",
      "Epoch: 91. Loss: 0.11008429527282715\n",
      "Epoch: 92. Loss: 0.1037311777472496\n",
      "Epoch: 93. Loss: 0.09228595346212387\n",
      "Epoch: 94. Loss: 0.08225298672914505\n",
      "Epoch: 95. Loss: 0.07776744663715363\n",
      "Epoch: 96. Loss: 0.07526222616434097\n",
      "Epoch: 97. Loss: 0.06960214674472809\n",
      "Epoch: 98. Loss: 0.06155131012201309\n",
      "Epoch: 99. Loss: 0.056785836815834045\n",
      "Epoch: 100. Loss: 0.0547393299639225\n",
      "Epoch: 101. Loss: 0.050268352031707764\n",
      "Epoch: 102. Loss: 0.0448734425008297\n",
      "Epoch: 103. Loss: 0.042557477951049805\n",
      "Epoch: 104. Loss: 0.04002206772565842\n",
      "Epoch: 105. Loss: 0.03591560572385788\n",
      "Epoch: 106. Loss: 0.03393884748220444\n",
      "Epoch: 107. Loss: 0.03189072012901306\n",
      "Epoch: 108. Loss: 0.028934724628925323\n",
      "Epoch: 109. Loss: 0.027639951556921005\n",
      "Epoch: 110. Loss: 0.02554069645702839\n",
      "Epoch: 111. Loss: 0.023793740198016167\n",
      "Epoch: 112. Loss: 0.022622190415859222\n",
      "Epoch: 113. Loss: 0.02084004320204258\n",
      "Epoch: 114. Loss: 0.019976697862148285\n",
      "Epoch: 115. Loss: 0.01854616589844227\n",
      "Epoch: 116. Loss: 0.01771954633295536\n",
      "Epoch: 117. Loss: 0.01664709486067295\n",
      "Epoch: 118. Loss: 0.01586170494556427\n",
      "Epoch: 119. Loss: 0.015039869584143162\n",
      "Epoch: 120. Loss: 0.01434123795479536\n",
      "Epoch: 121. Loss: 0.013678309507668018\n",
      "Epoch: 122. Loss: 0.013080205768346786\n",
      "Epoch: 123. Loss: 0.012523654848337173\n",
      "Epoch: 124. Loss: 0.012030566111207008\n",
      "Epoch: 125. Loss: 0.011544626206159592\n",
      "Epoch: 126. Loss: 0.011142863892018795\n",
      "Epoch: 127. Loss: 0.010714792646467686\n",
      "Epoch: 128. Loss: 0.010392618365585804\n",
      "Epoch: 129. Loss: 0.010011217556893826\n",
      "Epoch: 130. Loss: 0.009743383154273033\n",
      "Epoch: 131. Loss: 0.009408858604729176\n",
      "Epoch: 132. Loss: 0.009180374443531036\n",
      "Epoch: 133. Loss: 0.008896447718143463\n",
      "Epoch: 134. Loss: 0.008689315058290958\n",
      "Epoch: 135. Loss: 0.008455484174191952\n",
      "Epoch: 136. Loss: 0.008261490613222122\n",
      "Epoch: 137. Loss: 0.00807133037596941\n",
      "Epoch: 138. Loss: 0.007885818369686604\n",
      "Epoch: 139. Loss: 0.007731693331152201\n",
      "Epoch: 140. Loss: 0.007561527192592621\n",
      "Epoch: 141. Loss: 0.007428449112921953\n",
      "Epoch: 142. Loss: 0.007277084980159998\n",
      "Epoch: 143. Loss: 0.007155554369091988\n",
      "Epoch: 144. Loss: 0.0070264614187181\n",
      "Epoch: 145. Loss: 0.006912253797054291\n",
      "Epoch: 146. Loss: 0.006802890915423632\n",
      "Epoch: 147. Loss: 0.006694010924547911\n",
      "Epoch: 148. Loss: 0.006599624641239643\n",
      "Epoch: 149. Loss: 0.006498669274151325\n",
      "Epoch: 150. Loss: 0.006414318457245827\n",
      "Epoch: 151. Loss: 0.0063253529369831085\n",
      "Epoch: 152. Loss: 0.006245508790016174\n",
      "Epoch: 153. Loss: 0.006166188977658749\n",
      "Epoch: 154. Loss: 0.006091837305575609\n",
      "Epoch: 155. Loss: 0.006022105924785137\n",
      "Epoch: 156. Loss: 0.005951513070613146\n",
      "Epoch: 157. Loss: 0.005888666491955519\n",
      "Epoch: 158. Loss: 0.005823711398988962\n",
      "Epoch: 159. Loss: 0.00576534029096365\n",
      "Epoch: 160. Loss: 0.005706104449927807\n",
      "Epoch: 161. Loss: 0.005651148501783609\n",
      "Epoch: 162. Loss: 0.005597208626568317\n",
      "Epoch: 163. Loss: 0.005545283667743206\n",
      "Epoch: 164. Loss: 0.0054961866699159145\n",
      "Epoch: 165. Loss: 0.0054474277421832085\n",
      "Epoch: 166. Loss: 0.0054018376395106316\n",
      "Epoch: 167. Loss: 0.005356600508093834\n",
      "Epoch: 168. Loss: 0.005313768982887268\n",
      "Epoch: 169. Loss: 0.005271183792501688\n",
      "Epoch: 170. Loss: 0.005231665913015604\n",
      "Epoch: 171. Loss: 0.005191920790821314\n",
      "Epoch: 172. Loss: 0.005153967067599297\n",
      "Epoch: 173. Loss: 0.005116727668792009\n",
      "Epoch: 174. Loss: 0.005081027280539274\n",
      "Epoch: 175. Loss: 0.005046433303505182\n",
      "Epoch: 176. Loss: 0.005012126639485359\n",
      "Epoch: 177. Loss: 0.004979797173291445\n",
      "Epoch: 178. Loss: 0.00494776526466012\n",
      "Epoch: 179. Loss: 0.004916984122246504\n",
      "Epoch: 180. Loss: 0.004886365961283445\n",
      "Epoch: 181. Loss: 0.00485734548419714\n",
      "Epoch: 182. Loss: 0.004828363191336393\n",
      "Epoch: 183. Loss: 0.004800849594175816\n",
      "Epoch: 184. Loss: 0.004773606546223164\n",
      "Epoch: 185. Loss: 0.004747182596474886\n",
      "Epoch: 186. Loss: 0.004721168894320726\n",
      "Epoch: 187. Loss: 0.00469633936882019\n",
      "Epoch: 188. Loss: 0.004671597387641668\n",
      "Epoch: 189. Loss: 0.004647467285394669\n",
      "Epoch: 190. Loss: 0.004624034743756056\n",
      "Epoch: 191. Loss: 0.004601502791047096\n",
      "Epoch: 192. Loss: 0.004579268861562014\n",
      "Epoch: 193. Loss: 0.004557373002171516\n",
      "Epoch: 194. Loss: 0.0045357621274888515\n",
      "Epoch: 195. Loss: 0.004514963831752539\n",
      "Epoch: 196. Loss: 0.004494800232350826\n",
      "Epoch: 197. Loss: 0.004474880639463663\n",
      "Epoch: 198. Loss: 0.004455484915524721\n",
      "Epoch: 199. Loss: 0.004436235874891281\n",
      "Epoch: 200. Loss: 0.0044175731018185616\n",
      "Epoch: 201. Loss: 0.004399511963129044\n",
      "Epoch: 202. Loss: 0.004381492733955383\n",
      "Epoch: 203. Loss: 0.004363951738923788\n",
      "Epoch: 204. Loss: 0.004346658941358328\n",
      "Epoch: 205. Loss: 0.004330080468207598\n",
      "Epoch: 206. Loss: 0.004313447047024965\n",
      "Epoch: 207. Loss: 0.004297320265322924\n",
      "Epoch: 208. Loss: 0.0042813485488295555\n",
      "Epoch: 209. Loss: 0.004265790805220604\n",
      "Epoch: 210. Loss: 0.004250493366271257\n",
      "Epoch: 211. Loss: 0.004235570318996906\n",
      "Epoch: 212. Loss: 0.0042208414524793625\n",
      "Epoch: 213. Loss: 0.004206547513604164\n",
      "Epoch: 214. Loss: 0.004192234482616186\n",
      "Epoch: 215. Loss: 0.004178262781351805\n",
      "Epoch: 216. Loss: 0.004164764657616615\n",
      "Epoch: 217. Loss: 0.004151301458477974\n",
      "Epoch: 218. Loss: 0.004138116724789143\n",
      "Epoch: 219. Loss: 0.004125427920371294\n",
      "Epoch: 220. Loss: 0.004112647846341133\n",
      "Epoch: 221. Loss: 0.004100034944713116\n",
      "Epoch: 222. Loss: 0.004087886773049831\n",
      "Epoch: 223. Loss: 0.00407577957957983\n",
      "Epoch: 224. Loss: 0.004063952714204788\n",
      "Epoch: 225. Loss: 0.004052391741424799\n",
      "Epoch: 226. Loss: 0.004040950443595648\n",
      "Epoch: 227. Loss: 0.004029587376862764\n",
      "Epoch: 228. Loss: 0.004018531646579504\n",
      "Epoch: 229. Loss: 0.004007676150649786\n",
      "Epoch: 230. Loss: 0.003996999468654394\n",
      "Epoch: 231. Loss: 0.003986604046076536\n",
      "Epoch: 232. Loss: 0.003976128995418549\n",
      "Epoch: 233. Loss: 0.003965773619711399\n",
      "Epoch: 234. Loss: 0.003955833613872528\n",
      "Epoch: 235. Loss: 0.003946163225919008\n",
      "Epoch: 236. Loss: 0.003936240449547768\n",
      "Epoch: 237. Loss: 0.003926558885723352\n",
      "Epoch: 238. Loss: 0.003917280118912458\n",
      "Epoch: 239. Loss: 0.003908198326826096\n",
      "Epoch: 240. Loss: 0.003898934694007039\n",
      "Epoch: 241. Loss: 0.0038899420760571957\n",
      "Epoch: 242. Loss: 0.0038810705300420523\n",
      "Epoch: 243. Loss: 0.0038723053876310587\n",
      "Epoch: 244. Loss: 0.0038639125414192677\n",
      "Epoch: 245. Loss: 0.0038552882615476847\n",
      "Epoch: 246. Loss: 0.0038470420986413956\n",
      "Epoch: 247. Loss: 0.0038389035034924746\n",
      "Epoch: 248. Loss: 0.0038309853989630938\n",
      "Epoch: 249. Loss: 0.003822788130491972\n",
      "Epoch: 250. Loss: 0.003814992029219866\n",
      "Epoch: 251. Loss: 0.0038073642645031214\n",
      "Epoch: 252. Loss: 0.0037995243910700083\n",
      "Epoch: 253. Loss: 0.0037923539057374\n",
      "Epoch: 254. Loss: 0.0037847377825528383\n",
      "Epoch: 255. Loss: 0.0037774641532450914\n",
      "Epoch: 256. Loss: 0.0037703211419284344\n",
      "Epoch: 257. Loss: 0.003763150190934539\n",
      "Epoch: 258. Loss: 0.0037562474608421326\n",
      "Epoch: 259. Loss: 0.003749330760911107\n",
      "Epoch: 260. Loss: 0.003742423141375184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261. Loss: 0.003735827747732401\n",
      "Epoch: 262. Loss: 0.0037291485350579023\n",
      "Epoch: 263. Loss: 0.003722871420904994\n",
      "Epoch: 264. Loss: 0.0037162327207624912\n",
      "Epoch: 265. Loss: 0.0037098033353686333\n",
      "Epoch: 266. Loss: 0.0037036980502307415\n",
      "Epoch: 267. Loss: 0.003697441192343831\n",
      "Epoch: 268. Loss: 0.003691348945721984\n",
      "Epoch: 269. Loss: 0.0036852224729955196\n",
      "Epoch: 270. Loss: 0.003679449437186122\n",
      "Epoch: 271. Loss: 0.00367349898442626\n",
      "Epoch: 272. Loss: 0.003667603712528944\n",
      "Epoch: 273. Loss: 0.00366196408867836\n",
      "Epoch: 274. Loss: 0.0036563824396580458\n",
      "Epoch: 275. Loss: 0.0036507737822830677\n",
      "Epoch: 276. Loss: 0.0036452694330364466\n",
      "Epoch: 277. Loss: 0.0036398505326360464\n",
      "Epoch: 278. Loss: 0.003634657012298703\n",
      "Epoch: 279. Loss: 0.0036293251905590296\n",
      "Epoch: 280. Loss: 0.003624028991907835\n",
      "Epoch: 281. Loss: 0.0036188983358442783\n",
      "Epoch: 282. Loss: 0.003613774897530675\n",
      "Epoch: 283. Loss: 0.003608692204579711\n",
      "Epoch: 284. Loss: 0.003603774355724454\n",
      "Epoch: 285. Loss: 0.0035988634917885065\n",
      "Epoch: 286. Loss: 0.0035940518137067556\n",
      "Epoch: 287. Loss: 0.0035892310552299023\n",
      "Epoch: 288. Loss: 0.0035845639649778605\n",
      "Epoch: 289. Loss: 0.0035799602046608925\n",
      "Epoch: 290. Loss: 0.003575362963601947\n",
      "Epoch: 291. Loss: 0.0035707526840269566\n",
      "Epoch: 292. Loss: 0.0035662916488945484\n",
      "Epoch: 293. Loss: 0.0035617807880043983\n",
      "Epoch: 294. Loss: 0.0035573504865169525\n",
      "Epoch: 295. Loss: 0.00355300004594028\n",
      "Epoch: 296. Loss: 0.0035486682318150997\n",
      "Epoch: 297. Loss: 0.003544552717357874\n",
      "Epoch: 298. Loss: 0.0035402998328208923\n",
      "Epoch: 299. Loss: 0.00353613100014627\n",
      "Epoch: 300. Loss: 0.0035320306196808815\n",
      "Epoch: 301. Loss: 0.003528062952682376\n",
      "Epoch: 302. Loss: 0.00352395698428154\n",
      "Epoch: 303. Loss: 0.003520065452903509\n",
      "Epoch: 304. Loss: 0.0035160528495907784\n",
      "Epoch: 305. Loss: 0.0035122043918818235\n",
      "Epoch: 306. Loss: 0.0035084800329059362\n",
      "Epoch: 307. Loss: 0.0035045251715928316\n",
      "Epoch: 308. Loss: 0.003500801045447588\n",
      "Epoch: 309. Loss: 0.003497113473713398\n",
      "Epoch: 310. Loss: 0.0034935204312205315\n",
      "Epoch: 311. Loss: 0.003489760449156165\n",
      "Epoch: 312. Loss: 0.0034862766042351723\n",
      "Epoch: 313. Loss: 0.003482707077637315\n",
      "Epoch: 314. Loss: 0.00347911287099123\n",
      "Epoch: 315. Loss: 0.0034757223911583424\n",
      "Epoch: 316. Loss: 0.0034724962897598743\n",
      "Epoch: 317. Loss: 0.003468833863735199\n",
      "Epoch: 318. Loss: 0.003465535817667842\n",
      "Epoch: 319. Loss: 0.003462154883891344\n",
      "Epoch: 320. Loss: 0.0034589373972266912\n",
      "Epoch: 321. Loss: 0.0034557790495455265\n",
      "Epoch: 322. Loss: 0.003452317090705037\n",
      "Epoch: 323. Loss: 0.0034492400009185076\n",
      "Epoch: 324. Loss: 0.00344610377214849\n",
      "Epoch: 325. Loss: 0.0034429116640239954\n",
      "Epoch: 326. Loss: 0.003439820371568203\n",
      "Epoch: 327. Loss: 0.0034367546904832125\n",
      "Epoch: 328. Loss: 0.0034337088000029325\n",
      "Epoch: 329. Loss: 0.003430750221014023\n",
      "Epoch: 330. Loss: 0.003427728544920683\n",
      "Epoch: 331. Loss: 0.0034247867297381163\n",
      "Epoch: 332. Loss: 0.0034218821674585342\n",
      "Epoch: 333. Loss: 0.0034190083388239145\n",
      "Epoch: 334. Loss: 0.0034161191433668137\n",
      "Epoch: 335. Loss: 0.0034133512526750565\n",
      "Epoch: 336. Loss: 0.0034105209633708\n",
      "Epoch: 337. Loss: 0.003407722571864724\n",
      "Epoch: 338. Loss: 0.0034050841350108385\n",
      "Epoch: 339. Loss: 0.0034022685140371323\n",
      "Epoch: 340. Loss: 0.003399594919756055\n",
      "Epoch: 341. Loss: 0.003396941814571619\n",
      "Epoch: 342. Loss: 0.0033942817244678736\n",
      "Epoch: 343. Loss: 0.0033917247783392668\n",
      "Epoch: 344. Loss: 0.003389188554137945\n",
      "Epoch: 345. Loss: 0.003386598778888583\n",
      "Epoch: 346. Loss: 0.0033839463721960783\n",
      "Epoch: 347. Loss: 0.003381533781066537\n",
      "Epoch: 348. Loss: 0.0033790282905101776\n",
      "Epoch: 349. Loss: 0.0033765321131795645\n",
      "Epoch: 350. Loss: 0.0033740312792360783\n",
      "Epoch: 351. Loss: 0.0033716741017997265\n",
      "Epoch: 352. Loss: 0.003369176760315895\n",
      "Epoch: 353. Loss: 0.0033668025862425566\n",
      "Epoch: 354. Loss: 0.003364556236192584\n",
      "Epoch: 355. Loss: 0.003362182527780533\n",
      "Epoch: 356. Loss: 0.0033598472364246845\n",
      "Epoch: 357. Loss: 0.003357677487656474\n",
      "Epoch: 358. Loss: 0.003355475375428796\n",
      "Epoch: 359. Loss: 0.003353063017129898\n",
      "Epoch: 360. Loss: 0.003350905142724514\n",
      "Epoch: 361. Loss: 0.003348857630044222\n",
      "Epoch: 362. Loss: 0.0033465505111962557\n",
      "Epoch: 363. Loss: 0.003344257827848196\n",
      "Epoch: 364. Loss: 0.0033423055429011583\n",
      "Epoch: 365. Loss: 0.0033402773551642895\n",
      "Epoch: 366. Loss: 0.0033379870001226664\n",
      "Epoch: 367. Loss: 0.0033358989749103785\n",
      "Epoch: 368. Loss: 0.003333859145641327\n",
      "Epoch: 369. Loss: 0.0033318193163722754\n",
      "Epoch: 370. Loss: 0.0033296586479991674\n",
      "Epoch: 371. Loss: 0.003327645594254136\n",
      "Epoch: 372. Loss: 0.003325625089928508\n",
      "Epoch: 373. Loss: 0.003323865821585059\n",
      "Epoch: 374. Loss: 0.0033219198230654\n",
      "Epoch: 375. Loss: 0.003319812472909689\n",
      "Epoch: 376. Loss: 0.003317779628559947\n",
      "Epoch: 377. Loss: 0.003315852489322424\n",
      "Epoch: 378. Loss: 0.0033139935694634914\n",
      "Epoch: 379. Loss: 0.0033120650332421064\n",
      "Epoch: 380. Loss: 0.0033102240413427353\n",
      "Epoch: 381. Loss: 0.003308336017653346\n",
      "Epoch: 382. Loss: 0.0033065874595195055\n",
      "Epoch: 383. Loss: 0.0033046728931367397\n",
      "Epoch: 384. Loss: 0.0033028775360435247\n",
      "Epoch: 385. Loss: 0.0033011329360306263\n",
      "Epoch: 386. Loss: 0.0032993631903082132\n",
      "Epoch: 387. Loss: 0.003297501942142844\n",
      "Epoch: 388. Loss: 0.003295765956863761\n",
      "Epoch: 389. Loss: 0.00329409446567297\n",
      "Epoch: 390. Loss: 0.0032924178522080183\n",
      "Epoch: 391. Loss: 0.0032905933912843466\n",
      "Epoch: 392. Loss: 0.0032889305148273706\n",
      "Epoch: 393. Loss: 0.0032871589064598083\n",
      "Epoch: 394. Loss: 0.0032856499310582876\n",
      "Epoch: 395. Loss: 0.0032839360646903515\n",
      "Epoch: 396. Loss: 0.0032822731882333755\n",
      "Epoch: 397. Loss: 0.003280655248090625\n",
      "Epoch: 398. Loss: 0.0032790254335850477\n",
      "Epoch: 399. Loss: 0.0032774864230304956\n",
      "Epoch: 400. Loss: 0.0032758789602667093\n",
      "Epoch: 401. Loss: 0.003274343442171812\n",
      "Epoch: 402. Loss: 0.003272741101682186\n",
      "Epoch: 403. Loss: 0.003271285444498062\n",
      "Epoch: 404. Loss: 0.003269712207838893\n",
      "Epoch: 405. Loss: 0.0032682670280337334\n",
      "Epoch: 406. Loss: 0.003267285181209445\n",
      "Epoch: 407. Loss: 0.003266015090048313\n",
      "Epoch: 408. Loss: 0.0032643333543092012\n",
      "Epoch: 409. Loss: 0.003262352431192994\n",
      "Epoch: 410. Loss: 0.0032606893219053745\n",
      "Epoch: 411. Loss: 0.0032594199292361736\n",
      "Epoch: 412. Loss: 0.0032581817358732224\n",
      "Epoch: 413. Loss: 0.003256849944591522\n",
      "Epoch: 414. Loss: 0.0032552152406424284\n",
      "Epoch: 415. Loss: 0.0032535919453948736\n",
      "Epoch: 416. Loss: 0.0032521896064281464\n",
      "Epoch: 417. Loss: 0.0032510608434677124\n",
      "Epoch: 418. Loss: 0.003250066190958023\n",
      "Epoch: 419. Loss: 0.003248916706070304\n",
      "Epoch: 420. Loss: 0.003247212152928114\n",
      "Epoch: 421. Loss: 0.0032455073669552803\n",
      "Epoch: 422. Loss: 0.003244099672883749\n",
      "Epoch: 423. Loss: 0.0032431562431156635\n",
      "Epoch: 424. Loss: 0.0032422326039522886\n",
      "Epoch: 425. Loss: 0.003240834455937147\n",
      "Epoch: 426. Loss: 0.0032391082495450974\n",
      "Epoch: 427. Loss: 0.0032375173177570105\n",
      "Epoch: 428. Loss: 0.003236334305256605\n",
      "Epoch: 429. Loss: 0.0032356088049709797\n",
      "Epoch: 430. Loss: 0.00323482695966959\n",
      "Epoch: 431. Loss: 0.0032334006391465664\n",
      "Epoch: 432. Loss: 0.0032317102886736393\n",
      "Epoch: 433. Loss: 0.0032301051542162895\n",
      "Epoch: 434. Loss: 0.003228795016184449\n",
      "Epoch: 435. Loss: 0.0032276541460305452\n",
      "Epoch: 436. Loss: 0.003226704429835081\n",
      "Epoch: 437. Loss: 0.0032256392296403646\n",
      "Epoch: 438. Loss: 0.003224365646019578\n",
      "Epoch: 439. Loss: 0.0032230974175035954\n",
      "Epoch: 440. Loss: 0.003221833147108555\n",
      "Epoch: 441. Loss: 0.003220584010705352\n",
      "Epoch: 442. Loss: 0.00321938400156796\n",
      "Epoch: 443. Loss: 0.0032182252034544945\n",
      "Epoch: 444. Loss: 0.0032170747872442007\n",
      "Epoch: 445. Loss: 0.0032159914262592793\n",
      "Epoch: 446. Loss: 0.0032148524187505245\n",
      "Epoch: 447. Loss: 0.003213724819943309\n",
      "Epoch: 448. Loss: 0.0032126621808856726\n",
      "Epoch: 449. Loss: 0.0032116640359163284\n",
      "Epoch: 450. Loss: 0.003210552269592881\n",
      "Epoch: 451. Loss: 0.003209460526704788\n",
      "Epoch: 452. Loss: 0.0032084500417113304\n",
      "Epoch: 453. Loss: 0.0032073725014925003\n",
      "Epoch: 454. Loss: 0.003206410678103566\n",
      "Epoch: 455. Loss: 0.003205515444278717\n",
      "Epoch: 456. Loss: 0.0032044993713498116\n",
      "Epoch: 457. Loss: 0.0032034339383244514\n",
      "Epoch: 458. Loss: 0.0032023959793150425\n",
      "Epoch: 459. Loss: 0.003201443236321211\n",
      "Epoch: 460. Loss: 0.0032004634849727154\n",
      "Epoch: 461. Loss: 0.0031998748891055584\n",
      "Epoch: 462. Loss: 0.0031999798957258463\n",
      "Epoch: 463. Loss: 0.0032006651163101196\n",
      "Epoch: 464. Loss: 0.0032013345044106245\n",
      "Epoch: 465. Loss: 0.0032020974904298782\n",
      "Epoch: 466. Loss: 0.003203019266948104\n",
      "Epoch: 467. Loss: 0.003204153385013342\n",
      "Epoch: 468. Loss: 0.0032047524582594633\n",
      "Epoch: 469. Loss: 0.0032048041466623545\n",
      "Epoch: 470. Loss: 0.003204926149919629\n",
      "Epoch: 471. Loss: 0.0032065901905298233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 472. Loss: 0.0032102365512400866\n",
      "Epoch: 473. Loss: 0.0032160275150090456\n",
      "Epoch: 474. Loss: 0.0032231782097369432\n",
      "Epoch: 475. Loss: 0.003233846742659807\n",
      "Epoch: 476. Loss: 0.003246038220822811\n",
      "Epoch: 477. Loss: 0.003257819451391697\n",
      "Epoch: 478. Loss: 0.0032691792584955692\n",
      "Epoch: 479. Loss: 0.0032801178749650717\n",
      "Epoch: 480. Loss: 0.0032866313122212887\n",
      "Epoch: 481. Loss: 0.0032860725186765194\n",
      "Epoch: 482. Loss: 0.0032964914571493864\n",
      "Epoch: 483. Loss: 0.003309728344902396\n",
      "Epoch: 484. Loss: 0.003323278622701764\n",
      "Epoch: 485. Loss: 0.0033365769777446985\n",
      "Epoch: 486. Loss: 0.003342409385368228\n",
      "Epoch: 487. Loss: 0.003336160909384489\n",
      "Epoch: 488. Loss: 0.0033204418141394854\n",
      "Epoch: 489. Loss: 0.0033002591226249933\n",
      "Epoch: 490. Loss: 0.003278893418610096\n",
      "Epoch: 491. Loss: 0.003258424811065197\n",
      "Epoch: 492. Loss: 0.0032404905650764704\n",
      "Epoch: 493. Loss: 0.0032262129243463278\n",
      "Epoch: 494. Loss: 0.0032156112138181925\n",
      "Epoch: 495. Loss: 0.003208945272490382\n",
      "Epoch: 496. Loss: 0.0032064171973615885\n",
      "Epoch: 497. Loss: 0.0032077401410788298\n",
      "Epoch: 498. Loss: 0.0032134265638887882\n",
      "Epoch: 499. Loss: 0.003226836444810033\n",
      "Epoch: 500. Loss: 0.0032491039019078016\n",
      "Epoch: 501. Loss: 0.0032803730573505163\n",
      "Epoch: 502. Loss: 0.003318758914247155\n",
      "Epoch: 503. Loss: 0.0033558662980794907\n",
      "Epoch: 504. Loss: 0.003376639448106289\n",
      "Epoch: 505. Loss: 0.003365833079442382\n",
      "Epoch: 506. Loss: 0.0033311902079731226\n",
      "Epoch: 507. Loss: 0.0032885822001844645\n",
      "Epoch: 508. Loss: 0.0032495460473001003\n",
      "Epoch: 509. Loss: 0.003217240795493126\n",
      "Epoch: 510. Loss: 0.0031936292070895433\n",
      "Epoch: 511. Loss: 0.003177632810547948\n",
      "Epoch: 512. Loss: 0.003167727030813694\n",
      "Epoch: 513. Loss: 0.0031614406034350395\n",
      "Epoch: 514. Loss: 0.0031577988993376493\n",
      "Epoch: 515. Loss: 0.003156025428324938\n",
      "Epoch: 516. Loss: 0.0031554356683045626\n",
      "Epoch: 517. Loss: 0.00315622566267848\n",
      "Epoch: 518. Loss: 0.00315867573954165\n",
      "Epoch: 519. Loss: 0.003162998938933015\n",
      "Epoch: 520. Loss: 0.0031711324118077755\n",
      "Epoch: 521. Loss: 0.003186060581356287\n",
      "Epoch: 522. Loss: 0.0032137189991772175\n",
      "Epoch: 523. Loss: 0.0032627275213599205\n",
      "Epoch: 524. Loss: 0.0033384840935468674\n",
      "Epoch: 525. Loss: 0.0034320689737796783\n",
      "Epoch: 526. Loss: 0.0035030092112720013\n",
      "Epoch: 527. Loss: 0.003503273008391261\n",
      "Epoch: 528. Loss: 0.0034332992509007454\n",
      "Epoch: 529. Loss: 0.003340024035423994\n",
      "Epoch: 530. Loss: 0.0032639596611261368\n",
      "Epoch: 531. Loss: 0.0032138261012732983\n",
      "Epoch: 532. Loss: 0.0031840638257563114\n",
      "Epoch: 533. Loss: 0.0031669519376009703\n",
      "Epoch: 534. Loss: 0.0031574557069689035\n",
      "Epoch: 535. Loss: 0.0031534330919384956\n",
      "Epoch: 536. Loss: 0.003153033321723342\n",
      "Epoch: 537. Loss: 0.0031546715181320906\n",
      "Epoch: 538. Loss: 0.003157753264531493\n",
      "Epoch: 539. Loss: 0.0031630813609808683\n",
      "Epoch: 540. Loss: 0.003172882366925478\n",
      "Epoch: 541. Loss: 0.0031922708731144667\n",
      "Epoch: 542. Loss: 0.003227963112294674\n",
      "Epoch: 543. Loss: 0.0032868273556232452\n",
      "Epoch: 544. Loss: 0.0033710021525621414\n",
      "Epoch: 545. Loss: 0.00346105033531785\n",
      "Epoch: 546. Loss: 0.00351046328432858\n",
      "Epoch: 547. Loss: 0.003482949687168002\n",
      "Epoch: 548. Loss: 0.0033979963045567274\n",
      "Epoch: 549. Loss: 0.00330698536708951\n",
      "Epoch: 550. Loss: 0.0032389843836426735\n",
      "Epoch: 551. Loss: 0.0031967556569725275\n",
      "Epoch: 552. Loss: 0.0031729962211102247\n",
      "Epoch: 553. Loss: 0.0031604671385139227\n",
      "Epoch: 554. Loss: 0.0031553241424262524\n",
      "Epoch: 555. Loss: 0.0031559220515191555\n",
      "Epoch: 556. Loss: 0.0031610082369297743\n",
      "Epoch: 557. Loss: 0.0031718388199806213\n",
      "Epoch: 558. Loss: 0.0031901400070637465\n",
      "Epoch: 559. Loss: 0.0032199849374592304\n",
      "Epoch: 560. Loss: 0.0032648087944835424\n",
      "Epoch: 561. Loss: 0.0033245328813791275\n",
      "Epoch: 562. Loss: 0.003388513345271349\n",
      "Epoch: 563. Loss: 0.003436034545302391\n",
      "Epoch: 564. Loss: 0.0034432332031428814\n",
      "Epoch: 565. Loss: 0.0034092336427420378\n",
      "Epoch: 566. Loss: 0.0033539992291480303\n",
      "Epoch: 567. Loss: 0.0033008994068950415\n",
      "Epoch: 568. Loss: 0.0032597023528069258\n",
      "Epoch: 569. Loss: 0.0032306332141160965\n",
      "Epoch: 570. Loss: 0.0032120647374540567\n",
      "Epoch: 571. Loss: 0.003205724060535431\n",
      "Epoch: 572. Loss: 0.003208630485460162\n",
      "Epoch: 573. Loss: 0.00321909855119884\n",
      "Epoch: 574. Loss: 0.003236275864765048\n",
      "Epoch: 575. Loss: 0.00326202972792089\n",
      "Epoch: 576. Loss: 0.003296018810942769\n",
      "Epoch: 577. Loss: 0.0033331739250570536\n",
      "Epoch: 578. Loss: 0.0033609166275709867\n",
      "Epoch: 579. Loss: 0.0033716836478561163\n",
      "Epoch: 580. Loss: 0.0033617382869124413\n",
      "Epoch: 581. Loss: 0.0033343692775815725\n",
      "Epoch: 582. Loss: 0.0032977391965687275\n",
      "Epoch: 583. Loss: 0.003265472361817956\n",
      "Epoch: 584. Loss: 0.0032424377277493477\n",
      "Epoch: 585. Loss: 0.003228851594030857\n",
      "Epoch: 586. Loss: 0.003222618019208312\n",
      "Epoch: 587. Loss: 0.0032213316299021244\n",
      "Epoch: 588. Loss: 0.003225859021767974\n",
      "Epoch: 589. Loss: 0.0032361692283302546\n",
      "Epoch: 590. Loss: 0.003253340721130371\n",
      "Epoch: 591. Loss: 0.003275359282270074\n",
      "Epoch: 592. Loss: 0.003298922209069133\n",
      "Epoch: 593. Loss: 0.0033164131455123425\n",
      "Epoch: 594. Loss: 0.0033217021264135838\n",
      "Epoch: 595. Loss: 0.0033111947122961283\n",
      "Epoch: 596. Loss: 0.0032900418154895306\n",
      "Epoch: 597. Loss: 0.003265690291300416\n",
      "Epoch: 598. Loss: 0.003243577666580677\n",
      "Epoch: 599. Loss: 0.0032277158461511135\n",
      "Epoch: 600. Loss: 0.003218947211280465\n",
      "Epoch: 601. Loss: 0.003218505997210741\n",
      "Epoch: 602. Loss: 0.003224434331059456\n",
      "Epoch: 603. Loss: 0.003234399715438485\n",
      "Epoch: 604. Loss: 0.003248050808906555\n",
      "Epoch: 605. Loss: 0.003265641164034605\n",
      "Epoch: 606. Loss: 0.0032848697155714035\n",
      "Epoch: 607. Loss: 0.00330266822129488\n",
      "Epoch: 608. Loss: 0.0033163484185934067\n",
      "Epoch: 609. Loss: 0.0033267990220338106\n",
      "Epoch: 610. Loss: 0.0033253610599786043\n",
      "Epoch: 611. Loss: 0.003314732573926449\n",
      "Epoch: 612. Loss: 0.0033007245510816574\n",
      "Epoch: 613. Loss: 0.0032863039523363113\n",
      "Epoch: 614. Loss: 0.0032731760293245316\n",
      "Epoch: 615. Loss: 0.0032589926850050688\n",
      "Epoch: 616. Loss: 0.0032452368177473545\n",
      "Epoch: 617. Loss: 0.003234456293284893\n",
      "Epoch: 618. Loss: 0.003228939138352871\n",
      "Epoch: 619. Loss: 0.0032296343706548214\n",
      "Epoch: 620. Loss: 0.0032375568989664316\n",
      "Epoch: 621. Loss: 0.0032542026601731777\n",
      "Epoch: 622. Loss: 0.003276679664850235\n",
      "Epoch: 623. Loss: 0.0032998528331518173\n",
      "Epoch: 624. Loss: 0.0033157081343233585\n",
      "Epoch: 625. Loss: 0.0033175586722791195\n",
      "Epoch: 626. Loss: 0.003305880818516016\n",
      "Epoch: 627. Loss: 0.0032839009072631598\n",
      "Epoch: 628. Loss: 0.0032578918617218733\n",
      "Epoch: 629. Loss: 0.0032350618857890368\n",
      "Epoch: 630. Loss: 0.003218306927010417\n",
      "Epoch: 631. Loss: 0.003206622553989291\n",
      "Epoch: 632. Loss: 0.0032024041283875704\n",
      "Epoch: 633. Loss: 0.0032053538598120213\n",
      "Epoch: 634. Loss: 0.0032164892181754112\n",
      "Epoch: 635. Loss: 0.0032353862188756466\n",
      "Epoch: 636. Loss: 0.0032598243560642004\n",
      "Epoch: 637. Loss: 0.0032849397975951433\n",
      "Epoch: 638. Loss: 0.0033081728033721447\n",
      "Epoch: 639. Loss: 0.003325420431792736\n",
      "Epoch: 640. Loss: 0.0033310134895145893\n",
      "Epoch: 641. Loss: 0.003320080926641822\n",
      "Epoch: 642. Loss: 0.003297653282061219\n",
      "Epoch: 643. Loss: 0.0032736670691519976\n",
      "Epoch: 644. Loss: 0.0032516061328351498\n",
      "Epoch: 645. Loss: 0.0032336474396288395\n",
      "Epoch: 646. Loss: 0.0032261223532259464\n",
      "Epoch: 647. Loss: 0.003229933325201273\n",
      "Epoch: 648. Loss: 0.0032412935979664326\n",
      "Epoch: 649. Loss: 0.0032562618143856525\n",
      "Epoch: 650. Loss: 0.003272346220910549\n",
      "Epoch: 651. Loss: 0.0032886459957808256\n",
      "Epoch: 652. Loss: 0.003299389500170946\n",
      "Epoch: 653. Loss: 0.0033025715965777636\n",
      "Epoch: 654. Loss: 0.003298782976344228\n",
      "Epoch: 655. Loss: 0.003290976630523801\n",
      "Epoch: 656. Loss: 0.003281289478763938\n",
      "Epoch: 657. Loss: 0.00326961069367826\n",
      "Epoch: 658. Loss: 0.0032568294554948807\n",
      "Epoch: 659. Loss: 0.003247419372200966\n",
      "Epoch: 660. Loss: 0.0032421760261058807\n",
      "Epoch: 661. Loss: 0.0032394533045589924\n",
      "Epoch: 662. Loss: 0.0032402900978922844\n",
      "Epoch: 663. Loss: 0.003247516928240657\n",
      "Epoch: 664. Loss: 0.0032597805839031935\n",
      "Epoch: 665. Loss: 0.003275256836786866\n",
      "Epoch: 666. Loss: 0.003290981985628605\n",
      "Epoch: 667. Loss: 0.0033008744940161705\n",
      "Epoch: 668. Loss: 0.0033016621600836515\n",
      "Epoch: 669. Loss: 0.003292802022770047\n",
      "Epoch: 670. Loss: 0.003281233599409461\n",
      "Epoch: 671. Loss: 0.003270439337939024\n",
      "Epoch: 672. Loss: 0.003261545905843377\n",
      "Epoch: 673. Loss: 0.00325512676499784\n",
      "Epoch: 674. Loss: 0.003252893453463912\n",
      "Epoch: 675. Loss: 0.0032536620274186134\n",
      "Epoch: 676. Loss: 0.0032539591193199158\n",
      "Epoch: 677. Loss: 0.003274644026532769\n",
      "Epoch: 678. Loss: 0.0033127269707620144\n",
      "Epoch: 679. Loss: 0.0033630914986133575\n",
      "Epoch: 680. Loss: 0.0034132853616029024\n",
      "Epoch: 681. Loss: 0.0034407239872962236\n",
      "Epoch: 682. Loss: 0.0034312838688492775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 683. Loss: 0.0033903131261467934\n",
      "Epoch: 684. Loss: 0.0033355415798723698\n",
      "Epoch: 685. Loss: 0.0032855942845344543\n",
      "Epoch: 686. Loss: 0.0032496112398803234\n",
      "Epoch: 687. Loss: 0.0032318660523742437\n",
      "Epoch: 688. Loss: 0.003230172907933593\n",
      "Epoch: 689. Loss: 0.003245389787480235\n",
      "Epoch: 690. Loss: 0.003273617709055543\n",
      "Epoch: 691. Loss: 0.003308319952338934\n",
      "Epoch: 692. Loss: 0.003341226140037179\n",
      "Epoch: 693. Loss: 0.003363740397617221\n",
      "Epoch: 694. Loss: 0.003371767234057188\n",
      "Epoch: 695. Loss: 0.0033611732069402933\n",
      "Epoch: 696. Loss: 0.0033337888307869434\n",
      "Epoch: 697. Loss: 0.003301451215520501\n",
      "Epoch: 698. Loss: 0.0032749546226114035\n",
      "Epoch: 699. Loss: 0.0032565584406256676\n",
      "Epoch: 700. Loss: 0.0032473281025886536\n",
      "Epoch: 701. Loss: 0.0032459229696542025\n",
      "Epoch: 702. Loss: 0.0032536936923861504\n",
      "Epoch: 703. Loss: 0.0032675459515303373\n",
      "Epoch: 704. Loss: 0.003286530962213874\n",
      "Epoch: 705. Loss: 0.003307074774056673\n",
      "Epoch: 706. Loss: 0.0033249743282794952\n",
      "Epoch: 707. Loss: 0.003336158348247409\n",
      "Epoch: 708. Loss: 0.0033362959511578083\n",
      "Epoch: 709. Loss: 0.0033226909581571817\n",
      "Epoch: 710. Loss: 0.0032979254610836506\n",
      "Epoch: 711. Loss: 0.0032710956875234842\n",
      "Epoch: 712. Loss: 0.0032503297552466393\n",
      "Epoch: 713. Loss: 0.003235690528526902\n",
      "Epoch: 714. Loss: 0.0032282001338899136\n",
      "Epoch: 715. Loss: 0.003228492336347699\n",
      "Epoch: 716. Loss: 0.003235065145418048\n",
      "Epoch: 717. Loss: 0.0032464703544974327\n",
      "Epoch: 718. Loss: 0.003260602243244648\n",
      "Epoch: 719. Loss: 0.003277047071605921\n",
      "Epoch: 720. Loss: 0.0032913219183683395\n",
      "Epoch: 721. Loss: 0.0032968453597277403\n",
      "Epoch: 722. Loss: 0.0032950055319815874\n",
      "Epoch: 723. Loss: 0.0032913319300860167\n",
      "Epoch: 724. Loss: 0.003284191247075796\n",
      "Epoch: 725. Loss: 0.0032749539241194725\n",
      "Epoch: 726. Loss: 0.003263791324570775\n",
      "Epoch: 727. Loss: 0.0032541092950850725\n",
      "Epoch: 728. Loss: 0.003249598201364279\n",
      "Epoch: 729. Loss: 0.003248868975788355\n",
      "Epoch: 730. Loss: 0.003250863403081894\n",
      "Epoch: 731. Loss: 0.0032568341121077538\n",
      "Epoch: 732. Loss: 0.0032682474702596664\n",
      "Epoch: 733. Loss: 0.0032834415324032307\n",
      "Epoch: 734. Loss: 0.003299073548987508\n",
      "Epoch: 735. Loss: 0.0033070603385567665\n",
      "Epoch: 736. Loss: 0.003304010722786188\n",
      "Epoch: 737. Loss: 0.003290919354185462\n",
      "Epoch: 738. Loss: 0.003276524366810918\n",
      "Epoch: 739. Loss: 0.003264722879976034\n",
      "Epoch: 740. Loss: 0.0032555675134062767\n",
      "Epoch: 741. Loss: 0.0032467276323586702\n",
      "Epoch: 742. Loss: 0.0032418454065918922\n",
      "Epoch: 743. Loss: 0.0032410253770649433\n",
      "Epoch: 744. Loss: 0.0032466587144881487\n",
      "Epoch: 745. Loss: 0.003259088611230254\n",
      "Epoch: 746. Loss: 0.0032741366885602474\n",
      "Epoch: 747. Loss: 0.0032871891744434834\n",
      "Epoch: 748. Loss: 0.0032971829641610384\n",
      "Epoch: 749. Loss: 0.0033010682091116905\n",
      "Epoch: 750. Loss: 0.0032967415172606707\n",
      "Epoch: 751. Loss: 0.0032874199096113443\n",
      "Epoch: 752. Loss: 0.0032768291421234608\n",
      "Epoch: 753. Loss: 0.0032679850701242685\n",
      "Epoch: 754. Loss: 0.0032616856042295694\n",
      "Epoch: 755. Loss: 0.003257238771766424\n",
      "Epoch: 756. Loss: 0.0032555479556322098\n",
      "Epoch: 757. Loss: 0.003256075782701373\n",
      "Epoch: 758. Loss: 0.0032585239969193935\n",
      "Epoch: 759. Loss: 0.003262194339185953\n",
      "Epoch: 760. Loss: 0.0032663389574736357\n",
      "Epoch: 761. Loss: 0.003269338048994541\n",
      "Epoch: 762. Loss: 0.0032744454219937325\n",
      "Epoch: 763. Loss: 0.003281149547547102\n",
      "Epoch: 764. Loss: 0.003281434765085578\n",
      "Epoch: 765. Loss: 0.0032748167868703604\n",
      "Epoch: 766. Loss: 0.0032655897084623575\n",
      "Epoch: 767. Loss: 0.003253854112699628\n",
      "Epoch: 768. Loss: 0.003243606071919203\n",
      "Epoch: 769. Loss: 0.003237508237361908\n",
      "Epoch: 770. Loss: 0.0032360400073230267\n",
      "Epoch: 771. Loss: 0.003240680554881692\n",
      "Epoch: 772. Loss: 0.003248468041419983\n",
      "Epoch: 773. Loss: 0.003256496973335743\n",
      "Epoch: 774. Loss: 0.0032658386044204235\n",
      "Epoch: 775. Loss: 0.003278668038547039\n",
      "Epoch: 776. Loss: 0.0032918157521635294\n",
      "Epoch: 777. Loss: 0.003300845855847001\n",
      "Epoch: 778. Loss: 0.003298881696537137\n",
      "Epoch: 779. Loss: 0.003284093923866749\n",
      "Epoch: 780. Loss: 0.003266248619183898\n",
      "Epoch: 781. Loss: 0.0032535381615161896\n",
      "Epoch: 782. Loss: 0.003247171174734831\n",
      "Epoch: 783. Loss: 0.0032449166756123304\n",
      "Epoch: 784. Loss: 0.003249994246289134\n",
      "Epoch: 785. Loss: 0.0032615733798593283\n",
      "Epoch: 786. Loss: 0.003276384901255369\n",
      "Epoch: 787. Loss: 0.003293771995231509\n",
      "Epoch: 788. Loss: 0.0033058803528547287\n",
      "Epoch: 789. Loss: 0.0033067516051232815\n",
      "Epoch: 790. Loss: 0.003295422298833728\n",
      "Epoch: 791. Loss: 0.003272852161899209\n",
      "Epoch: 792. Loss: 0.0032500955276191235\n",
      "Epoch: 793. Loss: 0.0032326525542885065\n",
      "Epoch: 794. Loss: 0.0032243498135358095\n",
      "Epoch: 795. Loss: 0.0032246620394289494\n",
      "Epoch: 796. Loss: 0.0032329843379557133\n",
      "Epoch: 797. Loss: 0.003248270833864808\n",
      "Epoch: 798. Loss: 0.003268606262281537\n",
      "Epoch: 799. Loss: 0.0032890222501009703\n",
      "Epoch: 800. Loss: 0.003306201659142971\n",
      "Epoch: 801. Loss: 0.0033123958855867386\n",
      "Epoch: 802. Loss: 0.0033048600889742374\n",
      "Epoch: 803. Loss: 0.003289290703833103\n",
      "Epoch: 804. Loss: 0.0032729723025113344\n",
      "Epoch: 805. Loss: 0.0032569426111876965\n",
      "Epoch: 806. Loss: 0.0032443117816001177\n",
      "Epoch: 807. Loss: 0.0032363387290388346\n",
      "Epoch: 808. Loss: 0.003233803203329444\n",
      "Epoch: 809. Loss: 0.0032359473407268524\n",
      "Epoch: 810. Loss: 0.003249409841373563\n",
      "Epoch: 811. Loss: 0.0032701431773602962\n",
      "Epoch: 812. Loss: 0.0032930297311395407\n",
      "Epoch: 813. Loss: 0.0033078372944146395\n",
      "Epoch: 814. Loss: 0.0033093104138970375\n",
      "Epoch: 815. Loss: 0.0033003806602209806\n",
      "Epoch: 816. Loss: 0.0032853554002940655\n",
      "Epoch: 817. Loss: 0.0032648365013301373\n",
      "Epoch: 818. Loss: 0.0032445485703647137\n",
      "Epoch: 819. Loss: 0.003232896327972412\n",
      "Epoch: 820. Loss: 0.0032290276139974594\n",
      "Epoch: 821. Loss: 0.003233344294130802\n",
      "Epoch: 822. Loss: 0.0032467381097376347\n",
      "Epoch: 823. Loss: 0.0032682528253644705\n",
      "Epoch: 824. Loss: 0.0032928907312452793\n",
      "Epoch: 825. Loss: 0.0033115430269390345\n",
      "Epoch: 826. Loss: 0.003319003852084279\n",
      "Epoch: 827. Loss: 0.003312276676297188\n",
      "Epoch: 828. Loss: 0.003294332418590784\n",
      "Epoch: 829. Loss: 0.0032729434315115213\n",
      "Epoch: 830. Loss: 0.003254764247685671\n",
      "Epoch: 831. Loss: 0.0032418423797935247\n",
      "Epoch: 832. Loss: 0.0032302059698849916\n",
      "Epoch: 833. Loss: 0.003223452949896455\n",
      "Epoch: 834. Loss: 0.0032275142148137093\n",
      "Epoch: 835. Loss: 0.0032418551854789257\n",
      "Epoch: 836. Loss: 0.0032637552358210087\n",
      "Epoch: 837. Loss: 0.0032891076989471912\n",
      "Epoch: 838. Loss: 0.0033139758743345737\n",
      "Epoch: 839. Loss: 0.0033288621343672276\n",
      "Epoch: 840. Loss: 0.003323975717648864\n",
      "Epoch: 841. Loss: 0.00330146262422204\n",
      "Epoch: 842. Loss: 0.003269833978265524\n",
      "Epoch: 843. Loss: 0.00324152666144073\n",
      "Epoch: 844. Loss: 0.0032211311627179384\n",
      "Epoch: 845. Loss: 0.003212159499526024\n",
      "Epoch: 846. Loss: 0.0032118463423103094\n",
      "Epoch: 847. Loss: 0.003217431018128991\n",
      "Epoch: 848. Loss: 0.003231546375900507\n",
      "Epoch: 849. Loss: 0.0032537607476115227\n",
      "Epoch: 850. Loss: 0.0032808948308229446\n",
      "Epoch: 851. Loss: 0.003306835424154997\n",
      "Epoch: 852. Loss: 0.003320870455354452\n",
      "Epoch: 853. Loss: 0.0033196916338056326\n",
      "Epoch: 854. Loss: 0.0033056819811463356\n",
      "Epoch: 855. Loss: 0.003282526507973671\n",
      "Epoch: 856. Loss: 0.003260229015722871\n",
      "Epoch: 857. Loss: 0.0032449914142489433\n",
      "Epoch: 858. Loss: 0.0032378816977143288\n",
      "Epoch: 859. Loss: 0.0032393415458500385\n",
      "Epoch: 860. Loss: 0.0032490654848515987\n",
      "Epoch: 861. Loss: 0.0032664467580616474\n",
      "Epoch: 862. Loss: 0.0032857756596058607\n",
      "Epoch: 863. Loss: 0.0033004654105752707\n",
      "Epoch: 864. Loss: 0.0033058393746614456\n",
      "Epoch: 865. Loss: 0.003300040727481246\n",
      "Epoch: 866. Loss: 0.003289847867563367\n",
      "Epoch: 867. Loss: 0.0032781572081148624\n",
      "Epoch: 868. Loss: 0.00326855038292706\n",
      "Epoch: 869. Loss: 0.0032600772101432085\n",
      "Epoch: 870. Loss: 0.003254516748711467\n",
      "Epoch: 871. Loss: 0.003252154216170311\n",
      "Epoch: 872. Loss: 0.003251286456361413\n",
      "Epoch: 873. Loss: 0.0032508105505257845\n",
      "Epoch: 874. Loss: 0.003252705791965127\n",
      "Epoch: 875. Loss: 0.0032576224766671658\n",
      "Epoch: 876. Loss: 0.0032646844629198313\n",
      "Epoch: 877. Loss: 0.003270980203524232\n",
      "Epoch: 878. Loss: 0.003275264985859394\n",
      "Epoch: 879. Loss: 0.0032766778022050858\n",
      "Epoch: 880. Loss: 0.0032775048166513443\n",
      "Epoch: 881. Loss: 0.0032766677904874086\n",
      "Epoch: 882. Loss: 0.0032738344743847847\n",
      "Epoch: 883. Loss: 0.0032698167487978935\n",
      "Epoch: 884. Loss: 0.003264262806624174\n",
      "Epoch: 885. Loss: 0.003258688608184457\n",
      "Epoch: 886. Loss: 0.003248852677643299\n",
      "Epoch: 887. Loss: 0.0032409511040896177\n",
      "Epoch: 888. Loss: 0.0032389082480221987\n",
      "Epoch: 889. Loss: 0.0032411026768386364\n",
      "Epoch: 890. Loss: 0.0032467313576489687\n",
      "Epoch: 891. Loss: 0.0032556531950831413\n",
      "Epoch: 892. Loss: 0.0032687904313206673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893. Loss: 0.0032814175356179476\n",
      "Epoch: 894. Loss: 0.0032888296991586685\n",
      "Epoch: 895. Loss: 0.0032902287784963846\n",
      "Epoch: 896. Loss: 0.003284320468083024\n",
      "Epoch: 897. Loss: 0.00327483844012022\n",
      "Epoch: 898. Loss: 0.0032659999560564756\n",
      "Epoch: 899. Loss: 0.0032598956022411585\n",
      "Epoch: 900. Loss: 0.0032563398126512766\n",
      "Epoch: 901. Loss: 0.003254230134189129\n",
      "Epoch: 902. Loss: 0.0032525742426514626\n",
      "Epoch: 903. Loss: 0.0032532671466469765\n",
      "Epoch: 904. Loss: 0.0032557642553001642\n",
      "Epoch: 905. Loss: 0.0032605717424303293\n",
      "Epoch: 906. Loss: 0.003267495660111308\n",
      "Epoch: 907. Loss: 0.003273898968473077\n",
      "Epoch: 908. Loss: 0.003277129726484418\n",
      "Epoch: 909. Loss: 0.003273302223533392\n",
      "Epoch: 910. Loss: 0.003265602281317115\n",
      "Epoch: 911. Loss: 0.0032571901101619005\n",
      "Epoch: 912. Loss: 0.0032491248566657305\n",
      "Epoch: 913. Loss: 0.003243230516090989\n",
      "Epoch: 914. Loss: 0.003237784141674638\n",
      "Epoch: 915. Loss: 0.00323508121073246\n",
      "Epoch: 916. Loss: 0.00323523860424757\n",
      "Epoch: 917. Loss: 0.0032397888135164976\n",
      "Epoch: 918. Loss: 0.0032499025110155344\n",
      "Epoch: 919. Loss: 0.0032636444084346294\n",
      "Epoch: 920. Loss: 0.003277491545304656\n",
      "Epoch: 921. Loss: 0.0032895482145249844\n",
      "Epoch: 922. Loss: 0.003301355754956603\n",
      "Epoch: 923. Loss: 0.0033069855999201536\n",
      "Epoch: 924. Loss: 0.003302611643448472\n",
      "Epoch: 925. Loss: 0.0032874085009098053\n",
      "Epoch: 926. Loss: 0.0032647745683789253\n",
      "Epoch: 927. Loss: 0.0032412740401923656\n",
      "Epoch: 928. Loss: 0.0032242843881249428\n",
      "Epoch: 929. Loss: 0.003216099226847291\n",
      "Epoch: 930. Loss: 0.0032152344938367605\n",
      "Epoch: 931. Loss: 0.0032223002053797245\n",
      "Epoch: 932. Loss: 0.0032361019402742386\n",
      "Epoch: 933. Loss: 0.0032566646113991737\n",
      "Epoch: 934. Loss: 0.003278569085523486\n",
      "Epoch: 935. Loss: 0.003295241156592965\n",
      "Epoch: 936. Loss: 0.003302094293758273\n",
      "Epoch: 937. Loss: 0.0032939675729721785\n",
      "Epoch: 938. Loss: 0.0032771043479442596\n",
      "Epoch: 939. Loss: 0.0032568657770752907\n",
      "Epoch: 940. Loss: 0.0032382477074861526\n",
      "Epoch: 941. Loss: 0.0032257442362606525\n",
      "Epoch: 942. Loss: 0.0032217809930443764\n",
      "Epoch: 943. Loss: 0.0032266785856336355\n",
      "Epoch: 944. Loss: 0.003237167140468955\n",
      "Epoch: 945. Loss: 0.003250079695135355\n",
      "Epoch: 946. Loss: 0.003261375008150935\n",
      "Epoch: 947. Loss: 0.003270460292696953\n",
      "Epoch: 948. Loss: 0.0032752538099884987\n",
      "Epoch: 949. Loss: 0.0032739154994487762\n",
      "Epoch: 950. Loss: 0.0032669338397681713\n",
      "Epoch: 951. Loss: 0.003259366611018777\n",
      "Epoch: 952. Loss: 0.0032537784427404404\n",
      "Epoch: 953. Loss: 0.0032521975226700306\n",
      "Epoch: 954. Loss: 0.003252991707995534\n",
      "Epoch: 955. Loss: 0.0032529984600842\n",
      "Epoch: 956. Loss: 0.0032567374873906374\n",
      "Epoch: 957. Loss: 0.0032604821026325226\n",
      "Epoch: 958. Loss: 0.003261252772063017\n",
      "Epoch: 959. Loss: 0.0032609240151941776\n",
      "Epoch: 960. Loss: 0.0032614150550216436\n",
      "Epoch: 961. Loss: 0.0032618078403174877\n",
      "Epoch: 962. Loss: 0.0032640311401337385\n",
      "Epoch: 963. Loss: 0.003266776679083705\n",
      "Epoch: 964. Loss: 0.0032712516840547323\n",
      "Epoch: 965. Loss: 0.003272878471761942\n",
      "Epoch: 966. Loss: 0.003267891239374876\n",
      "Epoch: 967. Loss: 0.003261841833591461\n",
      "Epoch: 968. Loss: 0.0032532259356230497\n",
      "Epoch: 969. Loss: 0.003243616782128811\n",
      "Epoch: 970. Loss: 0.0032387827523052692\n",
      "Epoch: 971. Loss: 0.003240979975089431\n",
      "Epoch: 972. Loss: 0.0032481257803738117\n",
      "Epoch: 973. Loss: 0.003259882563725114\n",
      "Epoch: 974. Loss: 0.0032716465648263693\n",
      "Epoch: 975. Loss: 0.0032786030787974596\n",
      "Epoch: 976. Loss: 0.003280398203060031\n",
      "Epoch: 977. Loss: 0.003278373973444104\n",
      "Epoch: 978. Loss: 0.0032697718124836683\n",
      "Epoch: 979. Loss: 0.0032570178154855967\n",
      "Epoch: 980. Loss: 0.0032436444889754057\n",
      "Epoch: 981. Loss: 0.0032340518664568663\n",
      "Epoch: 982. Loss: 0.0032259745057672262\n",
      "Epoch: 983. Loss: 0.00321920495480299\n",
      "Epoch: 984. Loss: 0.0032178338151425123\n",
      "Epoch: 985. Loss: 0.003225791035220027\n",
      "Epoch: 986. Loss: 0.0032419252675026655\n",
      "Epoch: 987. Loss: 0.0032606010790914297\n",
      "Epoch: 988. Loss: 0.0032779420726001263\n",
      "Epoch: 989. Loss: 0.003286871360614896\n",
      "Epoch: 990. Loss: 0.003281788667663932\n",
      "Epoch: 991. Loss: 0.003263381775468588\n",
      "Epoch: 992. Loss: 0.0032445096876472235\n",
      "Epoch: 993. Loss: 0.0032328853849321604\n",
      "Epoch: 994. Loss: 0.0032310010865330696\n",
      "Epoch: 995. Loss: 0.0032343105413019657\n",
      "Epoch: 996. Loss: 0.0032399948686361313\n",
      "Epoch: 997. Loss: 0.003248337423428893\n",
      "Epoch: 998. Loss: 0.003258854616433382\n",
      "Epoch: 999. Loss: 0.003266106592491269\n"
     ]
    }
   ],
   "source": [
    "loss_cnn_test = []\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 120\n",
    "cnn_logits_lst_test = []\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    #for i in tqdm(range(0, len(X_train_CNN), BATCH_SIZE)):\n",
    "    batch_X = X_test_CNN.view(-1, 1, 30,30)\n",
    "    batch_y = Y_test_CNN\n",
    "\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net(batch_X.float())[0]\n",
    "    cnn_logits_test = net(batch_X.float())[1]\n",
    "    #cnn_logits_lst.extend(cnn_logits)\n",
    "    #print(outputs)\n",
    "    loss = loss_function(outputs,  batch_y.float().reshape((-1,1)))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_cnn_test.append(loss)\n",
    "    if EPOCHS % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "id": "eb202216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 1093,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAai0lEQVR4nO3dfXBd9X3n8fdHV7p6srElW+bBNpYTDKkLIYBwSNllaBNak2xxt0m2dvchabP17mxoaJLZLEx3SMpOp5uSTTbdeDLxJuw2nU1cymayKnHrJEB2m2zCWhRiMMYgjME2GMnP2LKtp+/+ca/MtZCsK+lKR+fcz2tGo3vO+d17vkfH/ujodx5+igjMzCz9apIuwMzMKsOBbmaWEQ50M7OMcKCbmWWEA93MLCNqk1rx4sWLo729PanVm5ml0hNPPHEoItrGWpZYoLe3t9PV1ZXU6s3MUknSy+Mtc5eLmVlGONDNzDKirECXtFbSbkndku4eY/mXJD1V/Hpe0rGKV2pmZhc0YR+6pBywCbgN2A9sl9QZEc+OtImIT5a0/33guhmo1czMLqCcI/Q1QHdE7ImIfmALsO4C7TcA365EcWZmVr5yAn0psK9ken9x3ltIWgGsBB4dZ/lGSV2Sunp7eydbq5mZXUClT4quBx6KiKGxFkbE5ojoiIiOtrYxL6M0M7MpKifQDwDLS6aXFeeNZT0z3N2yfe8R/tP3dzMwNDyTqzEzS51yAn07sErSSkl5CqHdObqRpHcALcBPK1vi+f7+5aP8l0e7HehmZqNMGOgRMQjcCWwDdgEPRsROSfdJuqOk6XpgS8zwiBm5GgEwOOyBOczMSpV1639EbAW2jpp376jpz1WurPGNBPrQkAPdzKxU6u4UrR0JdA+dZ2Z2ntQFes1IoLvLxczsPKkL9FoHupnZmFIX6DVyoJuZjSV1gV6b81UuZmZjSV2g52oKJfsI3czsfOkLdHe5mJmNKX2B7pOiZmZjcqCbmWVE6gK99tyt/36Wi5lZqdQF+sgR+rDvFDUzO09qA/17Ow4mXImZ2dyS2kB/4CcvJVyJmdncktpANzOz8znQzcwyInWBPvIsF4BnDhxPsBIzs7kldYFeOvSc+9HNzN6UukA/O/BmoB/rG0iwEjOzuSV1gd4/NHTu9aPP9fDa8dMJVmNmNneUFeiS1kraLalb0t3jtPknkp6VtFPStypb5ptuvmLxedP3/+3umVqVmVmqTBjoknLAJuB2YDWwQdLqUW1WAfcAN0fELwJ/UPlSC+prc7zwx7efmz5xxt0uZmZQ3hH6GqA7IvZERD+wBVg3qs3vAZsi4ihARPRUtszz1eVq+MKHrwXgh7t62PRY90yuzswsFcoJ9KXAvpLp/cV5pa4ErpT0E0k/k7S2UgWO50M3LON9v7AEgPu37ebMwNAE7zAzy7ZKnRStBVYBtwIbgP8qaeHoRpI2SuqS1NXb2zvtld7/oWtpyucA+OffeHzan2dmlmblBPoBYHnJ9LLivFL7gc6IGIiIl4DnKQT8eSJic0R0RERHW1vbVGs+p6U5z4P/6j0AbN97lM6fvzrtzzQzS6tyAn07sErSSkl5YD3QOarNdykcnSNpMYUumD2VK3N8Vy9dwJ//7hoAPvHtJ+nrH5yN1ZqZzTkTBnpEDAJ3AtuAXcCDEbFT0n2S7ig22wYclvQs8BjwbyPi8EwVPdotqxbzuV8vXHjzgT/78Wyt1sxsTlEkNFBER0dHdHV1VezzhoaDd/3R93nj7CDf+Te/xPWXt1Tss83M5gpJT0REx1jLUnen6HhyNWLrXf8QgM88tIOkflGZmSUlM4EOsLy1ic/++mq6e07yo93Tv4rGzCxNMhXoAL95/TIuW9DAF3/wfNKlmJnNqswF+oLGOn7n5pU8feA4T75yNOlyzMxmTeYCHeCDNyxjQWMd3/ixn5duZtUjk4He2pzn/ddcyqPP9XC6348EMLPqkMlAB3j/NZfQ1z/Ej7sPJV2KmdmsyGyg3/S2RcxvqOX7Ow8mXYqZ2azIbKDX5Wp47zuW8MNdrzM87GvSzSz7MhvoALdc2cbRvgGe73kj6VLMzGZcpgP9xvZWoPAkRjOzrMt0oC9raWTJ/Hqe2Hvk3Lzbvvi/+cqjLyRYlZnZzMh0oEvixvbW847QX+g5yRe+77tIzSx7Mh3oADesaOHAsdO8dvx00qWYmc2ozAd6R3vhMbpd7kc3s4zLfKCvvvQimvI5nnjZgW5m2Zb5QK/N1fCu5QvZvveIn5FuZpmW+UAH6FjRwq7XTnDitMcbNbPsqopAv2bZQoYDnjt4IulSzMxmTFUE+srFTQB0955MuBIzs5lTVqBLWitpt6RuSXePsfyjknolPVX8+peVL3XqlrU0IcGLPaeSLsXMbMbUTtRAUg7YBNwG7Ae2S+qMiGdHNf3LiLhzBmqctoa6HJde1MCLPkI3swwr5wh9DdAdEXsioh/YAqyb2bIqb8WiZrp7HOhmll3lBPpSYF/J9P7ivNE+KGmHpIckLa9IdRXUvriJA8d8t6iZZVelTor+NdAeEe8EfgD8+ViNJG2U1CWpq7e3t0KrLs+KRc2zuj4zs9lWTqAfAEqPuJcV550TEYcj4mxx8uvADWN9UERsjoiOiOhoa2ubSr1T1r6oaVbXZ2Y228oJ9O3AKkkrJeWB9UBnaQNJl5ZM3gHsqlyJleEjdDPLugmvcomIQUl3AtuAHPBAROyUdB/QFRGdwCck3QEMAkeAj85gzVOywkfoZpZxEwY6QERsBbaOmndvyet7gHsqW1plNeXL2lQzs9SqijtFR9TllHQJZmYzpqoCvb42d+716f6hBCsxM6u8qgr0r/z2dede//XPX02wEjOzyquqQL/1qiXnXh8/PZBgJWZmlVdVgQ7w6duuBODQqbMTtDQzS5eqC/Tff+8qlsyv54SP0M0sY6ou0AEWNNZxrM+BbmbZUpWBvrCpzn3oZpY5VRroeQ6f7E+6DDOziqrKQF+6sJFX/ShdM8uYqg30N84OutvFzDKlOgO9pRGA/Uf7Eq7EzKxyqjLQlxUD/cBRd7uYWXZUZaBftrAQ6O5HN7MsqcpAb2nKUyM45CtdzCxDqjLQczViYVOePYdOJl2KmVnFVGWgAxw51c/Wpw8yODScdClmZhVRtYH+gWsKw6Ce8nPRzSwjqjbQb75iMeCBLswsO6o20JvyhdGLTvUPJlyJmVlllBXoktZK2i2pW9LdF2j3QUkhqaNyJc6MkUDf+M2uhCsxM6uMCQNdUg7YBNwOrAY2SFo9Rrv5wF3A45UuciY0FgP9xd5TCVdiZlYZ5RyhrwG6I2JPRPQDW4B1Y7T7D8DngTMVrG/GnB3w1S1mli3lBPpSYF/J9P7ivHMkXQ8sj4jvXeiDJG2U1CWpq7e3d9LFVtKKRU2Jrt/MrNJqp/sBkmqALwIfnahtRGwGNgN0dHTEdNc9Hasuns+1yxZQU6MkyzAzq5hyjtAPAMtLppcV542YD1wN/EjSXuAmoDMNJ0bztTU8+coxP0bXzDKhnEDfDqyStFJSHlgPdI4sjIjjEbE4Itojoh34GXBHRMz5y0e27z0KwHefPDBBSzOzuW/CQI+IQeBOYBuwC3gwInZKuk/SHTNd4GxY2FSXdAlmZtNWVh96RGwFto6ad+84bW+dflmzo31RE3sP91Ej96ObWfpV7Z2iAA989EYATg/49n8zS7+qDvSLGgtdLY8915NwJWZm01fVgd5YV7hb9G+eOZhwJWZm0+dANzPLiKoOdN9UZGZZUtWBDnDLlW1Jl2BmVhFVH+g3rmgBYMBD0ZlZylV9oI88RvfQybMJV2JmNj1VH+gjz3H5zEM7Eq7EzGx6qj7QB4YKD3187XgqHuNuZjauqg/0j//y2wG4ZZVPjppZulV9oM9vqGPxvLxv/zez1Kv6QAdoytdyun8w6TLMzKbFgQ5I8Kif52JmKedAB14+3MeJM4OcOOORi8wsvRzoJU54KDozSzEHOrDpt68H4ORZ96ObWXo50IHm+sLdoqcc6GaWYg50YF59YSS+k2d96aKZpZcDHWguBrqP0M0szcoKdElrJe2W1C3p7jGW/2tJT0t6StKPJa2ufKkzp7U5D8DrJ3z7v5ml14SBLikHbAJuB1YDG8YI7G9FxDUR8S7gT4EvVrrQmbRkfj2XXNTAU/uOJV2KmdmUlXOEvgbojog9EdEPbAHWlTaIiBMlk81AVK7EmSeJZS2N9JzwI3TNLL1qy2izFNhXMr0fePfoRpI+DnwKyAO/MtYHSdoIbAS4/PLLJ1vrjFrYlGf/0b6kyzAzm7KKnRSNiE0R8Xbg3wH/fpw2myOiIyI62trm1tMNW5vrONrXn3QZZmZTVk6gHwCWl0wvK84bzxbgN6ZRUyJamvMc7RsgIlW9RWZm55QT6NuBVZJWSsoD64HO0gaSVpVMfgB4oXIlzo6Wpjz9g8P09ftadDNLpwn70CNiUNKdwDYgBzwQETsl3Qd0RUQncKek9wEDwFHgIzNZ9ExobSpcunjkVP+569LNzNKkrOSKiK3A1lHz7i15fVeF65p1C5vqADjWN8Dy1oSLMTObAt8pWrR4fj0AL/aeTLgSM7OpcaAXvXPpAhY01vH4S4eTLsXMbEoc6EW1uRoWNed544yf52Jm6eRALzGvodYP6DKz1HKgl2jO13qQCzNLLQd6iXkNte5yMbPUcqCXmF/vQDez9HKgl1hyUQM9b5xhYGg46VLMzCbNgV7iyovnMTAUvHz4VNKlmJlNmgO9xJUXzwfg+dd9c5GZpY8DvcQVS+YB8GKPA93M0seBXqKhLkdzPsfx0wNJl2JmNmkO9FHmNfhadDNLJwf6KPPqa3nDgW5mKeRAH2VeQx0nfS26maWQA32U+fXucjGzdHKgj7KgqY4jpzxYtJmljwN9lEsvauDg8TMeLNrMUseBPsolCxo4PTDEsT5fumhm6eJAH+Xa5QsB+LvuQ8kWYmY2SWUFuqS1knZL6pZ09xjLPyXpWUk7JD0iaUXlS50d1yxdAMC+I30JV2JmNjkTBrqkHLAJuB1YDWyQtHpUsyeBjoh4J/AQ8KeVLnS2NNTlaMrnfGLUzFKnnCP0NUB3ROyJiH5gC7CutEFEPBYRI4e0PwOWVbbM2dXSlOeoA93MUqacQF8K7CuZ3l+cN56PAX8z1gJJGyV1Serq7e0tv8pZ1tqc50ifA93M0qWiJ0Ul/TOgA7h/rOURsTkiOiKio62trZKrrqjWZh+hm1n6lBPoB4DlJdPLivPOI+l9wB8Cd0TE2cqUl4zW5jyHHehmljLlBPp2YJWklZLywHqgs7SBpOuAr1EI857Klzm73IduZmk0YaBHxCBwJ7AN2AU8GBE7Jd0n6Y5is/uBecBfSXpKUuc4H5cKrc11nOof4szAUNKlmJmVrbacRhGxFdg6at69Ja/fV+G6EtXSnAfgWN8AlyzIJVyNmVl5fKfoGBYVA/3wqVSfCjCzKuNAH0NLUyHQj57y81zMLD0c6GO4ZEEDAHsPn0q4EjOz8jnQx3B5axOXLWjgJ35Al5mliAN9DJK4+YrF/HTP4aRLMTMrmwN9HFdePJ9jfQMc93PRzSwlHOjjWN7aCMC+o36MrpmlgwN9HBdfVDgx2vPGmYQrMTMrjwN9HIua6wE4dNKPADCzdHCgj6N1XuFadA90YWZp4UAfR3M+R762xoFuZqnhQB+HJBY35znsLhczSwkH+gW0zstzxM9zMbOUcKBfQGtzvbtczCw1HOgXsMgjF5lZijjQL6C1Oe8jdDNLDQf6BbQ25+nrH+J0v0cuMrO5z4F+AR7owszSxIF+Ae2LmwF4/vU3Eq7EzGxiDvQLuHbZQupyYvveo0mXYmY2obICXdJaSbsldUu6e4zlt0j6e0mDkj5U+TKT0ZjPcfXSBWx/6UjSpZiZTWjCQJeUAzYBtwOrgQ2SVo9q9grwUeBblS4waWvaW9mx/zhnB31i1MzmtnKO0NcA3RGxJyL6gS3AutIGEbE3InYAwzNQY6JWXTyf/qFhDh73Y3TNbG4rJ9CXAvtKpvcX502apI2SuiR19fb2TuUjZt2S+YXH6Pa84StdzGxum9WTohGxOSI6IqKjra1tNlc9ZUsuKgb6CQe6mc1t5QT6AWB5yfSy4ryqsGS+Ry4ys3QoJ9C3A6skrZSUB9YDnTNb1tzR0lRHXU7ucjGzOW/CQI+IQeBOYBuwC3gwInZKuk/SHQCSbpS0H/gw8DVJO2ey6NkkiSXzG3j9hI/QzWxuqy2nUURsBbaOmndvyevtFLpiMmnFoibfLWpmc57vFC3Du1cu4tlXT3Dg2OmkSzEzG5cDvQwfeOclDAf83+5DSZdiZjYuB3oZVi6eR2NdjmdfO5F0KWZm43KglyFXI9oXN/PK4b6kSzEzG5cDvUzLWxp5+YgD3czmLgd6ma5YMo+9h05xZsAP6TKzucmBXqYbV7YyOBw8+lxP0qWYmY3JgV6mW1a1sWR+PVuffi3pUszMxuRAL1OuRtx8xWJ++uJhhocj6XLMzN7CgT4Jv/T2RRw+1c/zPb5r1MzmHgf6JNx8xWIAHtnlfnQzm3sc6JNw2cJGbmxv4eEd7kc3s7nHgT5Jt161hF2vnfDDusxsznGgT9KGNZczr76Wr/7oxaRLMTM7jwN9klqb8/zj65by8I5X2X3QR+lmNnc40Kfgk7ddSVO+ls917mTIlzCa2RzhQJ+C1uY8d9/+Dn665zBf+P7upMsxMwPKHLHI3mr9jct58pWjfPVHLzI0HHzm166iNuffj2aWHAf6FEniT37zndTX5tj8f/bwdy8c4lO3XckvX9XmYDezRDjQpyFXI+5b94u85+2L+OPv7eL3vtnFkvn13HpVGx0rWvmFSy/i8kVNLGisS7pUM6sCipj4pJ6ktcCXgRzw9Yj4j6OW1wPfBG4ADgO/FRF7L/SZHR0d0dXVNcWy556BoWEe2dVD588P8OMXDnHizOC5ZQsa61g0L09rU56W5jwtTXXMq6+jMV9DY12OhrocTflaGvM11NfmyNWIupyoramhtuR73blpUZurobZG5GpEjUSNCn81jPW9RkKjvpe+x8zSQ9ITEdEx1rIJj9Al5YBNwG3AfmC7pM6IeLak2ceAoxFxhaT1wOeB35p+6elRl6th7dWXsPbqSxgeDvYePsXzr5/klSOn2HfkNEdO9XO0r599R/rYsb+fvrND9A0MzYmrZN4M+ULAizfDH0Dn2um8aUqWn1s2wXs06s067zNGXl/4s0rrZpz3jfeepJRz4DTjNSRdADAHfgzEHPhJfPq2q/iN65ZW/HPL6XJZA3RHxB4ASVuAdUBpoK8DPld8/RDwFUmKufCvOAE1NeJtbfN4W9u8CdsODA1zemCIM/1Dhe8DwwwMDTM0HAwODzMwFAwOBQPDwwwNlcwbHmZwKBgaLvzzHI5gOArBMXxuXnH63LJCuyhOj8xn1PTIe+DN/4AjO/LN6fOXF15HWW1HL4d4c1mZ7zn3n/Ity2OMtiW/hBI0F363zIES5sQv2aQrWDK/fkY+t5xAXwrsK5neD7x7vDYRMSjpOLAIOFTaSNJGYCPA5ZdfPsWSs6UuV0NdroaLGtzPbmbTM6uXY0TE5ojoiIiOtra22Vy1mVnmlRPoB4DlJdPLivPGbCOpFlhA4eSomZnNknICfTuwStJKSXlgPdA5qk0n8JHi6w8Bj1Zr/7mZWVIm7EMv9onfCWyjcNniAxGxU9J9QFdEdALfAP5CUjdwhELom5nZLCrrxqKI2ApsHTXv3pLXZ4APV7Y0MzObDN+jbmaWEQ50M7OMcKCbmWVEWc9ymZEVS73Ay1N8+2JG3bRUBbzN1cHbXB2ms80rImLMG3kSC/TpkNQ13sNpssrbXB28zdVhprbZXS5mZhnhQDczy4i0BvrmpAtIgLe5Onibq8OMbHMq+9DNzOyt0nqEbmZmozjQzcwyInWBLmmtpN2SuiXdnXQ9lSJpuaTHJD0raaeku4rzWyX9QNILxe8txfmS9GfFn8MOSdcnuwVTIykn6UlJDxenV0p6vLhdf1l8wieS6ovT3cXl7YkWPkWSFkp6SNJzknZJek8V7ONPFv9NPyPp25IasrifJT0gqUfSMyXzJr1vJX2k2P4FSR8Za13jSVWgl4xvejuwGtggaXWyVVXMIPDpiFgN3AR8vLhtdwOPRMQq4JHiNBR+BquKXxuBr85+yRVxF7CrZPrzwJci4grgKIXxaqFk3FrgS8V2afRl4G8j4h3AtRS2PbP7WNJS4BNAR0RcTeGJrSPjDmdtP/93YO2oeZPat5Jagc9SGBVuDfDZkV8CZYniGJNp+ALeA2wrmb4HuCfpumZoW/8XhYG5dwOXFuddCuwuvv4asKGk/bl2afmiMFjKI8CvAA9TGOrxEFA7en9TeHzze4qva4vtlPQ2THJ7FwAvja474/t4ZHjK1uJ+exj4tazuZ6AdeGaq+xbYAHytZP557Sb6StUROmOPb1r5obMTVvwz8zrgceDiiHituOggcHHxdRZ+Fv8Z+AwwXJxeBByLiMHidOk2nTduLTAybm2arAR6gf9W7Gb6uqRmMryPI+IA8AXgFeA1CvvtCbK9n0tNdt9Oa5+nLdAzT9I84H8CfxARJ0qXReFXdiauM5X0j4CeiHgi6VpmUS1wPfDViLgOOMWbf4ID2drHAMXugnUUfpldBjTz1m6JqjAb+zZtgV7O+KapJamOQpj/j4j4TnH265IuLS6/FOgpzk/7z+Jm4A5Je4EtFLpdvgwsLI5LC+dvUxbGrd0P7I+Ix4vTD1EI+KzuY4D3AS9FRG9EDADfobDvs7yfS012305rn6ct0MsZ3zSVJInCUH67IuKLJYtKx2v9CIW+9ZH5/6J4tvwm4HjJn3ZzXkTcExHLIqKdwn58NCL+KfAYhXFp4a3bm+pxayPiILBP0lXFWe8FniWj+7joFeAmSU3Ff+Mj25zZ/TzKZPftNuBXJbUU/7r51eK88iR9EmEKJx3eDzwPvAj8YdL1VHC7/gGFP8d2AE8Vv95Pof/wEeAF4IdAa7G9KFzx8yLwNIWrCBLfjilu+63Aw8XXbwP+H9AN/BVQX5zfUJzuLi5/W9J1T3Fb3wV0Fffzd4GWrO9j4I+A54BngL8A6rO4n4FvUzhPMEDhr7GPTWXfAr9b3P5u4HcmU4Nv/Tczy4i0dbmYmdk4HOhmZhnhQDczywgHuplZRjjQzcwywoFuZpYRDnQzs4z4/+BjP1izVpTjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_cnn_test)\n",
    "len(cnn_logits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "id": "2e1550d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['CNN_logits'] =cnn_logits_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "18947324",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_withlogits_prod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759641f",
   "metadata": {},
   "source": [
    "# Classifiers on final train/test set: logistic reg, SVM(rbf), Random Forest etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "id": "fddceabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "id": "ba13abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_withlogits_prod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "id": "35472147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_withlogits_prod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "369f7c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>ओ</td>\n",
       "      <td>بیتاب</td>\n",
       "      <td>o</td>\n",
       "      <td>bjtɒb</td>\n",
       "      <td>O</td>\n",
       "      <td>Impatient</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420495</td>\n",
       "      <td>0.697084</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-16.218758</td>\n",
       "      <td>-6.709172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>624</td>\n",
       "      <td>दुख़्तर</td>\n",
       "      <td>دختر</td>\n",
       "      <td>duxtər</td>\n",
       "      <td>dxtr</td>\n",
       "      <td>pain</td>\n",
       "      <td>Girl</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452070</td>\n",
       "      <td>0.516028</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.018238</td>\n",
       "      <td>7.578733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1079</td>\n",
       "      <td>रूह</td>\n",
       "      <td>روح</td>\n",
       "      <td>ruːɦ</td>\n",
       "      <td>rvh</td>\n",
       "      <td>spirit</td>\n",
       "      <td>Soul</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506079</td>\n",
       "      <td>0.719540</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>8.149307</td>\n",
       "      <td>13.127642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>976</td>\n",
       "      <td>मुअल्लिमा</td>\n",
       "      <td>معلمه</td>\n",
       "      <td>muallimaː</td>\n",
       "      <td>mʔlmh</td>\n",
       "      <td>Muallima</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>3.527778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436408</td>\n",
       "      <td>0.725925</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>7.360963</td>\n",
       "      <td>11.419708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4646</td>\n",
       "      <td>मलिका</td>\n",
       "      <td>ماجراجو</td>\n",
       "      <td>məlikaː</td>\n",
       "      <td>mɒd͡ʒrɒd͡ʒv</td>\n",
       "      <td>malika</td>\n",
       "      <td>Adventurer</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>1.864583</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697087</td>\n",
       "      <td>0.756841</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-16.619883</td>\n",
       "      <td>-9.829449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>4191</td>\n",
       "      <td>260</td>\n",
       "      <td>ख़ालिक़</td>\n",
       "      <td>جاری</td>\n",
       "      <td>xaːliq</td>\n",
       "      <td>d͡ʒɒrj</td>\n",
       "      <td>pure</td>\n",
       "      <td>Current</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333789</td>\n",
       "      <td>0.746394</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>-7.594423</td>\n",
       "      <td>-20.254333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>4192</td>\n",
       "      <td>2208</td>\n",
       "      <td>पकड़ना</td>\n",
       "      <td>فهم</td>\n",
       "      <td>pəkɽənaː</td>\n",
       "      <td>fhm</td>\n",
       "      <td>Catch</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>4.140625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493462</td>\n",
       "      <td>0.387989</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...</td>\n",
       "      <td>-7.256489</td>\n",
       "      <td>-12.875332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>4193</td>\n",
       "      <td>4169</td>\n",
       "      <td>प्रशंसा</td>\n",
       "      <td>ستایش</td>\n",
       "      <td>prəʃənsaː</td>\n",
       "      <td>stɒjʃ</td>\n",
       "      <td>Praise</td>\n",
       "      <td>Praise</td>\n",
       "      <td>synonym</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421296</td>\n",
       "      <td>3.638889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537812</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>-20.811543</td>\n",
       "      <td>-18.016346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>4194</td>\n",
       "      <td>1263</td>\n",
       "      <td>सिलसिला</td>\n",
       "      <td>سلسله</td>\n",
       "      <td>silsilaː</td>\n",
       "      <td>slslh</td>\n",
       "      <td>continuation</td>\n",
       "      <td>Series</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286458</td>\n",
       "      <td>2.515625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.469336</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>9.651113</td>\n",
       "      <td>10.320404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>4195</td>\n",
       "      <td>323</td>\n",
       "      <td>गिरह</td>\n",
       "      <td>نیست و نابود</td>\n",
       "      <td>ɡirəɦ</td>\n",
       "      <td>njst v nɒbvd</td>\n",
       "      <td>gang</td>\n",
       "      <td>Not and destroyed</td>\n",
       "      <td>random</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496528</td>\n",
       "      <td>4.177083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329257</td>\n",
       "      <td>0.111304</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>-28.924120</td>\n",
       "      <td>-9.297581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4194 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  loan_word original_word loan_word_epitran  \\\n",
       "0              0           136          ओ         بیتاب                 o   \n",
       "1              1           624    दुख़्तर          دختر            duxtər   \n",
       "2              2          1079        रूह           روح              ruːɦ   \n",
       "3              3           976  मुअल्लिमा         معلمه         muallimaː   \n",
       "4              4          4646      मलिका      ماجراجو            məlikaː   \n",
       "...          ...           ...        ...           ...               ...   \n",
       "4189        4191           260    ख़ालिक़          جاری            xaːliq   \n",
       "4190        4192          2208     पकड़ना           فهم          pəkɽənaː   \n",
       "4191        4193          4169    प्रशंसा         ستایش         prəʃənsaː   \n",
       "4192        4194          1263    सिलसिला         سلسله          silsilaː   \n",
       "4193        4195           323       गिरह  نیست و نابود             ɡirəɦ   \n",
       "\n",
       "     original_word_epitran  loan_english   original_english          label  \\\n",
       "0                    bjtɒb             O          Impatient         random   \n",
       "1                     dxtr          pain               Girl           loan   \n",
       "2                      rvh        spirit               Soul           loan   \n",
       "3                    mʔlmh      Muallima            Teacher           loan   \n",
       "4             mɒd͡ʒrɒd͡ʒv         malika         Adventurer  hard_negative   \n",
       "...                    ...           ...                ...            ...   \n",
       "4189                d͡ʒɒrj          pure            Current         random   \n",
       "4190                   fhm         Catch      Understanding        synonym   \n",
       "4191                 stɒjʃ        Praise             Praise        synonym   \n",
       "4192                 slslh  continuation             Series           loan   \n",
       "4193          njst v nɒbvd          gang  Not and destroyed         random   \n",
       "\n",
       "      Fast Levenshtein  ...  Hamming Feature Distance  \\\n",
       "0             1.000000  ...                  0.808333   \n",
       "1             0.333333  ...                  0.333333   \n",
       "2             0.750000  ...                  0.114583   \n",
       "3             0.666667  ...                  0.398148   \n",
       "4             0.916667  ...                  0.177083   \n",
       "...                ...  ...                       ...   \n",
       "4189          1.000000  ...                  0.291667   \n",
       "4190          1.000000  ...                  0.546875   \n",
       "4191          1.000000  ...                  0.421296   \n",
       "4192          0.500000  ...                  0.286458   \n",
       "4193          1.000000  ...                  0.496528   \n",
       "\n",
       "      Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  \\\n",
       "0                      5.900000                              1.000000   \n",
       "1                      2.416667                              0.333333   \n",
       "2                      2.187500                              0.750000   \n",
       "3                      3.527778                              0.666667   \n",
       "4                      1.864583                              0.916667   \n",
       "...                         ...                                   ...   \n",
       "4189                   2.666667                              1.000000   \n",
       "4190                   4.140625                              1.000000   \n",
       "4191                   3.638889                              1.000000   \n",
       "4192                   2.515625                              0.500000   \n",
       "4193                   4.177083                              1.000000   \n",
       "\n",
       "      label_bin  mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0             0              0.420495            0.697084   \n",
       "1             1              0.452070            0.516028   \n",
       "2             1              0.506079            0.719540   \n",
       "3             1              0.436408            0.725925   \n",
       "4             0              0.697087            0.756841   \n",
       "...         ...                   ...                 ...   \n",
       "4189          0              0.333789            0.746394   \n",
       "4190          0              0.493462            0.387989   \n",
       "4191          0              0.537812            0.747160   \n",
       "4192          1              0.469336            0.535746   \n",
       "4193          0              0.329257            0.111304   \n",
       "\n",
       "                                          features_loan  \\\n",
       "0     [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "1     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "2     [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "3     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "4     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "...                                                 ...   \n",
       "4189  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "4190  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4191  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "4192  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "4193  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, -1, ...   \n",
       "\n",
       "                                          features_orig DNN_logits CNN_logits  \n",
       "0     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -... -16.218758  -6.709172  \n",
       "1     [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   8.018238   7.578733  \n",
       "2     [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   8.149307  13.127642  \n",
       "3     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   7.360963  11.419708  \n",
       "4     [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,... -16.619883  -9.829449  \n",
       "...                                                 ...        ...        ...  \n",
       "4189  [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...  -7.594423 -20.254333  \n",
       "4190  [-1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -...  -7.256489 -12.875332  \n",
       "4191  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1... -20.811543 -18.016346  \n",
       "4192  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   9.651113  10.320404  \n",
       "4193  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ... -28.924120  -9.297581  \n",
       "\n",
       "[4194 rows x 22 columns]"
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7820d",
   "metadata": {},
   "source": [
    "# Try both a miniature train-like test set and a class-balanced test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "091032ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]\n",
    "\n",
    "# features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "#        'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "#        'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen', \n",
    "#         ]\n",
    "\n",
    "labels = ['label_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "9b19d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = test['label_bin']==0\n",
    "testneg = test.loc[idx][:140] \n",
    "testpos = test.loc[test['label_bin']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "id": "d283b60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 22)"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testneg.shape, testpos.shape\n",
    "test_balanced = pd.concat([testpos, testneg])\n",
    "test_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae3623",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "ac734dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()\n",
    "\n",
    "# x_test = test_balanced[features].values\n",
    "# y_test = test_balanced[labels].values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "id": "8cadae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 10), (4194,), (466, 10), (466,))"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "id": "16542723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 1119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "id": "f454c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,   0.8       ,   0.72916667, ...,   0.69708389,\n",
       "         -6.7091722 , -16.218758  ],\n",
       "       [  0.33333333,   0.33333333,   0.29861111, ...,   0.51602769,\n",
       "          7.5787334 ,   8.018238  ],\n",
       "       [  0.75      ,   0.25      ,   0.09895833, ...,   0.71954006,\n",
       "         13.127642  ,   8.149307  ],\n",
       "       ...,\n",
       "       [  1.        ,   0.55555556,   0.375     , ...,   0.7471602 ,\n",
       "        -18.016346  , -20.811543  ],\n",
       "       [  0.5       ,   0.375     ,   0.25520833, ...,   0.53574562,\n",
       "         10.320404  ,   9.6511135 ],\n",
       "       [  1.        ,   0.75      ,   0.44618056, ...,   0.11130369,\n",
       "         -9.297581  , -28.92412   ]])"
      ]
     },
     "execution_count": 1116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53473df",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with logits plus edit dist and cosine sims  as features: 10 features total: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "id": "2a6ef842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "b20bf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try after standardizing the data including the logits. \n",
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "id": "a942bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "id": "620d6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "id": "cc46d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9965156794425087\n",
      "precision :  0.9930555555555556\n",
      "recall :  1.0\n",
      "accuracy :  0.9978540772532188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       323\n",
      "           1       0.99      1.00      1.00       143\n",
      "\n",
      "    accuracy                           1.00       466\n",
      "   macro avg       1.00      1.00      1.00       466\n",
      "weighted avg       1.00      1.00      1.00       466\n",
      "\n",
      "[[322   1]\n",
      " [  0 143]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "2ea289b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "ab829b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa587ea",
   "metadata": {},
   "source": [
    "# for train-like test set, recall is very high with a slightly lower precision, 8 FP and no FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "9ad09735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>305</td>\n",
       "      <td>1443</td>\n",
       "      <td>ख़्वाजा</td>\n",
       "      <td>خواجه</td>\n",
       "      <td>xvaːd͡ʒaː</td>\n",
       "      <td>xvɒd͡ʒh</td>\n",
       "      <td>Khwaja</td>\n",
       "      <td>خواجه</td>\n",
       "      <td>hard_negative</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.513219</td>\n",
       "      <td>0.78262</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.64623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1 loan_word original_word loan_word_epitran  \\\n",
       "305         305          1443   ख़्वाजा        خواجه          xvaːd͡ʒaː   \n",
       "\n",
       "    original_word_epitran loan_english original_english          label  \\\n",
       "305              xvɒd͡ʒh        Khwaja            خواجه  hard_negative   \n",
       "\n",
       "     Fast Levenshtein  ...  Hamming Feature Distance  \\\n",
       "305          0.444444  ...                  0.046296   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  \\\n",
       "305                   0.791667                              0.444444   \n",
       "\n",
       "     label_bin  mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "305          0              0.513219             0.78262   \n",
       "\n",
       "                                         features_loan  \\\n",
       "305  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "\n",
       "                                         features_orig DNN_logits CNN_logits  \n",
       "305  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...  -0.000329   -0.64623  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "50971cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Unnamed: 0.1, Unnamed: 0.1.1, loan_word, original_word, loan_word_epitran, original_word_epitran, Fast Levenshtein, Dolgo Prime Distance, Feature Edit Distance, Hamming Feature Distance, Weighted Feature Distance, Fast Levenshtein Distance Div Maxlen, label, mbert_cos_similarity, xlm_cos_similarity, features_loan, features_orig, DNN_logits, CNN_logits]\n",
       "Index: []"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "56edb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try after standardizing the data including the logits. \n",
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292ea95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18338c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "fca06bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "d25d1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "b3b9a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9225092250922509\n",
      "precision :  0.9765625\n",
      "recall :  0.8741258741258742\n",
      "accuracy :  0.9257950530035336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       140\n",
      "           1       0.98      0.87      0.92       143\n",
      "\n",
      "    accuracy                           0.93       283\n",
      "   macro avg       0.93      0.93      0.93       283\n",
      "weighted avg       0.93      0.93      0.93       283\n",
      "\n",
      "[[137   3]\n",
      " [ 18 125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "a981c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_pred, y_test, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "6447157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore: [0.92881356 0.92250923]\n",
      "precision: [0.97857143 0.87412587]\n",
      "recall: [0.88387097 0.9765625 ]\n",
      "support: [155 128]\n"
     ]
    }
   ],
   "source": [
    "print('fscore: {}'.format(fscore))\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "1816ca13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3198"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "0fc8f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "e37c51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "7411ff84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>11390</td>\n",
       "      <td>दुश्वार</td>\n",
       "      <td>دور</td>\n",
       "      <td>duʃvaːr</td>\n",
       "      <td>dvr</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3.107143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428112</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>-8.963047</td>\n",
       "      <td>-11.555681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>8530</td>\n",
       "      <td>जामा</td>\n",
       "      <td>جگر</td>\n",
       "      <td>d͡ʒaːmaː</td>\n",
       "      <td>d͡ʒɡr</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>0.213542</td>\n",
       "      <td>2.281250</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.612502</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>-13.170180</td>\n",
       "      <td>-7.685933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>9841</td>\n",
       "      <td>ज़र्रा</td>\n",
       "      <td>زار زار</td>\n",
       "      <td>zərraː</td>\n",
       "      <td>zɒr zɒr</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.172619</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.462647</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>-7.707266</td>\n",
       "      <td>-11.462407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "78           78            78           11390   दुश्वार           دور   \n",
       "94           94            94            8530      जामा           جگر   \n",
       "182         182           182            9841    ज़र्रा       زار زار   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "78            duʃvaːr                   dvr          0.571429   \n",
       "94           d͡ʒaːmaː                 d͡ʒɡr          0.625000   \n",
       "182            zərraː               zɒr zɒr          0.714286   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "78               0.428571               0.386905                  0.428571   \n",
       "94               0.375000               0.182292                  0.213542   \n",
       "182              0.285714               0.172619                  0.208333   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "78                    3.107143                              0.571429      0   \n",
       "94                    2.281250                              0.625000      0   \n",
       "182                   1.857143                              0.714286      0   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "78               0.428112            0.321477   \n",
       "94               0.630003            0.612502   \n",
       "182              0.462647            0.596593   \n",
       "\n",
       "                                         features_loan  \\\n",
       "78   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "94   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "182  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "78   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   -8.963047  -11.555681  \n",
       "94   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...  -13.170180   -7.685933  \n",
       "182  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   -7.707266  -11.462407  "
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "72a30ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>अंदरुनी</td>\n",
       "      <td>اندرونی</td>\n",
       "      <td>ndəruniː</td>\n",
       "      <td>ɒndrvnj</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153646</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>3.171875</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.484583</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.576007</td>\n",
       "      <td>8.895311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>604</td>\n",
       "      <td>-दान</td>\n",
       "      <td>دانستن</td>\n",
       "      <td>-daːn</td>\n",
       "      <td>dɒnstn</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293757</td>\n",
       "      <td>0.621542</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>6.620969</td>\n",
       "      <td>7.180840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>601</td>\n",
       "      <td>दहेज़</td>\n",
       "      <td>جهیز</td>\n",
       "      <td>dəɦez</td>\n",
       "      <td>d͡ʒhjz</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466845</td>\n",
       "      <td>0.620716</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>29.674936</td>\n",
       "      <td>10.942152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>526</td>\n",
       "      <td>तश्तरी</td>\n",
       "      <td>تشت</td>\n",
       "      <td>təʃtəriː</td>\n",
       "      <td>tʃt</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.440104</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.547848</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...</td>\n",
       "      <td>14.743056</td>\n",
       "      <td>10.560655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>1236</td>\n",
       "      <td>सही</td>\n",
       "      <td>صحیح</td>\n",
       "      <td>səɦiː</td>\n",
       "      <td>shjh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488294</td>\n",
       "      <td>0.608805</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>8.049563</td>\n",
       "      <td>8.439472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>272</td>\n",
       "      <td>ख़ुशनवीसी</td>\n",
       "      <td>خُوشنَوِیسی</td>\n",
       "      <td>xuʃnəviːsiː</td>\n",
       "      <td>xُvʃnowejsj</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.160985</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560712</td>\n",
       "      <td>0.560799</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>7.862397</td>\n",
       "      <td>8.812596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>727</td>\n",
       "      <td>पहल</td>\n",
       "      <td>پهلو</td>\n",
       "      <td>pəɦəl</td>\n",
       "      <td>پhlv</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>4.775000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708759</td>\n",
       "      <td>0.752857</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...</td>\n",
       "      <td>8.313698</td>\n",
       "      <td>9.281388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>76</td>\n",
       "      <td>आलू</td>\n",
       "      <td>آلو</td>\n",
       "      <td>aːluː</td>\n",
       "      <td>ɒlv</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625537</td>\n",
       "      <td>0.419534</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>8.036224</td>\n",
       "      <td>11.560460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>781</td>\n",
       "      <td>फ़िरंगी</td>\n",
       "      <td>فرنگی</td>\n",
       "      <td>firəŋɡiː</td>\n",
       "      <td>frŋj</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>3.078125</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574454</td>\n",
       "      <td>0.696136</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>7.528652</td>\n",
       "      <td>8.187250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>725</td>\n",
       "      <td>पहरा</td>\n",
       "      <td>پهره</td>\n",
       "      <td>pəɦraː</td>\n",
       "      <td>پhrh</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>3.395833</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700302</td>\n",
       "      <td>0.729959</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>6.872724</td>\n",
       "      <td>6.540695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>अंगूरी</td>\n",
       "      <td>انگوری</td>\n",
       "      <td>ŋɡuːriː</td>\n",
       "      <td>ɒŋvrj</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.127976</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>2.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.457744</td>\n",
       "      <td>0.781127</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.712349</td>\n",
       "      <td>8.891312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>482</td>\n",
       "      <td>तंबू</td>\n",
       "      <td>تنبو</td>\n",
       "      <td>təmbuː</td>\n",
       "      <td>tnbv</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609910</td>\n",
       "      <td>0.777764</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...</td>\n",
       "      <td>9.243767</td>\n",
       "      <td>7.644005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>403</td>\n",
       "      <td>ज़ंग</td>\n",
       "      <td>زنگ</td>\n",
       "      <td>zəŋɡə</td>\n",
       "      <td>zŋ</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598645</td>\n",
       "      <td>0.770941</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>9.147930</td>\n",
       "      <td>9.274412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>423</td>\n",
       "      <td>423</td>\n",
       "      <td>158</td>\n",
       "      <td>क़बूल</td>\n",
       "      <td>قبول</td>\n",
       "      <td>qəbuːl</td>\n",
       "      <td>ɣbvl</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323891</td>\n",
       "      <td>0.571688</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>9.164741</td>\n",
       "      <td>15.450974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>445</td>\n",
       "      <td>445</td>\n",
       "      <td>176</td>\n",
       "      <td>क़िला</td>\n",
       "      <td>قلعه</td>\n",
       "      <td>qilaː</td>\n",
       "      <td>ɣlʔh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465095</td>\n",
       "      <td>0.758047</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...</td>\n",
       "      <td>6.803464</td>\n",
       "      <td>8.363299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "      <td>109</td>\n",
       "      <td>इर्द-गिर्द</td>\n",
       "      <td>گرد</td>\n",
       "      <td>irdə-ɡirdə</td>\n",
       "      <td>ɡrd</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352625</td>\n",
       "      <td>0.543652</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>10.556375</td>\n",
       "      <td>19.998860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>714</td>\n",
       "      <td>परवानगी</td>\n",
       "      <td>پروانگی</td>\n",
       "      <td>pərvaːnɡiː</td>\n",
       "      <td>پrvɒŋj</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706648</td>\n",
       "      <td>0.784062</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>7.333721</td>\n",
       "      <td>9.131607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1   loan_word original_word  \\\n",
       "66           66            66               4     अंदरुनी       اندرونی   \n",
       "71           71            71             604        -दान        دانستن   \n",
       "76           76            76             601       दहेज़          جهیز   \n",
       "81           81            81             526      तश्तरी           تشت   \n",
       "92           92            92            1236         सही          صحیح   \n",
       "107         107           107             272   ख़ुशनवीसी   خُوشنَوِیسی   \n",
       "112         112           112             727         पहल          پهلو   \n",
       "136         136           136              76         आलू           آلو   \n",
       "167         167           167             781     फ़िरंगी         فرنگی   \n",
       "220         220           220             725        पहरा          پهره   \n",
       "248         248           248               1      अंगूरी        انگوری   \n",
       "290         290           290             482        तंबू          تنبو   \n",
       "370         370           370             403        ज़ंग           زنگ   \n",
       "423         423           423             158       क़बूल          قبول   \n",
       "445         445           445             176       क़िला          قلعه   \n",
       "466         466           466             109  इर्द-गिर्द           گرد   \n",
       "480         480           480             714     परवानगी       پروانگی   \n",
       "494         494           494             137        ओहदा          عهده   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "66           ndəruniː               ɒndrvnj          0.625000   \n",
       "71              -daːn                dɒnstn          0.833333   \n",
       "76              dəɦez                d͡ʒhjz          0.666667   \n",
       "81           təʃtəriː                   tʃt          0.625000   \n",
       "92              səɦiː                  shjh          0.800000   \n",
       "107       xuʃnəviːsiː           xُvʃnowejsj          0.727273   \n",
       "112             pəɦəl                  پhlv          1.000000   \n",
       "136             aːluː                   ɒlv          0.800000   \n",
       "167          firəŋɡiː                  frŋj          0.625000   \n",
       "220            pəɦraː                  پhrh          0.833333   \n",
       "248           ŋɡuːriː                 ɒŋvrj          0.857143   \n",
       "290            təmbuː                  tnbv          0.666667   \n",
       "370             zəŋɡə                    zŋ          0.600000   \n",
       "423            qəbuːl                  ɣbvl          0.666667   \n",
       "445             qilaː                  ɣlʔh          0.800000   \n",
       "466        irdə-ɡirdə                   ɡrd          0.700000   \n",
       "480        pərvaːnɡiː                پrvɒŋj          0.800000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "66               0.500000               0.153646                  0.182292   \n",
       "71               0.500000               0.479167                  0.520833   \n",
       "76               0.500000               0.197917                  0.222222   \n",
       "81               0.500000               0.440104                  0.500000   \n",
       "92               0.400000               0.108333                  0.125000   \n",
       "107              0.363636               0.160985                  0.181818   \n",
       "112              0.800000               0.462500                  0.516667   \n",
       "136              0.200000               0.095833                  0.108333   \n",
       "167              0.500000               0.351562                  0.395833   \n",
       "220              0.500000               0.347222                  0.388889   \n",
       "248              0.571429               0.127976                  0.148810   \n",
       "290              0.500000               0.225694                  0.256944   \n",
       "370              0.600000               0.537500                  0.600000   \n",
       "423              0.333333               0.229167                  0.256944   \n",
       "445              0.400000               0.191667                  0.216667   \n",
       "466              0.600000               0.533333                  0.600000   \n",
       "480              0.400000               0.304167                  0.337500   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "66                    3.171875                              0.625000      1   \n",
       "71                    3.875000                              0.833333      1   \n",
       "76                    1.937500                              0.666667      1   \n",
       "81                    3.625000                              0.625000      1   \n",
       "92                    2.650000                              0.800000      1   \n",
       "107                   2.363636                              0.727273      1   \n",
       "112                   4.775000                              1.000000      1   \n",
       "136                   2.000000                              0.800000      1   \n",
       "167                   3.078125                              0.625000      1   \n",
       "220                   3.395833                              0.833333      1   \n",
       "248                   2.928571                              0.857143      1   \n",
       "290                   2.812500                              0.666667      1   \n",
       "370                   4.350000                              0.600000      1   \n",
       "423                   2.916667                              0.666667      1   \n",
       "445                   3.700000                              0.800000      1   \n",
       "466                   4.350000                              0.700000      1   \n",
       "480                   2.825000                              0.800000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "66               0.484583            0.788032   \n",
       "71               0.293757            0.621542   \n",
       "76               0.466845            0.620716   \n",
       "81               0.590817            0.547848   \n",
       "92               0.488294            0.608805   \n",
       "107              0.560712            0.560799   \n",
       "112              0.708759            0.752857   \n",
       "136              0.625537            0.419534   \n",
       "167              0.574454            0.696136   \n",
       "220              0.700302            0.729959   \n",
       "248              0.457744            0.781127   \n",
       "290              0.609910            0.777764   \n",
       "370              0.598645            0.770941   \n",
       "423              0.323891            0.571688   \n",
       "445              0.465095            0.758047   \n",
       "466              0.352625            0.543652   \n",
       "480              0.706648            0.784062   \n",
       "494              0.249290            0.475189   \n",
       "\n",
       "                                         features_loan  \\\n",
       "66   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "71   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   \n",
       "76   [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "81   [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "92   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "107  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "112  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...   \n",
       "136  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...   \n",
       "167  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...   \n",
       "220  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "248  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "290  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "370  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "423  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "445  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...   \n",
       "466  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "480  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "66   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.576007    8.895311  \n",
       "71   [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    6.620969    7.180840  \n",
       "76   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...   29.674936   10.942152  \n",
       "81   [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, ...   14.743056   10.560655  \n",
       "92   [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...    8.049563    8.439472  \n",
       "107  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    7.862397    8.812596  \n",
       "112  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, 1, 1...    8.313698    9.281388  \n",
       "136  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...    8.036224   11.560460  \n",
       "167  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    7.528652    8.187250  \n",
       "220  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    6.872724    6.540695  \n",
       "248  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.712349    8.891312  \n",
       "290  [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, 1, ...    9.243767    7.644005  \n",
       "370  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    9.147930    9.274412  \n",
       "423  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    9.164741   15.450974  \n",
       "445  [1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1,...    6.803464    8.363299  \n",
       "466  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   10.556375   19.998860  \n",
       "480  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    7.333721    9.131607  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  "
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d5102",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with   edit dist as features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "723fa910",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen', \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "id": "2da7a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "id": "9b14d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "id": "3306288e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 6), (4194,), (466, 6), (4194,))"
      ]
     },
     "execution_count": 1140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_train.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "7d366fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "id": "50968c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "id": "d14c4981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9052631578947369\n",
      "precision :  0.9084507042253521\n",
      "recall :  0.9020979020979021\n",
      "accuracy :  0.9420600858369099\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "id": "b4b3f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       323\n",
      "           1       0.91      0.90      0.91       143\n",
      "\n",
      "    accuracy                           0.94       466\n",
      "   macro avg       0.93      0.93      0.93       466\n",
      "weighted avg       0.94      0.94      0.94       466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e9110",
   "metadata": {},
   "source": [
    "# Train a binary logistic regression classifier and testing with   edit dist and cosine sims as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "efd2aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "e1181b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "fd767893",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "ed627508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 8), (4194,), (466, 8), (466,))"
      ]
     },
     "execution_count": 1148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "cc24da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=500 ).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "a823c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "4010fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9295774647887325\n",
      "precision :  0.9361702127659575\n",
      "recall :  0.9230769230769231\n",
      "accuracy :  0.9570815450643777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       323\n",
      "           1       0.94      0.92      0.93       143\n",
      "\n",
      "    accuracy                           0.96       466\n",
      "   macro avg       0.95      0.95      0.95       466\n",
      "weighted avg       0.96      0.96      0.96       466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f10fe",
   "metadata": {},
   "source": [
    "# Try with SVM with various combinations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "66b5c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "8d7b1324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "5bbfa0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "id": "4d2b3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9305555555555555\n",
      "precision :  0.9241379310344827\n",
      "recall :  0.9370629370629371\n",
      "accuracy :  0.9570815450643777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       323\n",
      "           1       0.92      0.94      0.93       143\n",
      "\n",
      "    accuracy                           0.96       466\n",
      "   macro avg       0.95      0.95      0.95       466\n",
      "weighted avg       0.96      0.96      0.96       466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad554105",
   "metadata": {},
   "source": [
    "# with all 10 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "id": "4d8e76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "id": "3391f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "38c9ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "d5ba2ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "4887350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "b20c6e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9965156794425087\n",
      "precision :  0.9930555555555556\n",
      "recall :  1.0\n",
      "accuracy :  0.9978540772532188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       323\n",
      "           1       0.99      1.00      1.00       143\n",
      "\n",
      "    accuracy                           1.00       466\n",
      "   macro avg       1.00      1.00      1.00       466\n",
      "weighted avg       1.00      1.00      1.00       466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e40b2",
   "metadata": {},
   "source": [
    "# with edit distances and cosine sim: 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "8b770da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "0233d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "4d986bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand= StandardScaler()\n",
    "# fit = stand.fit(x_train)\n",
    "# x_train = fit.transform(x_train)\n",
    "\n",
    "# x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "7ae5956c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 8), (4194,), (466, 8), (466,))"
      ]
     },
     "execution_count": 1166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "id": "bf43cfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "2e105da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "275761c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9305555555555555\n",
      "precision :  0.9241379310344827\n",
      "recall :  0.9370629370629371\n",
      "accuracy :  0.9570815450643777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       323\n",
      "           1       0.92      0.94      0.93       143\n",
      "\n",
      "    accuracy                           0.96       466\n",
      "   macro avg       0.95      0.95      0.95       466\n",
      "weighted avg       0.96      0.96      0.96       466\n",
      "\n",
      "[[312  11]\n",
      " [  9 134]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b32dc8",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "e6ca607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "9cc31c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ca2a6",
   "metadata": {},
   "source": [
    "# with 8 features, edits and cosine w/o logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "08e96e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4194, 8), (4194,), (466, 8), (466,))"
      ]
     },
     "execution_count": 1173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "28339d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "85554373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 1175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "418b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "40b50382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9209621993127148\n",
      "precision :  0.9054054054054054\n",
      "recall :  0.9370629370629371\n",
      "accuracy :  0.9506437768240343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       323\n",
      "           1       0.91      0.94      0.92       143\n",
      "\n",
      "    accuracy                           0.95       466\n",
      "   macro avg       0.94      0.95      0.94       466\n",
      "weighted avg       0.95      0.95      0.95       466\n",
      "\n",
      "[[309  14]\n",
      " [  9 134]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef47cdb",
   "metadata": {},
   "source": [
    "# 10 features, all with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "0d5e8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Fast Levenshtein', 'Dolgo Prime Distance',\n",
    "       'Feature Edit Distance', 'Hamming Feature Distance',\n",
    "       'Weighted Feature Distance', 'Fast Levenshtein Distance Div Maxlen','mbert_cos_similarity', 'xlm_cos_similarity',\n",
    "             'CNN_logits', 'DNN_logits'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "18163a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features].values\n",
    "y_train = train[labels].values.ravel()\n",
    "x_test = test[features].values\n",
    "y_test = test[labels].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "a4ee48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand= StandardScaler()\n",
    "fit = stand.fit(x_train)\n",
    "x_train = fit.transform(x_train)\n",
    "\n",
    "x_test = fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "61b0862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "1345cc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 1182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "de880450",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "d035fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score :  0.9964912280701755\n",
      "precision :  1.0\n",
      "recall :  0.993006993006993\n",
      "accuracy :  0.9978540772532188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       323\n",
      "           1       1.00      0.99      1.00       143\n",
      "\n",
      "    accuracy                           1.00       466\n",
      "   macro avg       1.00      1.00      1.00       466\n",
      "weighted avg       1.00      1.00      1.00       466\n",
      "\n",
      "[[323   0]\n",
      " [  1 142]]\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score : \", f1_score(y_test, y_pred ))\n",
    "print(\"precision : \",precision_score(y_test, y_pred))\n",
    "print(\"recall : \",recall_score(y_test, y_pred )) \n",
    "print(\"accuracy : \",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "ba9ed688",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = np.array([x + 2*y for x, y in zip(y_pred, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "7305af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.array(np.where(unq == 3)).tolist()[0]\n",
    "fp = np.array(np.where(unq == 1)).tolist()[0]\n",
    "tn = np.array(np.where(unq == 0)).tolist()[0]\n",
    "fn = np.array(np.where(unq == 2)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "4c765561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[336]"
      ]
     },
     "execution_count": 1187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "481174c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>336</td>\n",
       "      <td>288</td>\n",
       "      <td>ख़्वाजा</td>\n",
       "      <td>خواجه</td>\n",
       "      <td>xvaːd͡ʒaː</td>\n",
       "      <td>xvɒd͡ʒh</td>\n",
       "      <td>Khwaja</td>\n",
       "      <td>خواجه</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513219</td>\n",
       "      <td>0.78262</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.64623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1 loan_word original_word loan_word_epitran  \\\n",
       "336         336           288   ख़्वाजा         خواجه         xvaːd͡ʒaː   \n",
       "\n",
       "    original_word_epitran loan_english original_english label  \\\n",
       "336               xvɒd͡ʒh       Khwaja            خواجه  loan   \n",
       "\n",
       "     Fast Levenshtein  ...  Hamming Feature Distance  \\\n",
       "336          0.444444  ...                  0.046296   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  \\\n",
       "336                   0.791667                              0.444444   \n",
       "\n",
       "     label_bin  mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "336          1              0.513219             0.78262   \n",
       "\n",
       "                                         features_loan  \\\n",
       "336  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "\n",
       "                                         features_orig DNN_logits CNN_logits  \n",
       "336  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...  -0.000329   -0.64623  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 1188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fn,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "f6fa2a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>loan_english</th>\n",
       "      <th>original_english</th>\n",
       "      <th>label</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>...</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Unnamed: 0.1, loan_word, original_word, loan_word_epitran, original_word_epitran, loan_english, original_english, label, Fast Levenshtein, Dolgo Prime Distance, Feature Edit Distance, Hamming Feature Distance, Weighted Feature Distance, Fast Levenshtein Distance Div Maxlen, label_bin, mbert_cos_similarity, xlm_cos_similarity, features_loan, features_orig, DNN_logits, CNN_logits]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 1189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[fp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "0c1a654a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "      <td>पेचकस</td>\n",
       "      <td>پیچ کش</td>\n",
       "      <td>pet͡ʃkəs</td>\n",
       "      <td>پjt͡ʃ kʃ</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524065</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>7.977158</td>\n",
       "      <td>9.532843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>154</td>\n",
       "      <td>क़तार</td>\n",
       "      <td>قطار</td>\n",
       "      <td>qətaːr</td>\n",
       "      <td>ɣtɒr</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496417</td>\n",
       "      <td>0.828683</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>8.294688</td>\n",
       "      <td>8.501902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>702</td>\n",
       "      <td>पंजा</td>\n",
       "      <td>پنجه</td>\n",
       "      <td>pəɲd͡ʒaː</td>\n",
       "      <td>پnd͡ʒh</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566448</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>7.139163</td>\n",
       "      <td>10.420475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>410</td>\n",
       "      <td>ज़बान</td>\n",
       "      <td>زبان</td>\n",
       "      <td>zəbaːn</td>\n",
       "      <td>zbɒn</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.327706</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>6.765188</td>\n",
       "      <td>7.704823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>482</td>\n",
       "      <td>1261</td>\n",
       "      <td>सिरका</td>\n",
       "      <td>سرکه</td>\n",
       "      <td>sirkaː</td>\n",
       "      <td>srkh</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.641863</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>6.061551</td>\n",
       "      <td>6.145308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>1059</td>\n",
       "      <td>राय</td>\n",
       "      <td>رأی</td>\n",
       "      <td>raːj</td>\n",
       "      <td>rɒʔj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.678451</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.641813</td>\n",
       "      <td>9.061541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "0             0             0            1246      साया          سایه   \n",
       "5             5             5             743     पेचकस        پیچ کش   \n",
       "7             7             7             154     क़तार          قطار   \n",
       "8             8             8             702      पंजा          پنجه   \n",
       "9             9             9             410     ज़बान          زبان   \n",
       "..          ...           ...             ...       ...           ...   \n",
       "482         482           482            1261     सिरका          سرکه   \n",
       "485         485           485            1059       राय           رأی   \n",
       "494         494           494             137      ओहदा          عهده   \n",
       "498         498           498            1047   रफ़्तार         رفتار   \n",
       "499         499           499             412     ज़ब्त           ضبط   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "0              saːjaː                  sɒjh          0.666667   \n",
       "5            pet͡ʃkəs              پjt͡ʃ kʃ          0.625000   \n",
       "7              qətaːr                  ɣtɒr          0.666667   \n",
       "8            pəɲd͡ʒaː                پnd͡ʒh          0.625000   \n",
       "9              zəbaːn                  zbɒn          0.500000   \n",
       "..                ...                   ...               ...   \n",
       "482            sirkaː                  srkh          0.500000   \n",
       "485              raːj                  rɒʔj          0.500000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "498           rəftaːr                 rftɒr          0.428571   \n",
       "499             zəbtə                   zbt          0.400000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                0.166667               0.062500                  0.069444   \n",
       "5                0.375000               0.250000                  0.281250   \n",
       "7                0.166667               0.190972                  0.208333   \n",
       "8                0.375000               0.273438                  0.307292   \n",
       "9                0.166667               0.170139                  0.187500   \n",
       "..                    ...                    ...                       ...   \n",
       "482              0.333333               0.190972                  0.215278   \n",
       "485              0.250000               0.255208                  0.281250   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "498              0.142857               0.145833                  0.160714   \n",
       "499              0.400000               0.358333                  0.400000   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "5                     2.265625                              0.625000      1   \n",
       "7                     1.750000                              0.666667      1   \n",
       "8                     2.718750                              0.625000      1   \n",
       "9                     1.458333                              0.500000      1   \n",
       "..                         ...                                   ...    ...   \n",
       "482                   2.145833                              0.500000      1   \n",
       "485                   2.187500                              0.500000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "5                0.524065            0.851965   \n",
       "7                0.496417            0.828683   \n",
       "8                0.566448            0.512996   \n",
       "9                0.407726            0.327706   \n",
       "..                    ...                 ...   \n",
       "482              0.607168            0.641863   \n",
       "485              0.490000            0.678451   \n",
       "494              0.249290            0.475189   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "5    [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "7    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "..                                                 ...   \n",
       "482  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "485  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "5    [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...    7.977158    9.532843  \n",
       "7    [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    8.294688    8.501902  \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    7.139163   10.420475  \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...    6.765188    7.704823  \n",
       "..                                                 ...         ...         ...  \n",
       "482  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    6.061551    6.145308  \n",
       "485  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.641813    9.061541  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "\n",
       "[137 rows x 20 columns]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[tp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "f4c5e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>loan_word</th>\n",
       "      <th>original_word</th>\n",
       "      <th>loan_word_epitran</th>\n",
       "      <th>original_word_epitran</th>\n",
       "      <th>Fast Levenshtein</th>\n",
       "      <th>Dolgo Prime Distance</th>\n",
       "      <th>Feature Edit Distance</th>\n",
       "      <th>Hamming Feature Distance</th>\n",
       "      <th>Weighted Feature Distance</th>\n",
       "      <th>Fast Levenshtein Distance Div Maxlen</th>\n",
       "      <th>label</th>\n",
       "      <th>mbert_cos_similarity</th>\n",
       "      <th>xlm_cos_similarity</th>\n",
       "      <th>features_loan</th>\n",
       "      <th>features_orig</th>\n",
       "      <th>DNN_logits</th>\n",
       "      <th>CNN_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1246</td>\n",
       "      <td>साया</td>\n",
       "      <td>سایه</td>\n",
       "      <td>saːjaː</td>\n",
       "      <td>sɒjh</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610435</td>\n",
       "      <td>0.627098</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>8.067153</td>\n",
       "      <td>13.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "      <td>पेचकस</td>\n",
       "      <td>پیچ کش</td>\n",
       "      <td>pet͡ʃkəs</td>\n",
       "      <td>پjt͡ʃ kʃ</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524065</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...</td>\n",
       "      <td>[-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "      <td>7.977158</td>\n",
       "      <td>9.532843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>154</td>\n",
       "      <td>क़तार</td>\n",
       "      <td>قطار</td>\n",
       "      <td>qətaːr</td>\n",
       "      <td>ɣtɒr</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496417</td>\n",
       "      <td>0.828683</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>8.294688</td>\n",
       "      <td>8.501902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>702</td>\n",
       "      <td>पंजा</td>\n",
       "      <td>پنجه</td>\n",
       "      <td>pəɲd͡ʒaː</td>\n",
       "      <td>پnd͡ʒh</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566448</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>7.139163</td>\n",
       "      <td>10.420475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>410</td>\n",
       "      <td>ज़बान</td>\n",
       "      <td>زبان</td>\n",
       "      <td>zəbaːn</td>\n",
       "      <td>zbɒn</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.327706</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>[-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...</td>\n",
       "      <td>6.765188</td>\n",
       "      <td>7.704823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>482</td>\n",
       "      <td>1261</td>\n",
       "      <td>सिरका</td>\n",
       "      <td>سرکه</td>\n",
       "      <td>sirkaː</td>\n",
       "      <td>srkh</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190972</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>2.145833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.641863</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>6.061551</td>\n",
       "      <td>6.145308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>1059</td>\n",
       "      <td>राय</td>\n",
       "      <td>رأی</td>\n",
       "      <td>raːj</td>\n",
       "      <td>rɒʔj</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.678451</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...</td>\n",
       "      <td>7.641813</td>\n",
       "      <td>9.061541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>137</td>\n",
       "      <td>ओहदा</td>\n",
       "      <td>عهده</td>\n",
       "      <td>oɦdaː</td>\n",
       "      <td>ʔhdh</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249290</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>[-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...</td>\n",
       "      <td>8.374212</td>\n",
       "      <td>8.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>1047</td>\n",
       "      <td>रफ़्तार</td>\n",
       "      <td>رفتار</td>\n",
       "      <td>rəftaːr</td>\n",
       "      <td>rftɒr</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...</td>\n",
       "      <td>8.593673</td>\n",
       "      <td>10.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>ज़ब्त</td>\n",
       "      <td>ضبط</td>\n",
       "      <td>zəbtə</td>\n",
       "      <td>zbt</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466774</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...</td>\n",
       "      <td>9.795804</td>\n",
       "      <td>14.636103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 loan_word original_word  \\\n",
       "0             0             0            1246      साया          سایه   \n",
       "5             5             5             743     पेचकस        پیچ کش   \n",
       "7             7             7             154     क़तार          قطار   \n",
       "8             8             8             702      पंजा          پنجه   \n",
       "9             9             9             410     ज़बान          زبان   \n",
       "..          ...           ...             ...       ...           ...   \n",
       "482         482           482            1261     सिरका          سرکه   \n",
       "485         485           485            1059       राय           رأی   \n",
       "494         494           494             137      ओहदा          عهده   \n",
       "498         498           498            1047   रफ़्तार         رفتار   \n",
       "499         499           499             412     ज़ब्त           ضبط   \n",
       "\n",
       "    loan_word_epitran original_word_epitran  Fast Levenshtein  \\\n",
       "0              saːjaː                  sɒjh          0.666667   \n",
       "5            pet͡ʃkəs              پjt͡ʃ kʃ          0.625000   \n",
       "7              qətaːr                  ɣtɒr          0.666667   \n",
       "8            pəɲd͡ʒaː                پnd͡ʒh          0.625000   \n",
       "9              zəbaːn                  zbɒn          0.500000   \n",
       "..                ...                   ...               ...   \n",
       "482            sirkaː                  srkh          0.500000   \n",
       "485              raːj                  rɒʔj          0.500000   \n",
       "494             oɦdaː                  ʔhdh          0.800000   \n",
       "498           rəftaːr                 rftɒr          0.428571   \n",
       "499             zəbtə                   zbt          0.400000   \n",
       "\n",
       "     Dolgo Prime Distance  Feature Edit Distance  Hamming Feature Distance  \\\n",
       "0                0.166667               0.062500                  0.069444   \n",
       "5                0.375000               0.250000                  0.281250   \n",
       "7                0.166667               0.190972                  0.208333   \n",
       "8                0.375000               0.273438                  0.307292   \n",
       "9                0.166667               0.170139                  0.187500   \n",
       "..                    ...                    ...                       ...   \n",
       "482              0.333333               0.190972                  0.215278   \n",
       "485              0.250000               0.255208                  0.281250   \n",
       "494              0.400000               0.116667                  0.133333   \n",
       "498              0.142857               0.145833                  0.160714   \n",
       "499              0.400000               0.358333                  0.400000   \n",
       "\n",
       "     Weighted Feature Distance  Fast Levenshtein Distance Div Maxlen  label  \\\n",
       "0                     1.187500                              0.666667      1   \n",
       "5                     2.265625                              0.625000      1   \n",
       "7                     1.750000                              0.666667      1   \n",
       "8                     2.718750                              0.625000      1   \n",
       "9                     1.458333                              0.500000      1   \n",
       "..                         ...                                   ...    ...   \n",
       "482                   2.145833                              0.500000      1   \n",
       "485                   2.187500                              0.500000      1   \n",
       "494                   2.150000                              0.800000      1   \n",
       "498                   1.250000                              0.428571      1   \n",
       "499                   2.900000                              0.400000      1   \n",
       "\n",
       "     mbert_cos_similarity  xlm_cos_similarity  \\\n",
       "0                0.610435            0.627098   \n",
       "5                0.524065            0.851965   \n",
       "7                0.496417            0.828683   \n",
       "8                0.566448            0.512996   \n",
       "9                0.407726            0.327706   \n",
       "..                    ...                 ...   \n",
       "482              0.607168            0.641863   \n",
       "485              0.490000            0.678451   \n",
       "494              0.249290            0.475189   \n",
       "498              0.340994            0.629839   \n",
       "499              0.466774            0.746048   \n",
       "\n",
       "                                         features_loan  \\\n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "5    [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1,...   \n",
       "7    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...   \n",
       "..                                                 ...   \n",
       "482  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "485  [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...   \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...   \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...   \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...   \n",
       "\n",
       "                                         features_orig  DNN_logits  CNN_logits  \n",
       "0    [-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1,...    8.067153   13.088353  \n",
       "5    [-1, -1, 1, -1, 1, -1, -1, 0, 1, -1, -1, -1, 1...    7.977158    9.532843  \n",
       "7    [-1, 1, 1, -1, -1, -1, 1, 0, 1, -1, -1, 1, -1,...    8.294688    8.501902  \n",
       "8    [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    7.139163   10.420475  \n",
       "9    [-1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, -...    6.765188    7.704823  \n",
       "..                                                 ...         ...         ...  \n",
       "482  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    6.061551    6.145308  \n",
       "485  [-1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -...    7.641813    9.061541  \n",
       "494  [-1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, ...    8.374212    8.988889  \n",
       "498  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, 1...    8.593673   10.353712  \n",
       "499  [-1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -...    9.795804   14.636103  \n",
       "\n",
       "[137 rows x 20 columns]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[tp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57682803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
