{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d900aa",
   "metadata": {},
   "source": [
    "Assumes you have run `Train_Testset.ipynb` first to make the `alldata`, `realdist`, and `balanced` train/test splits for the chosen language pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a3a5e",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from epitran.backoff import Backoff\n",
    "from googletrans import Translator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "epitran.download.cedict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4977c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import io\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726f5cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,\\\n\u001b[1;32m      7\u001b[0m     BertForSequenceClassification, BertForPreTraining, AutoModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLMTokenizer, XLMWithLMHeadModel, XLMModel\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# transformer specific imports \n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,\\\n",
    "    BertForSequenceClassification, BertForPreTraining, AutoModel\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel, XLMModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, mean_squared_error\n",
    "import time\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b4fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "#device = torch.device(\"cuda:0:3\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80da282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e42f7",
   "metadata": {},
   "source": [
    "# DNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e92610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            \n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits_new = self.linear_relu_stack(x)\n",
    "        logits  = self.dropout(logits_new)\n",
    "        \n",
    "        return torch.sigmoid(logits), logits_new\n",
    "    \n",
    "    def fit(self, X_train, Y_train, X_val, Y_val, criterion, optimizer, n_epochs=5000):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accur = []\n",
    "        val_accur = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            y_pred, logits = self(X_train.float())\n",
    "\n",
    "            train_loss = criterion(y_pred, Y_train.float())\n",
    "\n",
    "            if epoch % (n_epochs // 50) == 0:\n",
    "                train_acc,_ = self.calculate_accuracy(Y_train, y_pred)\n",
    "\n",
    "                y_val_pred = self(X_val.float())[0]\n",
    "\n",
    "                val_loss = criterion(y_val_pred, Y_val.float())\n",
    "\n",
    "                val_acc, total_corr = self.calculate_accuracy(Y_val, y_val_pred)\n",
    "\n",
    "                print(f'''epoch {epoch}\n",
    "                    Train set - loss: {self.round_tensor(train_loss)}, accuracy: {self.round_tensor(train_acc)} \n",
    "                    Val set - loss: {self.round_tensor(val_loss)}, accuracy: {self.round_tensor(val_acc)}''')\n",
    "                \n",
    "                train_losses.append(train_loss.detach().cpu().numpy())\n",
    "                val_losses.append(val_loss.detach().cpu().numpy())\n",
    "\n",
    "                val_accur.append(val_acc.detach().cpu().numpy())\n",
    "                train_accur.append(train_acc.detach().cpu().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        return train_losses,val_losses,train_accur,val_accur\n",
    "    \n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        predicted = y_pred.ge(.5) \n",
    "        return ((y_true == predicted).sum().float() / len(y_true), (y_true == predicted).sum())\n",
    "    \n",
    "    def round_tensor(self, t, decimal_places=3):\n",
    "        return round(t.item(), decimal_places)\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses, train_accur, val_accur):\n",
    "        epochs = range(1, len(train_accur) + 1)\n",
    "\n",
    "        plt.plot(epochs, train_accur, 'bo', label='Training acc')\n",
    "        plt.plot(epochs, val_accur, 'b', label='Vaidation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_losses, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36154569",
   "metadata": {},
   "source": [
    "# MyDataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b14ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding the Dataset class required for the use of PyTorch's data loader classes.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l1_encodings, l2_encodings):\n",
    "        self.l1_encodings = l1_encodings\n",
    "        self.l2_encodings = l2_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {('l1_' + key): torch.tensor(val[idx]) for key, val in self.l1_encodings.items()}\n",
    "        item2 = {('l2_' + key): torch.tensor(val[idx]) for key, val in self.l2_encodings.items()}\n",
    "        item.update(item2)\n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l1_encodings['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a47bf8",
   "metadata": {},
   "source": [
    "# Download LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b243a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-100-1280 were not used when initializing XLMModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "xlm_tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "xlm_model = XLMModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "947738c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMModel(\n",
       "  (position_embeddings): Embedding(512, 1280)\n",
       "  (embeddings): Embedding(200000, 1280, padding_idx=2)\n",
       "  (layer_norm_emb): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (1): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (2): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (3): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (4): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (5): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (6): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (7): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (8): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (9): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (10): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (11): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (12): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (13): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (14): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (15): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm1): ModuleList(\n",
       "    (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (ffns): ModuleList(\n",
       "    (0): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (1): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (2): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (3): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (4): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (5): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (6): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (7): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (8): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (9): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (10): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (11): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (12): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (13): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (14): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "    (15): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (act): GELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (layer_norm2): ModuleList(\n",
       "    (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55834b9",
   "metadata": {},
   "source": [
    "# Pipeline function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4051b",
   "metadata": {},
   "source": [
    "## Get Panphon phonetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4354d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_panphon_features(train_set, test_set):\n",
    "    #get phonetic features using PanPhon\n",
    "    ft = panphon.FeatureTable()   \n",
    "    \n",
    "    train_set['features_loan'] = train_set.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "    train_set['features_orig'] = train_set.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "    test_set['features_loan'] = test_set.apply(lambda x:ft.word_to_vector_list(x[\"loan_word_epitran\"],numeric=True ), axis=1)\n",
    "    test_set['features_orig'] = test_set.apply(lambda x:ft.word_to_vector_list(x[\"original_word_epitran\"],numeric=True ), axis=1)\n",
    "\n",
    "    train_set['features_loan'] = train_set['features_loan'].apply(lambda x:sum(x, []))\n",
    "    train_set['features_orig'] = train_set['features_orig'].apply(lambda x:sum(x, []))\n",
    "    test_set['features_orig'] = test_set['features_orig'].apply(lambda x:sum(x, []))\n",
    "    test_set['features_loan'] = test_set['features_loan'].apply(lambda x:sum(x, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413d0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_panphon_features(train_set, test_set, maxlen, verbose=False):\n",
    "    # Pad the phonetic features of the loan word and original word out to the maxlen \n",
    "    # of the features appearing in the training set (format: `<loan><pad 0s><orig><pad 0s>`).\n",
    "    train_set['features_loan'] = train_set['features_loan'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[0]-len(x)), 'constant'))\n",
    "    train_set['features_orig'] = train_set['features_orig'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[1]-len(x)), 'constant'))\n",
    "    test_set['features_loan'] = test_set['features_loan'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[0]-len(x)), 'constant'))\n",
    "    test_set['features_orig'] = test_set['features_orig'].apply(lambda x: \\\n",
    "                                    np.pad(x,\\\n",
    "                                    (0,maxlen[1]-len(x)), 'constant'))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Sample train features:\\n\",\\\n",
    "                train_set['features_loan'][np.random.randint(len(train_set['features_loan']))],\\\n",
    "                train_set['features_orig'][np.random.randint(len(train_set['features_loan']))])\n",
    "\n",
    "        print(\"Sample test features:\\n\",\\\n",
    "                test_set['features_loan'][np.random.randint(len(test_set['features_loan']))],\\\n",
    "                test_set['features_orig'][np.random.randint(len(test_set['features_orig']))])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a031511",
   "metadata": {},
   "source": [
    "## Add target labels and make train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900f4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_labels(train_set, test_set):\n",
    "    Y_train = np.array([y for y in train_set['label_bin']])\n",
    "    Y_test = np.array([y for y in test_set['label_bin']])\n",
    "    return Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93f3aa",
   "metadata": {},
   "source": [
    "Make a validation split for training the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16fd11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_set(train_set, test_set, Y_train):\n",
    "    X_train = np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                np.array([x for x in train_alldata['features_orig']])])\n",
    "    X_test = np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                np.array([x for x in test_alldata['features_orig']])])\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2,\\\n",
    "                                                      random_state=1, stratify=Y_train)\n",
    "    return X_train, X_val, X_test, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee312b",
   "metadata": {},
   "source": [
    "Make tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44851fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensors(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    X_train = torch.tensor(X_train).to(device)\n",
    "    Y_train = torch.tensor(Y_train).to(device).reshape((-1,1))\n",
    "\n",
    "    X_val = torch.tensor(X_val).to(device)\n",
    "    Y_val = torch.tensor(Y_val).to(device).reshape((-1,1))\n",
    "    \n",
    "    X_test = torch.tensor(X_test).to(device)\n",
    "    Y_test = torch.tensor(Y_test).to(device).reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40b267",
   "metadata": {},
   "source": [
    "## Get cosine similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683ed59",
   "metadata": {},
   "source": [
    "MBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48e84130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbert_cos_sims(l1_data,l2_data):\n",
    "    with torch.no_grad():\n",
    "        tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_bert_MODEL)\n",
    "        tokenizer.model_max_length = MAXTOKENS\n",
    "        l1_encodings = tokenizer(l1_data, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "        l2_encodings = tokenizer(l2_data, truncation=False, padding=True, max_length=MAXTOKENS)\n",
    "        \n",
    "        dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "        \n",
    "        base_model = BertModel.from_pretrained(PRE_TRAINED_bert_MODEL).to(device)\n",
    "        base_model.eval()\n",
    "        cos_s = torch.nn.CosineSimilarity()\n",
    "        \n",
    "        sim_lst = []\n",
    "        \n",
    "        #loop through dataset \n",
    "        for step, batch in enumerate(data_loader):\n",
    "            l1_vector = base_model(batch['l1_input_ids'].to(device),\n",
    "                                          attention_mask=batch['l1_attention_mask'].to(device),\n",
    "                                          return_dict=True).last_hidden_state[:, 0, :]\n",
    "            l2_vector = base_model(batch['l2_input_ids'].to(device),\n",
    "                                          attention_mask=batch['l2_attention_mask'].to(device),\n",
    "                                          return_dict=True).last_hidden_state[:, 0, :]\n",
    "            sims = cos_s(l1_vector, l2_vector).data.cpu().numpy()\n",
    "            sim_lst.extend(list(sims))\n",
    "            if (step * BS) % 100 < BS:\n",
    "                print(\"Got {}\".format(len(sim_lst)))\n",
    "        print()\n",
    "                \n",
    "    return sim_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643675a",
   "metadata": {},
   "source": [
    "XLM-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6ecafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xlm_cos_sims(l1_data,l2_data):\n",
    "    with torch.no_grad():\n",
    "        tokenizer = XLMTokenizer.from_pretrained(PRE_TRAINED_xlm_MODEL)\n",
    "        tokenizer.model_max_length = MAXTOKENS \n",
    "        l1_encodings = tokenizer(l1_data, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "        l2_encodings = tokenizer(l2_data, truncation=False, padding=True, max_length=MAXTOKENS, return_tensors=\"pt\", return_special_tokens_mask=True)\n",
    "\n",
    "        dataset = MyDataset(l1_encodings, l2_encodings)\n",
    "\n",
    "        data_loader = DataLoader(dataset, batch_size=BS, shuffle=False)  # shuffle False for reproducibility\n",
    "\n",
    "        \n",
    "        base_model = XLMModel.from_pretrained(PRE_TRAINED_xlm_MODEL).to(device)\n",
    "        \n",
    "        base_model.eval()\n",
    "        cos_s = torch.nn.CosineSimilarity()\n",
    "        \n",
    "        sim_lst = []\n",
    "\n",
    "        #loop through dataset \n",
    "        for step, batch in enumerate(data_loader):\n",
    "            \n",
    "            l1_vector = base_model(batch['l1_input_ids'].to(device), output_hidden_states=False).last_hidden_state\n",
    "            l2_vector = base_model(batch['l2_input_ids'].to(device), output_hidden_states=False).last_hidden_state \n",
    "            \n",
    "            sims = cos_s(l1_vector[:,0,:],l2_vector[:,0,:]).data.cpu().numpy()\n",
    "            sim_lst.extend(list(sims))\n",
    "            if (step * BS) % 100 < BS:\n",
    "                print(\"Got {}\".format(len(sim_lst)))\n",
    "        print()\n",
    "                \n",
    "    return sim_lst "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcffe48",
   "metadata": {},
   "source": [
    "# Load `language-pairs.json` list and run pipeline for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86256869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assamese-Bengali\n",
      "Bengali-Assamese\n"
     ]
    }
   ],
   "source": [
    "pairs = None\n",
    "\n",
    "with open('language-pairs.json', 'r') as f: # for getting logits from all languages\n",
    " \n",
    "    \n",
    "    pairs = json.loads(f.read())\n",
    "for pair in pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23084e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persian-Arabic\n",
      "Hungarian-German\n",
      "German-Italian\n",
      "Catalan-Arabic\n"
     ]
    }
   ],
   "source": [
    "pairs = None\n",
    "\n",
    "with open('../language-pairs-holdout.json', 'r') as f: # for getting logits from all languages\n",
    " \n",
    "    \n",
    "    pairs = json.loads(f.read())\n",
    "for pair in pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189eca7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assamese-Bengali\n",
      "\n",
      "Using cpu device\n",
      "\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=624, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") \n",
      "\n",
      "epoch 0\n",
      "                    Train set - loss: 0.702, accuracy: 0.27 \n",
      "                    Val set - loss: 0.699, accuracy: 0.314\n",
      "epoch 100\n",
      "                    Train set - loss: 0.552, accuracy: 0.747 \n",
      "                    Val set - loss: 0.537, accuracy: 0.791\n",
      "epoch 200\n",
      "                    Train set - loss: 0.491, accuracy: 0.752 \n",
      "                    Val set - loss: 0.472, accuracy: 0.785\n",
      "epoch 300\n",
      "                    Train set - loss: 0.475, accuracy: 0.758 \n",
      "                    Val set - loss: 0.49, accuracy: 0.75\n",
      "epoch 400\n",
      "                    Train set - loss: 0.472, accuracy: 0.752 \n",
      "                    Val set - loss: 0.498, accuracy: 0.733\n",
      "epoch 500\n",
      "                    Train set - loss: 0.467, accuracy: 0.747 \n",
      "                    Val set - loss: 0.486, accuracy: 0.759\n",
      "epoch 600\n",
      "                    Train set - loss: 0.45, accuracy: 0.756 \n",
      "                    Val set - loss: 0.501, accuracy: 0.747\n",
      "epoch 700\n",
      "                    Train set - loss: 0.448, accuracy: 0.739 \n",
      "                    Val set - loss: 0.492, accuracy: 0.762\n",
      "epoch 800\n",
      "                    Train set - loss: 0.424, accuracy: 0.756 \n",
      "                    Val set - loss: 0.483, accuracy: 0.756\n",
      "epoch 900\n",
      "                    Train set - loss: 0.402, accuracy: 0.769 \n",
      "                    Val set - loss: 0.48, accuracy: 0.779\n",
      "epoch 1000\n",
      "                    Train set - loss: 0.388, accuracy: 0.769 \n",
      "                    Val set - loss: 0.486, accuracy: 0.756\n",
      "epoch 1100\n",
      "                    Train set - loss: 0.367, accuracy: 0.77 \n",
      "                    Val set - loss: 0.488, accuracy: 0.747\n",
      "epoch 1200\n",
      "                    Train set - loss: 0.345, accuracy: 0.78 \n",
      "                    Val set - loss: 0.499, accuracy: 0.741\n",
      "epoch 1300\n",
      "                    Train set - loss: 0.315, accuracy: 0.801 \n",
      "                    Val set - loss: 0.48, accuracy: 0.733\n",
      "epoch 1400\n",
      "                    Train set - loss: 0.289, accuracy: 0.818 \n",
      "                    Val set - loss: 0.478, accuracy: 0.753\n",
      "epoch 1500\n",
      "                    Train set - loss: 0.268, accuracy: 0.83 \n",
      "                    Val set - loss: 0.487, accuracy: 0.744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# train and plot losses, accuracy\u001b[39;00m\n\u001b[1;32m     52\u001b[0m train_losses, val_losses, train_accur, val_accur \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mplot_losses(train_losses,val_losses,train_accur,val_accur)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# get and pad PanPhon features for realdist and balanced splits\u001b[39;00m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, Y_train, X_val, Y_val, criterion, optimizer, n_epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     52\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 54\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses,val_losses,train_accur,val_accur\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m     98\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pairs = None\n",
    "\n",
    "with open('language-pairs.json', 'r') as f:\n",
    "    pairs = json.loads(f.read())\n",
    "\n",
    "for pair in pairs:\n",
    "    print(pair)\n",
    "    L1 = pairs[pair]['target']['name']\n",
    "    L2 = pairs[pair]['source']['name']\n",
    "    \n",
    "    # load datasets\n",
    "    prefix = f'production_train_test/{L1}-{L2}'\n",
    "    \n",
    "    train_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')\n",
    "\n",
    "    # get and pad PanPhon features for alldata split\n",
    "    get_panphon_features(train_alldata, test_alldata)\n",
    "    alldata_maxlen = (max(np.max(train_alldata['features_loan'].str.len()),\\\n",
    "                          np.max(test_alldata['features_loan'].str.len())),\\\n",
    "                      max(np.max(train_alldata['features_orig'].str.len()),\\\n",
    "                          np.max(test_alldata['features_orig'].str.len())))\n",
    "    pad_panphon_features(train_alldata, test_alldata, alldata_maxlen)\n",
    "\n",
    "    # add target labels\n",
    "    Y_train, Y_test = add_target_labels(train_alldata, test_alldata)\n",
    "\n",
    "    # make train and val splits\n",
    "    X_train, X_val, X_test, Y_train, Y_val = make_train_val_set(train_alldata, test_alldata, Y_train)\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = make_tensors(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "\n",
    "    # instantiate network\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nUsing {device} device\\n\")\n",
    "    \n",
    "    # set random seeds for reproducibility\n",
    "    np.random.seed(666)\n",
    "\n",
    "    model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "    print(model,\"\\n\")\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # train and plot losses, accuracy\n",
    "    train_losses, val_losses, train_accur, val_accur = \\\n",
    "        model.fit(X_train, Y_train, X_val, Y_val, criterion, optimizer)\n",
    "    model.plot_losses(train_losses,val_losses,train_accur,val_accur)\n",
    "\n",
    "    # get and pad PanPhon features for realdist and balanced splits\n",
    "    get_panphon_features(train_realdist,test_realdist)\n",
    "    pad_panphon_features(train_realdist,test_realdist,alldata_maxlen)\n",
    "\n",
    "    get_panphon_features(train_balanced,test_balanced)\n",
    "    pad_panphon_features(train_balanced,test_balanced,alldata_maxlen)\n",
    "\n",
    "    # create data to get logits for\n",
    "    X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                         np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "    X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                        np.array([x for x in test_alldata['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                         np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "    X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                        np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                         np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "    X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                        np.array([x for x in test_balanced['features_orig']])])).to(device)\n",
    "\n",
    "    # place model in eval mode and get logits from DNN for all datasets/splits\n",
    "    print(\"Getting logits from DNN\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_logits_dnn_alldata = model(X_train_alldata.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_alldata = model(X_test_alldata.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_realdist = model(X_train_realdist.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_realdist = model(X_test_realdist.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_balanced = model(X_train_balanced.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_balanced = model(X_test_balanced.float())[1].detach().cpu().numpy()\n",
    "\n",
    "    # remove PanPhon features from dataframe and add logits column\n",
    "    train_alldata = train_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_alldata['DNN_logits'] = train_logits_dnn_alldata\n",
    "\n",
    "    test_alldata = test_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_alldata['DNN_logits'] = test_logits_dnn_alldata\n",
    "\n",
    "    train_realdist = train_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_realdist['DNN_logits'] = train_logits_dnn_realdist\n",
    "\n",
    "    test_realdist = test_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_realdist['DNN_logits'] = test_logits_dnn_realdist\n",
    "\n",
    "    train_balanced = train_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_balanced['DNN_logits'] = train_logits_dnn_balanced\n",
    "\n",
    "    test_balanced = test_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_balanced['DNN_logits'] = test_logits_dnn_balanced\n",
    "\n",
    "    #set the seeds for reproducibility even though we are not fine-tuning or training and the weights \n",
    "    #for both these models are effectively frozen for our purpose \n",
    "    torch.manual_seed(7)\n",
    "    random.seed(7)\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # Setting PyTorch's required configuration variables for reproducibility.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "    PRE_TRAINED_bert_MODEL = 'bert-base-multilingual-cased'\n",
    "    PRE_TRAINED_xlm_MODEL = 'xlm-mlm-100-1280'\n",
    "\n",
    "    MAXTOKENS = 5\n",
    "    BS = 8  # batch size\n",
    "\n",
    "    #list of loan-original words for train sets\n",
    "    l1_train_alldata = list(train_alldata[\"loan_word\"])\n",
    "    l2_train_alldata = list(train_alldata[\"original_word\"])\n",
    "\n",
    "    l1_train_realdist = list(train_realdist[\"loan_word\"])\n",
    "    l2_train_realdist = list(train_realdist[\"original_word\"])\n",
    "\n",
    "    l1_train_balanced = list(train_balanced[\"loan_word\"])\n",
    "    l2_train_balanced = list(train_balanced[\"original_word\"])\n",
    "\n",
    "    #list of loan-original words for test sets\n",
    "    l1_test_alldata = list(test_alldata[\"loan_word\"])\n",
    "    l2_test_alldata = list(test_alldata[\"original_word\"])\n",
    "\n",
    "    l1_test_realdist = list(test_realdist[\"loan_word\"])\n",
    "    l2_test_realdist = list(test_realdist[\"original_word\"])\n",
    "\n",
    "    l1_test_balanced = list(test_balanced[\"loan_word\"])\n",
    "    l2_test_balanced = list(test_balanced[\"original_word\"])\n",
    "\n",
    "    print(\"Getting MBERT similarities\")\n",
    "    train_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['MBERT_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    train_balanced['MBERT_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    test_realdist['MBERT_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    test_balanced['MBERT_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    print()\n",
    "    print(\"Getting XLM similarities\")\n",
    "    train_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['XLM_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    train_balanced['XLM_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "\n",
    "    test_realdist['XLM_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    test_balanced['XLM_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "        \n",
    "    train_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f72eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_test_alldata,l2_test_alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced['MBERT_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23111eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(train_realdist.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b372a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = None\n",
    "\n",
    "with open('../language-pairs.json', 'r') as f: # for getting logits from all languages\n",
    " \n",
    "    \n",
    "    pairs = json.loads(f.read())\n",
    "\n",
    "for pair in pairs:\n",
    "    print(pair)\n",
    "    L1 = pairs[pair]['target']['name']\n",
    "    L2 = pairs[pair]['source']['name']\n",
    "    \n",
    "    # load datasets\n",
    "    prefix = f'../Datasets/production_train_test/{L1}-{L2}'\n",
    "    \n",
    "    train_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata = pd.read_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist = pd.read_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced = pd.read_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')\n",
    "\n",
    "    # get and pad PanPhon features for alldata split\n",
    "    get_panphon_features(train_alldata, test_alldata)\n",
    "    alldata_maxlen = (max(np.max(train_alldata['features_loan'].str.len()),\\\n",
    "                          np.max(test_alldata['features_loan'].str.len())),\\\n",
    "                      max(np.max(train_alldata['features_orig'].str.len()),\\\n",
    "                          np.max(test_alldata['features_orig'].str.len())))\n",
    "    pad_panphon_features(train_alldata, test_alldata, alldata_maxlen)\n",
    "\n",
    "    # add target labels\n",
    "    Y_train, Y_test = add_target_labels(train_alldata, test_alldata)\n",
    "\n",
    "    # make train and val splits\n",
    "    X_train, X_val, X_test, Y_train, Y_val = make_train_val_set(train_alldata, test_alldata, Y_train)\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = make_tensors(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "\n",
    "    # instantiate network\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nUsing {device} device\\n\")\n",
    "    \n",
    "    # set random seeds for reproducibility\n",
    "    np.random.seed(666)\n",
    "\n",
    "    model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "    print(model,\"\\n\")\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # train and plot losses, accuracy\n",
    "    train_losses, val_losses, train_accur, val_accur = \\\n",
    "        model.fit(X_train, Y_train, X_val, Y_val, criterion, optimizer)\n",
    "    model.plot_losses(train_losses,val_losses,train_accur,val_accur)\n",
    "\n",
    "    # get and pad PanPhon features for realdist and balanced splits\n",
    "    get_panphon_features(train_realdist,test_realdist)\n",
    "    pad_panphon_features(train_realdist,test_realdist,alldata_maxlen)\n",
    "\n",
    "    get_panphon_features(train_balanced,test_balanced)\n",
    "    pad_panphon_features(train_balanced,test_balanced,alldata_maxlen)\n",
    "\n",
    "    # create data to get logits for\n",
    "    X_train_alldata = torch.tensor(np.hstack([np.array([x for x in train_alldata['features_loan']]),\\\n",
    "                         np.array([x for x in train_alldata['features_orig']])])).to(device)\n",
    "    X_test_alldata = torch.tensor(np.hstack([np.array([x for x in test_alldata['features_loan']]),\\\n",
    "                        np.array([x for x in test_alldata['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_realdist = torch.tensor(np.hstack([np.array([x for x in train_realdist['features_loan']]),\\\n",
    "                         np.array([x for x in train_realdist['features_orig']])])).to(device)\n",
    "    X_test_realdist = torch.tensor(np.hstack([np.array([x for x in test_realdist['features_loan']]),\\\n",
    "                        np.array([x for x in test_realdist['features_orig']])])).to(device)\n",
    "\n",
    "    X_train_balanced = torch.tensor(np.hstack([np.array([x for x in train_balanced['features_loan']]),\\\n",
    "                         np.array([x for x in train_balanced['features_orig']])])).to(device)\n",
    "    X_test_balanced = torch.tensor(np.hstack([np.array([x for x in test_balanced['features_loan']]),\\\n",
    "                        np.array([x for x in test_balanced['features_orig']])])).to(device)\n",
    "\n",
    "    # place model in eval mode and get logits from DNN for all datasets/splits\n",
    "    print(\"Getting logits from DNN\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_logits_dnn_alldata = model(X_train_alldata.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_alldata = model(X_test_alldata.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_realdist = model(X_train_realdist.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_realdist = model(X_test_realdist.float())[1].detach().cpu().numpy()\n",
    "        train_logits_dnn_balanced = model(X_train_balanced.float())[1].detach().cpu().numpy()\n",
    "        test_logits_dnn_balanced = model(X_test_balanced.float())[1].detach().cpu().numpy()\n",
    "\n",
    "    # remove PanPhon features from dataframe and add logits column\n",
    "    train_alldata = train_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_alldata['DNN_logits'] = train_logits_dnn_alldata\n",
    "\n",
    "    test_alldata = test_alldata.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_alldata['DNN_logits'] = test_logits_dnn_alldata\n",
    "\n",
    "    train_realdist = train_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_realdist['DNN_logits'] = train_logits_dnn_realdist\n",
    "\n",
    "    test_realdist = test_realdist.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_realdist['DNN_logits'] = test_logits_dnn_realdist\n",
    "\n",
    "    train_balanced = train_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    train_balanced['DNN_logits'] = train_logits_dnn_balanced\n",
    "\n",
    "    test_balanced = test_balanced.drop(['features_loan','features_orig'], axis=1)\n",
    "    test_balanced['DNN_logits'] = test_logits_dnn_balanced\n",
    "\n",
    "    #set the seeds for reproducibility even though we are not fine-tuning or training and the weights \n",
    "    #for both these models are effectively frozen for our purpose \n",
    "    torch.manual_seed(7)\n",
    "    random.seed(7)\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # Setting PyTorch's required configuration variables for reproducibility.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "    PRE_TRAINED_bert_MODEL = 'bert-base-multilingual-cased'\n",
    "    PRE_TRAINED_xlm_MODEL = 'xlm-mlm-100-1280'\n",
    "\n",
    "    MAXTOKENS = 5\n",
    "    MAXTOKENS_XLM = 9\n",
    "    BS = 8  # batch size\n",
    "\n",
    "    #list of loan-original words for train sets\n",
    "    l1_train_alldata = list(train_alldata[\"loan_word\"])\n",
    "    l2_train_alldata = list(train_alldata[\"original_word\"])\n",
    "\n",
    "    l1_train_realdist = list(train_realdist[\"loan_word\"])\n",
    "    l2_train_realdist = list(train_realdist[\"original_word\"])\n",
    "\n",
    "    l1_train_balanced = list(train_balanced[\"loan_word\"])\n",
    "    l2_train_balanced = list(train_balanced[\"original_word\"])\n",
    "\n",
    "    #list of loan-original words for test sets\n",
    "    l1_test_alldata = list(test_alldata[\"loan_word\"])\n",
    "    l2_test_alldata = list(test_alldata[\"original_word\"])\n",
    "\n",
    "    l1_test_realdist = list(test_realdist[\"loan_word\"])\n",
    "    l2_test_realdist = list(test_realdist[\"original_word\"])\n",
    "\n",
    "    l1_test_balanced = list(test_balanced[\"loan_word\"])\n",
    "    l2_test_balanced = list(test_balanced[\"original_word\"])\n",
    "\n",
    "    print(\"Getting MBERT similarities\")\n",
    "    train_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['MBERT_cos_sim'] = get_mbert_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['MBERT_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    train_balanced['MBERT_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    test_realdist['MBERT_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "    test_balanced['MBERT_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['MBERT_cos_sim']\n",
    "\n",
    "    print()\n",
    "    print(\"Getting XLM similarities\")\n",
    "    train_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_train_alldata,l2_train_alldata)\n",
    "    test_alldata['XLM_cos_sim'] = get_xlm_cos_sims(l1_test_alldata,l2_test_alldata)\n",
    "\n",
    "    train_realdist['XLM_cos_sim'] = train_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    train_balanced['XLM_cos_sim'] = train_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                           on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "\n",
    "    test_realdist['XLM_cos_sim'] = test_realdist.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "    test_balanced['XLM_cos_sim'] = test_balanced.merge(pd.concat([train_alldata,test_alldata]),\\\n",
    "                                                         on=['loan_word','original_word'], how=\"left\")['XLM_cos_sim']\n",
    "        \n",
    "    train_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-train_production_alldata.csv')\n",
    "    test_alldata.to_csv(f'{prefix}/alldata/{L1}-{L2}-test_production_alldata.csv')\n",
    "\n",
    "    train_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-train_production_realdist.csv')\n",
    "    test_realdist.to_csv(f'{prefix}/realdist/{L1}-{L2}-test_production_realdist.csv')\n",
    "\n",
    "    train_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-train_production_balanced.csv')\n",
    "    test_balanced.to_csv(f'{prefix}/balanced/{L1}-{L2}-test_production_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadc6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
