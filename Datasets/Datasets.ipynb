{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnQ4Pf0UX34X"
   },
   "source": [
    "Run one of the next cells, depending on language pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qnaq9qM8vRx3"
   },
   "outputs": [],
   "source": [
    "L1 = \"Hindi\"\n",
    "L2 = \"Persian\"\n",
    "L1_epi = \"hin-Deva\"\n",
    "L2_epi = \"fas-Arab\"\n",
    "L1_gtrans = \"hi\"\n",
    "L2_gtrans = \"fa\"\n",
    "L1_unicode = \"Devanagari\"\n",
    "L2_unicode = \"Arabic\"\n",
    "synonym_file_name = \"English-Hindi-Syn-Ant.csv\"\n",
    "colab = False # Change to False if not using google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BBuV_RPPTBCM"
   },
   "outputs": [],
   "source": [
    "L1 = \"English\"\n",
    "L2 = \"French\"\n",
    "L1_epi = \"eng-Latn\"\n",
    "L2_epi = \"fra-Latn\"\n",
    "L1_gtrans = \"en\"\n",
    "L2_gtrans = \"fr\"\n",
    "L1_unicode = \"Latin\"\n",
    "L2_unicode = \"Latin\"\n",
    "synonym_file_name = \"English-Hindi-Syn-Ant.csv\"\n",
    "colab = False # Change to False if not using google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgcepdoRE22Q"
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7808f6cc",
    "outputId": "8942db66-f179-494d-8e1e-aa60b8b03bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Epitran in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.17)\n",
      "Requirement already satisfied: marisa-trie in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (0.7.7)\n",
      "Requirement already satisfied: panphon>=0.19 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (0.19.1)\n",
      "Requirement already satisfied: requests in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (2.27.1)\n",
      "Requirement already satisfied: regex in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (2022.3.15)\n",
      "Requirement already satisfied: unicodecsv in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (0.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Epitran) (58.1.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon>=0.19->Epitran) (6.0)\n",
      "Requirement already satisfied: editdistance in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon>=0.19->Epitran) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.20.2 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon>=0.19->Epitran) (1.21.5)\n",
      "Requirement already satisfied: munkres in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon>=0.19->Epitran) (1.1.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->Epitran) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->Epitran) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->Epitran) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->Epitran) (2.0.12)\n",
      "Requirement already satisfied: eng_to_ipa in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: panphon in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (58.1.0)\n",
      "Requirement already satisfied: munkres in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (1.1.4)\n",
      "Requirement already satisfied: unicodecsv in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (0.14.1)\n",
      "Requirement already satisfied: regex in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (2022.3.15)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (6.0)\n",
      "Requirement already satisfied: numpy>=1.20.2 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (1.21.5)\n",
      "Requirement already satisfied: editdistance in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from panphon) (0.6.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: googletrans in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from googletrans) (0.13.3)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (2021.10.8)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans) (2021.12.1)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.12.1)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Requirement already satisfied: unicodeblock in c:\\users\\benkh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Epitran\n",
    "!pip install eng_to_ipa\n",
    "!pip install panphon\n",
    "!pip install -U PyYAML\n",
    "!pip install googletrans\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install unicodeblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1QGbqVyEwBL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "03c198c0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import editdistance # levenshtein\n",
    "import epitran\n",
    "import eng_to_ipa as eng\n",
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "import itertools\n",
    "import unicodeblock.blocks\n",
    "# epitran.download.cedict() # required for Mandarin Chinese (could be replace by cedict_file='cedict.txt' as argument)\n",
    "if colab:\n",
    "     from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXdHobNKvAc3"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bLKUl-OoAnm1"
   },
   "outputs": [],
   "source": [
    "dst = panphon.distance.Distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nFwW2Fu15Fwi"
   },
   "outputs": [],
   "source": [
    "edit_dists = [dst.fast_levenshtein_distance_div_maxlen, dst.dolgo_prime_distance_div_maxlen, \\\n",
    "              dst.feature_edit_distance_div_maxlen, dst.hamming_feature_edit_distance_div_maxlen, dst.weighted_feature_edit_distance_div_maxlen, \\\n",
    "              dst.partial_hamming_feature_edit_distance_div_maxlen]\n",
    "edit_dists_names = [\"Fast Levenshtein Distance Div Maxlen\", \"Dolgo Prime Distance Div Maxlen\", \\\n",
    "                    \"Feature Edit Distance Div Maxlen\", \"Hamming Feature Distance Div Maxlen\", \"Weighted Feature Distance Div Maxlen\", \\\n",
    "                    \"Partial Hamming Feature Distance Div Maxlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XRJjSKMtRr09"
   },
   "outputs": [],
   "source": [
    "l = ['loan_word', 'original_word', 'loan_word_epitran', 'original_word_epitran', 'loan_english', 'original_english', 'Fast Levenshtein Distance Div Maxlen', 'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen', 'Hamming Feature Distance Div Maxlen', 'Weighted Feature Distance Div Maxlen', 'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein', 'loan_unicode', 'original_unicode', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pBZFLYe9B9Xz"
   },
   "outputs": [],
   "source": [
    "def filter(test_str, all_freq):\n",
    "    for i in test_str:\n",
    "        if i.lower() in all_freq:\n",
    "            all_freq[i.lower()] += 1\n",
    "        else:\n",
    "            all_freq[i.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "75K5w0nhP-BT"
   },
   "outputs": [],
   "source": [
    "L1_L2 = pd.read_csv(f\"{L1}-{L2}.csv\")\n",
    "L1_L2['original_word'] = L1_L2['original_word'].astype(str)\n",
    "L1_L2['loan_word'] = L1_L2['loan_word'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DyQvbpoTPylS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARABIC',\n",
       " 'BASIC_LATIN',\n",
       " 'BASIC_PUNCTUATION',\n",
       " 'DEVANAGARI',\n",
       " 'GENERAL_PUNCTUATION',\n",
       " 'GURMUKHI',\n",
       " 'SPACE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=set()\n",
    "for i in range(L1_L2.shape[0]):\n",
    "    for letter in L1_L2.original_word.iloc[i]:\n",
    "        s.add(unicodeblock.blocks.of(letter))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RpCdYL3nP1nO"
   },
   "outputs": [],
   "source": [
    "L1_uni_blocks = {'BASIC_LATIN', 'BASIC_PUNCTUATION', 'LATIN_EXTENDED_LETTER', 'SPACE'}\n",
    "L2_uni_blocks = {'BASIC_LATIN', 'BASIC_PUNCTUATION', 'LATIN_EXTENDED_A', 'LATIN_EXTENDED_LETTER', 'SPACE'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoCiy00Cuw2U"
   },
   "source": [
    "# Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "75Qt04xgu5qC"
   },
   "outputs": [],
   "source": [
    "def get_L1_L2(L1, L1_epi, L2, L2_epi):\n",
    "    L1_L2 = pd.read_csv(f\"{L1}-{L2}.csv\")\n",
    "    L1_L2['original_word'] = L1_L2['original_word'].astype(str)\n",
    "    L1_L2['loan_word'] = L1_L2['loan_word'].astype(str)\n",
    "    L1_L2 = L1_L2.dropna()\n",
    "    freq_L1 = dict()\n",
    "    freq_L2 = dict()\n",
    "    L1_L2[\"loan_word\"].apply(lambda x:filter(x, freq_L1))\n",
    "    L1_L2[\"original_word\"].apply(lambda x:filter(x, freq_L2))\n",
    "    L1_L2[\"plain Levenshtein\"] = L1_L2.apply(lambda x:dst.fast_levenshtein_distance(x[\"loan_word\"], x[\"original_word\"]), axis=1)\n",
    "    alpha_L1 = [key for key, value in freq_L1.items() if value > 10]\n",
    "    alpha_L2 = [key for key, value in freq_L2.items() if value > 10]\n",
    "    drop = []\n",
    "    for row in range(L1_L2.shape[0]):\n",
    "        if L1_L2.iloc[row][\"loan_word\"] == 'nan':\n",
    "            drop.append(row)\n",
    "        if L1_L2.iloc[row][\"original_word\"] == 'nan':\n",
    "            drop.append(row)\n",
    "\n",
    "        for letter in L1_L2.iloc[row][\"loan_word\"]:\n",
    "            if letter not in alpha_L1 and unicodeblock.blocks.of(letter) not in L1_uni_blocks:\n",
    "                drop.append(row)\n",
    "                break\n",
    "        for letter in L1_L2.iloc[row][\"original_word\"]:\n",
    "            if letter not in alpha_L2 and unicodeblock.blocks.of(letter) not in L2_uni_blocks:\n",
    "                drop.append(row)\n",
    "        if L1_L2.iloc[row][\"plain Levenshtein\"] > 25:\n",
    "                drop.append(row)\n",
    "    L1_L2 = L1_L2.loc[~L1_L2.index.isin(drop)].reset_index(drop=True)\n",
    "    \n",
    "    if L1_epi == \"eng-Latn\":\n",
    "        L1_L2[\"loan_word_epitran\"] = L1_L2.apply(lambda x:eng.convert(x[\"loan_word\"]), axis=1)\n",
    "    else:\n",
    "        epi = epitran.Epitran(L1_epi)\n",
    "        L1_L2[\"loan_word_epitran\"] = L1_L2.apply(lambda x:epi.transliterate(x[\"loan_word\"]), axis=1)\n",
    "\n",
    "    if L2_epi == \"eng-Latn\":\n",
    "        L1_L2[\"original_word_epitran\"] = L1_L2.apply(lambda x:eng.convert(x[\"original_word\"]), axis=1)\n",
    "    \n",
    "    else:\n",
    "        epi = epitran.Epitran(L2_epi)\n",
    "        L1_L2[\"original_word_epitran\"] = L1_L2.apply(lambda x:epi.transliterate(x[\"original_word\"]), axis=1)\n",
    "\n",
    "    \n",
    "    translator = Translator()\n",
    "    L1_L2[\"loan_english\"] = L1_L2.apply(lambda x: translator.translate(x[\"loan_word\"], dest=\"en\").text, axis=1)\n",
    "    L1_L2[\"original_english\"] = L1_L2.apply(lambda x: translator.translate(x[\"original_word\"], dest=\"en\").text, axis=1)\n",
    "\n",
    "    L1_L2[\"label\"] = \"loan\"\n",
    "    L1_L2['original_word_epitran'] = L1_L2['original_word_epitran'].apply(lambda x: x.replace(\"پ\", \"p\").replace(\"ك\", \"g\").replace(\"ي\", \"j\").replace(\"گ\", \"g\").replace(\"چ\", \"tʃ\").replace(\"ژ\", \"ʒ\").replace(\"ء\", \"ʔ\"))\n",
    "    L1_L2 = L1_L2.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    for dist, name  in zip(edit_dists, edit_dists_names):\n",
    "        L1_L2[name] = L1_L2.apply(lambda x:dist(x[\"loan_word_epitran\"], x[\"original_word_epitran\"]), axis=1)\n",
    "    L1_L2['loan_unicode'] = L1_unicode\n",
    "    L1_L2['original_unicode'] = L2_unicode\n",
    "    L1_L2 = L1_L2[l]\n",
    "    return L1_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EbcTeuUU5Pqa"
   },
   "outputs": [],
   "source": [
    "L1_L2 = get_L1_L2(L1, L1_epi, L2, L2_epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--ZZDnZeTBtI"
   },
   "outputs": [],
   "source": [
    "L1_L2.to_csv(f\"{L1}-{L2}-Loans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIcyuzav_mpe"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    files.download(f\"{L1}-{L2}-Loans.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhFr9Jmhv-oM"
   },
   "source": [
    "# Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VhMxarPCv_ui"
   },
   "outputs": [],
   "source": [
    "def get_L1_L2_Synonyms(L1_epi, L2_epi, L1_gtrans, L2_gtrans, file_name=synonym_file_name):\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    translator = Translator()\n",
    "\n",
    "    l1, l2 = [], []\n",
    "    for j in range(len(df)):\n",
    "        l = df.iloc[j][\"Synonym\"].split(', ')\n",
    "        a = translator.translate(l, dest=L1_gtrans)\n",
    "        b = translator.translate(l, dest=L2_gtrans)\n",
    "        for i in range(len(a)):\n",
    "            a[i] = a[i].text\n",
    "            b[i] = b[i].text\n",
    "        pair_l = list(itertools.product(a,b))\n",
    "        l1 += [e[0] for e in pair_l]\n",
    "        l2 += [e[1] for e in pair_l]\n",
    "    L1_L2_Synonyms = pd.DataFrame(list(zip(l1, l2)), columns =['loan_word', 'original_word'])\n",
    "    L1_L2 = pd.read_csv(f\"{L1}-{L2}.csv\")\n",
    "    L1_L2['original_word'] = L1_L2['original_word'].astype(str)\n",
    "    L1_L2['loan_word'] = L1_L2['loan_word'].astype(str)\n",
    "    L1_L2_Synonyms = L1_L2_Synonyms.dropna()\n",
    "    freq_L1 = dict()\n",
    "    freq_L2 = dict()\n",
    "    L1_L2[\"loan_word\"].apply(lambda x:filter(x, freq_L1))\n",
    "    L1_L2[\"original_word\"].apply(lambda x:filter(x, freq_L2))\n",
    "    L1_L2_Synonyms[\"plain Levenshtein\"] = L1_L2_Synonyms.apply(lambda x:dst.fast_levenshtein_distance(x[\"loan_word\"], x[\"original_word\"]), axis=1)\n",
    "    alpha_L1 = [key for key, value in freq_L1.items() if value > 10]\n",
    "    alpha_L2 = [key for key, value in freq_L2.items() if value > 10]\n",
    "    drop = []\n",
    "    for row in range(L1_L2.shape[0]):\n",
    "        if L1_L2_Synonyms.iloc[row][\"loan_word\"] == 'nan':\n",
    "            drop.append(row)\n",
    "        if L1_L2_Synonyms.iloc[row][\"original_word\"] == 'nan':\n",
    "            drop.append(row)\n",
    "\n",
    "        for letter in L1_L2_Synonyms.iloc[row][\"loan_word\"]:\n",
    "            if letter not in alpha_L1 and unicodeblock.blocks.of(letter) not in L1_uni_blocks:\n",
    "                drop.append(row)\n",
    "                break\n",
    "        for letter in L1_L2_Synonyms.iloc[row][\"original_word\"]:\n",
    "            if letter not in alpha_L2 and unicodeblock.blocks.of(letter) not in L2_uni_blocks:\n",
    "                drop.append(row)\n",
    "        if L1_L2_Synonyms.iloc[row][\"plain Levenshtein\"] > 25:\n",
    "                drop.append(row)\n",
    "    L1_L2_Synonyms = L1_L2_Synonyms.loc[~L1_L2_Synonyms.index.isin(drop)].reset_index(drop=True)\n",
    "\n",
    "    L1_L2_Synonyms = L1_L2_Synonyms[(~L1_L2_Synonyms['loan_word'].isin(L1_L2['loan_word'])) | (~L1_L2_Synonyms['original_word'].isin(L1_L2['original_word']))]\n",
    "\n",
    "    if L1_epi == \"eng-Latn\":\n",
    "        L1_L2_Synonyms[\"loan_word_epitran\"] = L1_L2_Synonyms.apply(lambda x:eng.convert(x[\"loan_word\"]), axis=1)\n",
    "    else:\n",
    "        epi = epitran.Epitran(L1_epi)\n",
    "        L1_L2_Synonyms[\"loan_word_epitran\"] = L1_L2_Synonyms.apply(lambda x:epi.transliterate(x[\"loan_word\"]), axis=1)\n",
    "\n",
    "    if L2_epi == \"eng-Latn\":\n",
    "        L1_L2_Synonyms[\"original_word_epitran\"] = L1_L2_Synonyms.apply(lambda x:eng.convert(x[\"original_word\"]), axis=1)\n",
    "\n",
    "    else:\n",
    "        epi = epitran.Epitran(L2_epi)\n",
    "        L1_L2_Synonyms[\"original_word_epitran\"] = L1_L2_Synonyms.apply(lambda x:epi.transliterate(x[\"original_word\"]), axis=1)\n",
    "\n",
    "\n",
    "    L1_L2_Synonyms[\"loan_english\"] = L1_L2_Synonyms.apply(lambda x: translator.translate(x[\"loan_word\"], dest=\"en\").text, axis=1)\n",
    "    L1_L2_Synonyms[\"original_english\"] = L1_L2_Synonyms.apply(lambda x: translator.translate(x[\"original_word\"], dest=\"en\").text, axis=1)\n",
    "    L1_L2_Synonyms= L1_L2_Synonyms.drop_duplicates(ignore_index=True)\n",
    "    L1_L2_Synonyms['original_word_epitran'] = L1_L2_Synonyms['original_word_epitran'].apply(lambda x: x.replace(\"پ\", \"p\").replace(\"ء\", \"ʔ\").replace(\"ي\", \"j\").replace(\"گ\", \"g\").replace(\"ك\", \"g\").replace(\"چ\", \"tʃ\").replace(\"ژ\", \"ʒ\"))\n",
    "    for dist, name  in zip(edit_dists, edit_dists_names):\n",
    "        L1_L2_Synonyms[name] = L1_L2_Synonyms.apply(lambda x:dist(x[\"loan_word_epitran\"], x[\"original_word_epitran\"]), axis=1)\n",
    "    L1_L2_Synonyms[\"label\"] = \"synonym\"\n",
    "    l = ['loan_word', 'original_word', 'loan_word_epitran', 'original_word_epitran', 'loan_english', 'original_english', 'Fast Levenshtein Distance Div Maxlen', 'Dolgo Prime Distance Div Maxlen', 'Feature Edit Distance Div Maxlen', 'Hamming Feature Distance Div Maxlen', 'Weighted Feature Distance Div Maxlen', 'Partial Hamming Feature Distance Div Maxlen', 'plain Levenshtein', 'loan_unicode', 'original_unicode', 'label']    \n",
    "    L1_L2_Synonyms['loan_unicode'] = L1_unicode\n",
    "    L1_L2_Synonyms['original_unicode'] = L2_unicode\n",
    "    L1_L2_Synonyms = L1_L2_Synonyms[l]\n",
    "\n",
    "    return L1_L2_Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oJJDoKuESGo1"
   },
   "outputs": [],
   "source": [
    "L1_L2_Synonyms = get_L1_L2_Synonyms(L1_epi, L2_epi, L1_gtrans, L2_gtrans, synonym_file_name)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7n4vdndTNNx"
   },
   "outputs": [],
   "source": [
    "L1_L2_Synonyms.to_csv(f\"{L1}-{L2}-Synonyms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "5N7v_WHrAIg0",
    "outputId": "3004ee5d-04af-4a3e-da78-1b778a575741"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e1b8e4ad-0c6e-4d12-8c4a-6f490473add4\", \"Hindi-Persian-Synonyms.csv\", 1054011)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if colab:\n",
    "    files.download(f\"{L1}-{L2}-Synonyms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otwzFRa6wwAq"
   },
   "source": [
    "# Hard Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV_uoFomVcfa"
   },
   "outputs": [],
   "source": [
    "# vocab = pd.read_csv(\"dictionary.csv\")[\"form\"]\n",
    "vocab = L1_L2.original_word.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRNi9oM8p7LZ"
   },
   "outputs": [],
   "source": [
    "vocab = vocab.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQE_Lh3shtYW"
   },
   "outputs": [],
   "source": [
    "vocab = vocab.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFG2h8Hvw4Jx"
   },
   "outputs": [],
   "source": [
    "if L2_epi == \"eng-Latn\":\n",
    "    vocab_ipa = vocab.apply(lambda x: eng.convert(x))\n",
    "    \n",
    "else:\n",
    "    epi = epitran.Epitran(L2_epi)\n",
    "    vocab_ipa = vocab.apply(lambda x: epi.transliterate(x))\n",
    "vocab_ipa.to_csv(f\"{L2}-vocab-ipa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_RQTNmRw6C7"
   },
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "vocab_translated = vocab.apply(lambda x:translator.translate(x, dest=\"en\").text)\n",
    "vocab_translated.to_csv(f\"{L2}-vocab-translated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF5wc2t-DocR"
   },
   "outputs": [],
   "source": [
    "# vocab = vocab.iloc[::20].reset_index(drop=True)\n",
    "# vocab_ipa = vocab_ipa.iloc[::20].reset_index(drop=True)\n",
    "# vocab_translated = vocab_translated.iloc[::20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQEllqYlxH5p"
   },
   "outputs": [],
   "source": [
    "def closest_neighbour_epi(anch, distance, df, vocab):\n",
    "    # returns the row id of the closest neighbor in terms of \"distance\"\n",
    "    min_dist = 10000\n",
    "    argmin = 0\n",
    "    for i in range(len(vocab)):\n",
    "        tmp_dist = distance(df.iloc[anch][\"loan_word_epitran\"], vocab_ipa[i])\n",
    "        if (tmp_dist < min_dist) and (df.iloc[anch][\"original_word\"] != vocab[i]):\n",
    "            argmin = i\n",
    "            min_dist = tmp_dist\n",
    "    return argmin, min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6dbSX9Zyxhh"
   },
   "outputs": [],
   "source": [
    "def get_Hard_Negatives():\n",
    "    L1_L2_Hard_Negatives = L1_L2.head(0).copy()\n",
    "    for edit_dist, edit_dist_name in zip(edit_dists, edit_dists_names):\n",
    "        for row in range(len(L1_L2)):\n",
    "            if row%50 == 0 : \n",
    "                print(row)\n",
    "            i, _ = closest_neighbour_epi(row, edit_dist, L1_L2, vocab)\n",
    "            if L1_L2.iloc[row][\"original_word\"] != vocab[i]:            \n",
    "                tmp_df = pd.DataFrame({'loan_word': L1_L2.iloc[row][\"loan_word\"],\n",
    "                                    'original_word' : vocab[i],\n",
    "                                    'loan_word_epitran' : L1_L2.iloc[row][\"loan_word_epitran\"],\n",
    "                                    'original_word_epitran' : vocab_ipa[i],\n",
    "                                    'loan_english' : L1_L2.iloc[row][\"loan_english\"],\n",
    "                                    'original_english' : [vocab_translated[i]]})\n",
    "                L1_L2_Hard_Negatives = L1_L2_Hard_Negatives.append(tmp_df, ignore_index=True)\n",
    "    L1_L2_Hard_Negatives = L1_L2_Hard_Negatives[(~L1_L2_Hard_Negatives['loan_word'].isin(L1_L2_Synonyms['loan_word'])) | (~L1_L2_Hard_Negatives['original_word'].isin(L1_L2_Synonyms['original_word']))]\n",
    "    L1_L2_Hard_Negatives[\"label\"]=\"hard_negative\"\n",
    "    L1_L2_Hard_Negatives['loan_unicode'] = L1_unicode\n",
    "    L1_L2_Hard_Negatives['original_unicode'] = L2_unicode\n",
    "    L1_L2_Hard_Negatives = L1_L2_Hard_Negatives.drop_duplicates(ignore_index=True)\n",
    "    L1_L2_Hard_Negatives['original_word_epitran'] = L1_L2_Hard_Negatives['original_word_epitran'].apply(lambda x: x.replace(\"پ\", \"p\").replace(\"ء\", \"ʔ\").replace(\"ي\", \"j\").replace(\"ك\", \"g\").replace(\"گ\", \"g\").replace(\"چ\", \"tʃ\").replace(\"ژ\", \"ʒ\"))\n",
    "    for dist, name  in zip(edit_dists, edit_dists_names):\n",
    "        L1_L2_Hard_Negatives[name] = L1_L2_Hard_Negatives.apply(lambda x:dist(x[\"loan_word_epitran\"], x[\"original_word_epitran\"]), axis=1)\n",
    "    L1_L2_Hard_Negatives[\"plain Levenshtein\"] = L1_L2_Hard_Negatives.apply(lambda x:dst.fast_levenshtein_distance(x[\"loan_word\"], x[\"original_word\"]), axis=1)\n",
    "    L1_L2_Hard_Negatives = L1_L2_Hard_Negatives[l]\n",
    "    return L1_L2_Hard_Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfhFgQ-KSXDb"
   },
   "outputs": [],
   "source": [
    "L1_L2_Hard_Negatives = get_Hard_Negatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUFbW9HZS2iB"
   },
   "outputs": [],
   "source": [
    "L1_L2_Hard_Negatives.to_csv(f\"{L1}-{L2}-Hard-Negatives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URE4SBusBl_4"
   },
   "outputs": [],
   "source": [
    "colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dHTfxAoAMKA"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    files.download(f\"{L1}-{L2}-Hard-Negatives.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WyqpJqBvtjX"
   },
   "source": [
    "# Randoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "uhNTD39avzO4"
   },
   "outputs": [],
   "source": [
    "def get_L1_L2_Randoms(L1_L2):\n",
    "    L1_L2['loan_word'] = L1_L2['loan_word'].astype(str)\n",
    "    L1_L2['original_word'] = L1_L2['original_word'].astype(str)\n",
    "    L1_L2_Randoms = L1_L2.copy()\n",
    "    L1_L2_Randoms = L1_L2_Randoms.drop(edit_dists_names , axis=1)\n",
    "    idx = np.random.permutation(L1_L2_Randoms.index)\n",
    "    L1_L2_Randoms[\"original_word\"] = L1_L2_Randoms[\"original_word\"].reindex(idx).reset_index(drop=True)\n",
    "    L1_L2_Randoms[\"original_english\"] = L1_L2_Randoms[\"original_english\"].reindex(idx).reset_index(drop=True)\n",
    "    L1_L2_Randoms[\"original_word_epitran\"] = L1_L2_Randoms[\"original_word_epitran\"].reindex(idx).reset_index(drop=True)\n",
    "    L1_L2_Randoms['original_word_epitran'] = L1_L2_Randoms['original_word_epitran'].apply(lambda x: x.replace(\"پ\", \"p\").replace(\"ي\", \"j\").replace(\"ك\", \"g\").replace(\"گ\", \"g\").replace(\"چ\", \"tʃ\").replace(\"ژ\", \"ʒ\").replace(\"ء\", \"ʔ\"))\n",
    "    L1_L2_Randoms = L1_L2_Randoms.iloc[(~L1_L2_Randoms[['loan_word', 'original_word']].isin(L1_L2[['loan_word', 'original_word']])).index]\n",
    "    L1_L2_Randoms = L1_L2_Randoms.iloc[(~L1_L2_Randoms[['loan_word', 'original_word']].isin(L1_L2_Synonyms[['loan_word', 'original_word']])).index]\n",
    "    L1_L2_Randoms = L1_L2_Randoms.iloc[(~L1_L2_Randoms[['loan_word', 'original_word']].isin(L1_L2_Hard_Negatives[['loan_word', 'original_word']])).index]\n",
    "    L1_L2_Randoms = L1_L2_Randoms.drop_duplicates(ignore_index=True)\n",
    "    for dist, name  in zip(edit_dists, edit_dists_names):\n",
    "        L1_L2_Randoms[name] = L1_L2_Randoms.apply(lambda x:dist(x[\"loan_word_epitran\"], x[\"original_word_epitran\"]), axis=1)\n",
    "    L1_L2_Randoms[\"plain Levenshtein\"] = L1_L2_Randoms.apply(lambda x:dst.fast_levenshtein_distance(x[\"loan_word\"], x[\"original_word\"]), axis=1)\n",
    "    L1_L2_Randoms[\"label\"] = \"random\"\n",
    "    L1_L2_Randoms.reset_index(drop=True, inplace=True)\n",
    "    L1_L2_Randoms = L1_L2_Randoms[l]\n",
    "    return L1_L2_Randoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "l0zcxTQI5Ybl"
   },
   "outputs": [],
   "source": [
    "L1_L2_Randoms = get_L1_L2_Randoms(L1_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DomI11QQTILk"
   },
   "outputs": [],
   "source": [
    "L1_L2_Randoms.to_csv(f\"{L1}-{L2}-Randoms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ro8Sc5iAGWV"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    files.download(f\"{L1}-{L2}-Randoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_L2 = pd.read_csv(f\"{L1}-{L2}-Loans.csv\")\n",
    "L1_L2_Synonyms = pd.read_csv(f\"{L1}-{L2}-Synonyms.csv\")\n",
    "L1_L2_Hard_Negatives = pd.read_csv(f\"{L1}-{L2}-Hard-Negatives.csv\")\n",
    "L1_L2_Randoms = pd.read_csv(f\"{L1}-{L2}-Randoms.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IgcepdoRE22Q",
    "r1QGbqVyEwBL",
    "QXdHobNKvAc3",
    "UoCiy00Cuw2U",
    "nhFr9Jmhv-oM",
    "otwzFRa6wwAq",
    "2WyqpJqBvtjX"
   ],
   "name": "16_Datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
