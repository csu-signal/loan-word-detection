{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import unicodeblock.blocks\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hindi-Persian': 'https://en.m.wiktionary.org/wiki/Category:Hindi_terms_borrowed_from_Persian',\n",
       " 'English-French': 'https://en.m.wiktionary.org/wiki/Category:English_terms_borrowed_from_French'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = {}\n",
    "\n",
    "with open('../language-pairs.json', 'r') as f:\n",
    "    pairs = json.loads(f.read())\n",
    "    \n",
    "    for pair in pairs:\n",
    "        links[pair] = pairs[pair]['wiki']\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_borrowed_words(dest, source, invalid=[\"Unsupported titles/Space\"]):\n",
    "    title = f\"Category:{dest}_terms_borrowed_from_{source}\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'exintro': True,\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    url = f\"https://en.wiktionary.org/w/api.php?action=query&list=categorymembers&cmtitle={title}&cmlimit=max\"\n",
    "    \n",
    "    borrowed_words = []\n",
    "    while(True):\n",
    "        r = requests.get(url,params)\n",
    "        try:\n",
    "            cmcontinue = r.json()['continue']['cmcontinue']\n",
    "            for cmember in r.json()['query']['categorymembers']:\n",
    "                if len(cmember['title']) > 1 and \\\n",
    "                    not cmember['title'].startswith('-') and \\\n",
    "                    not cmember['title'].endswith('-') and \\\n",
    "                    cmember['title'] not in invalid and \\\n",
    "                    unicodeblock.blocks.of(cmember['title'][0]) not in ['DIGIT', 'BASIC_PUNCTUATION']:\n",
    "                    borrowed_words.append(cmember['title'])\n",
    "                    if len(borrowed_words) % 1000 == 0:\n",
    "                        print(f\"Got {len(borrowed_words)}\")\n",
    "            url = url.split(\"&cmcontinue\")[0]\n",
    "            url+=f\"&cmcontinue={cmcontinue}\"\n",
    "        except KeyError:\n",
    "            break\n",
    "    print(f\"Done, {len(borrowed_words)}\")\n",
    "    return borrowed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_all_loans_and_false_friends:\n",
    "    for each word in borrowed words:\n",
    "        1. get word page from wiktionary\n",
    "        2. extract source word from source language\n",
    "        3. extract false friends from language!=source language\n",
    "\n",
    "    return loan_pairs, false_friends\n",
    "'''\n",
    "\n",
    "def get_source_word(soup, src_lang, lst_invalid_words):\n",
    "    for src_soup in soup.find_all(\"span\", class_=\"etyl\"):\n",
    "        if src_soup != None and src_soup.find(lambda tag: tag.name == 'a' and src_lang.lower() in tag.text.lower()):\n",
    "            \n",
    "            src_soup_final = src_soup.find_next(\"i\")\n",
    "            src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "            if src_word not in lst_invalid_words:\n",
    "                source_word = src_word\n",
    "            else:\n",
    "                src_soup_final = src_soup.find_next(\"strong\")\n",
    "                src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "                source_word = src_word\n",
    "            return source_word\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def get_false_friend(borrowed_word, soup, src_lang, dest_lang, lst_invalid_words):\n",
    "    all_false_friends = []\n",
    "    # print(\"here\")\n",
    "    for header in soup.find_all(\"h3\", class_=\"mw-headline\"):\n",
    "        # print(\"here1\")\n",
    "        if re.match(\"Etymology [1-9]\",header.text):\n",
    "            # print(\"here2\")\n",
    "            try:\n",
    "                if dest_lang == header.parent.parent.find_previous_siblings()[0].find_all(\"span\", class_=\"mw-headline\")[0].text:\n",
    "                    # print(\"here3\")\n",
    "                    if len(header.parent.find_next_siblings()) > 0:\n",
    "                        # print(\"here4\")\n",
    "                        if len(header.parent.find_next_siblings()[0].find_all(\"span\", class_=\"etyl\")) > 0:\n",
    "                            # print(\"here5\")\n",
    "                            etym = header.parent.find_next_siblings()[0].find_all(\"span\", class_=\"etyl\")[0]\n",
    "                            for tag in etym.find(lambda tag: tag.name == 'a'):\n",
    "                                # print(\"here6\")\n",
    "\n",
    "                                if src_lang.lower() not in tag.text.lower():\n",
    "                                    false_friend_soup = etym.find_next(\"i\")\n",
    "                                    meaning = header.parent.find_next_siblings('ol')[0].text.split('\\n')[0]\n",
    "                                    meaning = re.sub(r'\\(.+?\\)',r'',meaning)\n",
    "                                    meaning = re.sub(r'\\[.+?\\]',r'',meaning)\n",
    "                                    meaning = meaning.replace('  ', ' ')\n",
    "                                    meaning = re.split('[.;:]',meaning)[0].strip()\n",
    "                                    ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                    ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "                                    if ff_word in lst_invalid_words:\n",
    "                                        false_friend_soup = etym.find_next(\"strong\")\n",
    "                                        ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                        ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "\n",
    "                                    all_false_friends.append([borrowed_word,ff_word,tag.text.lower(),meaning])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return all_false_friends\n",
    "\n",
    "            \n",
    "def get_all_loans_and_false_friends(borrowed_words, dest_lang, src_lang, lst_invalid_words=['plural', 'not comparable', 'Urdu spelling', 'Urdu spelling']):\n",
    "\n",
    "    loan_pairs = [] # [[borrowed_word, source_word]]\n",
    "    false_friends = [] # \n",
    "    for word in tqdm(borrowed_words):\n",
    "\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'prop': 'extracts',\n",
    "            'exintro': True,\n",
    "            'explaintext': True,\n",
    "        }\n",
    "\n",
    "        url = 'https://en.wiktionary.org/w/rest.php/v1/page/' + word + '/html'\n",
    "        url = 'https://en.wiktionary.org/w/rest.php/v1/page/%E0%A4%85%E0%A4%97%E0%A4%B0/html'\n",
    "        # print(url)\n",
    "        response = requests.get(url,params)\n",
    "\n",
    "        # soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        source_word = get_source_word(\n",
    "            BeautifulSoup(response.content, 'html.parser'), src_lang, lst_invalid_words)\n",
    "        false_friend = get_false_friend(\n",
    "            word, BeautifulSoup(response.content, 'html.parser'), src_lang, dest_lang, lst_invalid_words)\n",
    "        # print(false_friend)\n",
    "        loan_pairs.append([word, source_word])\n",
    "        false_friends.extend(false_friend)\n",
    "    \n",
    "    return loan_pairs, false_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi-Persian\n",
      "Done, 984\n",
      "Getting loan pairs and false friends\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 984/984 [10:12<00:00,  1.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi-Persian done\n",
      "\n",
      "\n",
      "English-French\n",
      "Got 1000\n",
      "Got 2000\n",
      "Got 3000\n",
      "Got 4000\n",
      "Done, 4980\n",
      "Getting loan pairs and false friends\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4980/4980 [35:36<00:00,  2.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-French done\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pair in links:\n",
    "    if os.path.exists(\"results/{}.csv\".format(pair)):\n",
    "        overwrite = input(\"{}.csv exists. Overwrite existing file? (y/n) \".format(pair))\n",
    "    if overwrite == \"y\" or not os.path.exists(\"results/{}.csv\".format(pair)):\n",
    "        [dest, src] = pair.split('-')\n",
    "        print(pair)\n",
    "        words = get_all_borrowed_words(dest, src, invalid=[\"Unsupported titles/Space\"])\n",
    "        print(\"Getting loan pairs and false friends\")\n",
    "        loan_words, false_friends = get_all_loans_and_false_friends(words, dest, src)\n",
    "\n",
    "        df_loans = pd.DataFrame(loan_words, columns=['loan_word', 'original_word'])\n",
    "        df_false_friends = pd.DataFrame(false_friends, columns=['loan_word', 'original_word', 'other_etymology', 'other_meaning'])\n",
    "        df_loans.to_csv(\"results/{}.csv\".format(pair), index=False)\n",
    "        df_false_friends.to_csv(\"results/{}_false_friends.csv\".format(pair), index=False)\n",
    "        print(pair, \"done\\n\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
