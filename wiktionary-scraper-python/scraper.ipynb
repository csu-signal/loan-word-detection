{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import unicodeblock.blocks\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hindi-Persian': 'https://en.m.wiktionary.org/wiki/Category:Hindi_terms_borrowed_from_Persian',\n",
       " 'English-French': 'https://en.m.wiktionary.org/wiki/Category:English_terms_borrowed_from_French'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = {}\n",
    "\n",
    "with open('../language-pairs.json', 'r') as f:\n",
    "    pairs = json.loads(f.read())\n",
    "    \n",
    "    for pair in pairs:\n",
    "        links[pair] = pairs[pair]['wiki']\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_borrowed_words(dest, source, invalid=[\"Unsupported titles/Space\"]):\n",
    "    title = f\"Category:{dest}_terms_borrowed_from_{source}\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'exintro': True,\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    url = f\"https://en.wiktionary.org/w/api.php?action=query&list=categorymembers&cmtitle={title}&cmlimit=max\"\n",
    "    \n",
    "    borrowed_words = []\n",
    "    while(True):\n",
    "        r = requests.get(url,params)\n",
    "        try:\n",
    "            cmcontinue = r.json()['continue']['cmcontinue']\n",
    "            for cmember in r.json()['query']['categorymembers']:\n",
    "                if len(cmember['title']) > 1 and \\\n",
    "                    not cmember['title'].startswith('-') and \\\n",
    "                    not cmember['title'].endswith('-') and \\\n",
    "                    cmember['title'] not in invalid and \\\n",
    "                    unicodeblock.blocks.of(cmember['title'][0]) not in ['DIGIT', 'BASIC_PUNCTUATION']:\n",
    "                    borrowed_words.append(cmember['title'])\n",
    "                    if len(borrowed_words) % 1000 == 0:\n",
    "                        print(f\"Got {len(borrowed_words)}\")\n",
    "            url = url.split(\"&cmcontinue\")[0]\n",
    "            url+=f\"&cmcontinue={cmcontinue}\"\n",
    "        except KeyError:\n",
    "            break\n",
    "    print(f\"Done, {len(borrowed_words)}\")\n",
    "    return borrowed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_all_loans_and_false_friends:\n",
    "    for each word in borrowed words:\n",
    "        1. get word page from wiktionary\n",
    "        2. extract source word from source language\n",
    "        3. extract false friends from language!=source language\n",
    "\n",
    "    return loan_pairs, false_friends\n",
    "'''\n",
    "\n",
    "def get_source_word(soup, src_lang, lst_invalid_words):\n",
    "    for src_soup in soup.find_all(\"span\", class_=\"etyl\"):\n",
    "        if src_soup != None and src_soup.find(lambda tag: tag.name == 'a' and src_lang.lower() in tag.text.lower()):\n",
    "            \n",
    "            src_soup_final = src_soup.find_next(\"i\")\n",
    "            src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "            if src_word not in lst_invalid_words:\n",
    "                source_word = src_word\n",
    "            else:\n",
    "                src_soup_final = src_soup.find_next(\"strong\")\n",
    "                src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "                source_word = src_word\n",
    "            return source_word\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def get_false_friend(borrowed_word, soup, src_lang, dest_lang, lst_invalid_words):\n",
    "    all_false_friends = []\n",
    "    for header in soup.find_all(\"h2\", id=True):\n",
    "        if dest_lang.lower() in [header.get('id').lower(), header.text.lower()]:\n",
    "            try:\n",
    "                for next_header in header.parent.find_all('h3'):\n",
    "                    if re.match(\"Etymology [1-9]\",next_header.text):\n",
    "                        # get next sibling and then find_all span etyl?\n",
    "                        next_sibs = next_header.find_next_siblings()\n",
    "                        if len(next_sibs) > 0:\n",
    "                            all_etyms = next_sibs[0].find_all(\"span\", class_=\"etyl\")\n",
    "                            if len(all_etyms) > 0:\n",
    "                                etym = all_etyms[0]\n",
    "                                for tag in etym.find(lambda tag: tag.name == 'a'):\n",
    "\n",
    "                                    if src_lang.lower() not in tag.text.lower():\n",
    "                                        false_friend_soup = etym.find_next(\"i\")\n",
    "                                        ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                        ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "                                        if ff_word in lst_invalid_words:\n",
    "                                            false_friend_soup = etym.find_next(\"strong\")\n",
    "                                            ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                            ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "\n",
    "                                        meaning = etym.parent.parent.find_all('ol')[0].text.split('\\n')[0]\n",
    "                                        meaning = re.sub(r'\\(.+?\\)',r'',meaning)\n",
    "                                        meaning = re.sub(r'\\[.+?\\]',r'',meaning)\n",
    "                                        meaning = meaning.replace('  ', ' ')\n",
    "                                        meaning = re.split('[.;:]',meaning)[0].strip()\n",
    "\n",
    "                                        all_false_friends.append([borrowed_word,ff_word,tag.text.lower(),meaning])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return all_false_friends\n",
    "\n",
    "            \n",
    "def get_all_loans_and_false_friends(borrowed_words, dest_lang, src_lang, lst_invalid_words=['plural', 'not comparable', 'Urdu spelling'], min_timeout=10, timeout_after_words=100):\n",
    "\n",
    "    loan_pairs = [] # [[borrowed_word, source_word]]\n",
    "    false_friends = [] # \n",
    "    p_bar = tqdm(borrowed_words)\n",
    "    for i, word in enumerate(p_bar):\n",
    "        while True:\n",
    "            try:\n",
    "                params = {\n",
    "                    'action': 'query',\n",
    "                    'format': 'json',\n",
    "                    'prop': 'extracts',\n",
    "                    'exintro': True,\n",
    "                    'explaintext': True,\n",
    "                }\n",
    "\n",
    "                url = 'https://en.wiktionary.org/w/rest.php/v1/page/' + word + '/html'\n",
    "\n",
    "                response = requests.get(url,params, timeout=300)\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                source_word = get_source_word(\n",
    "                    soup, src_lang, lst_invalid_words)\n",
    "                false_friend = get_false_friend(\n",
    "                    word, soup, src_lang, dest_lang, lst_invalid_words)\n",
    "                loan_pairs.append([word, source_word])\n",
    "                false_friends.extend(false_friend)\n",
    "                p_bar.set_description(\"Processed word: {}\".format(word))\n",
    "                if i>0 and i%timeout_after_words==0:\n",
    "                    sleep_time = (random() * min_timeout) + min_timeout\n",
    "                    p_bar.set_description(\"Collected {} word pairs, sleeping for {}seconds\".format(i, sleep_time))\n",
    "                    time.sleep(sleep_time)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                p_bar.set_description(\"Word: {}, Error: {}, sleeping for 1 minute\".format(word, e))\n",
    "                time.sleep(60)\n",
    "\n",
    "        \n",
    "\n",
    "    return loan_pairs, false_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-French\n",
      "Got 1000\n",
      "Got 2000\n",
      "Got 3000\n",
      "Got 4000\n",
      "Done, 4981\n",
      "Getting loan pairs and false friends\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed word: souffle:  95%|█████████▍| 4720/4981 [1:13:12<05:25,  1.25s/it]                                                                     "
     ]
    }
   ],
   "source": [
    "for pair in links:\n",
    "    if os.path.exists(\"results/{}.csv\".format(pair)):\n",
    "        overwrite = input(\"{}.csv exists. Overwrite existing file? (y/n) \".format(pair))\n",
    "    if overwrite == \"y\" or not os.path.exists(\"results/{}.csv\".format(pair)):\n",
    "        [dest, src] = pair.split('-')\n",
    "        print(pair)\n",
    "        words = get_all_borrowed_words(dest, src, invalid=[\"Unsupported titles/Space\"])\n",
    "        print(\"Getting loan pairs and false friends\")\n",
    "        loan_words, false_friends = get_all_loans_and_false_friends(words, dest, src)\n",
    "\n",
    "        df_loans = pd.DataFrame(loan_words, columns=['loan_word', 'original_word'])\n",
    "        df_false_friends = pd.DataFrame(false_friends, columns=['loan_word', 'original_word', 'other_etymology', 'other_meaning'])\n",
    "        df_loans.to_csv(\"results/{}.csv\".format(pair), index=False)\n",
    "        df_false_friends.to_csv(\"results/{}_false_friends.csv\".format(pair), index=False)\n",
    "        # print(df_false_friends)\n",
    "        # print(df_loans)\n",
    "        print(pair, \"done\\n\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
