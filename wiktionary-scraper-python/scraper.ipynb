{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import unicodeblock.blocks\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Hindi-Persian': 'https://en.m.wiktionary.org/wiki/Category:Hindi_terms_borrowed_from_Persian',\n",
       "  'English-French': 'https://en.m.wiktionary.org/wiki/Category:English_terms_borrowed_from_French'},\n",
       " {'Hindi-Persian': 'Arabic', 'English-French': 'Latin'})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = {}\n",
    "expected_unicodes = {}\n",
    "\n",
    "with open('../language-pairs.json', 'r') as f:\n",
    "    pairs = json.loads(f.read())\n",
    "    \n",
    "    for pair in pairs:\n",
    "        expected_unicodes[pair] = pairs[pair]['source']['unicode']\n",
    "        links[pair] = pairs[pair]['wiki']\n",
    "\n",
    "links, expected_unicodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected Unicode blocks for certain scripts\n",
    "unicode_block_map = {\n",
    "    \"Latin\" : ['BASIC_LATIN', 'LATIN_1_SUPPLEMENT', 'LATIN_EXTENDED_LETTER', 'LATIN_EXTENDED_A', 'LATIN_EXTENDED_B',\\\n",
    "               'LATIN_EXTENDED_C'],\n",
    "    \"Greek\" : ['GREEK'],\n",
    "    \"Cyrillic\" : ['CYRILLIC', 'CYRILLIC_SUPPLEMENTARY', 'CYRILLIC_EXTENDED_A', 'CYRILLIC_EXTENDED_B'],\n",
    "    \"Arabic\" : ['ARABIC', 'ARABIC_SUPPLEMENT', 'ARABIC_PRESENTATION_FORMS_A', 'ARABIC_PRESENTATION_FORMS_B'],\n",
    "    \"Devanagari\" : ['DEVANAGARI', 'VEDIC_EXTENSIONS', 'DEVANAGARI_EXTENDED'],\n",
    "    \"Bengali\" : ['BENGALI'],\n",
    "    \"Gurmukhi\" : ['GURMUKHI'],\n",
    "    \"Tamil\" : ['TAMIL'],\n",
    "    \"Telugu\" : ['TELUGU'],\n",
    "    \"Malayalam\" : ['MALAYALAM'],\n",
    "    \"Myanmar\" : ['MYANMAR', 'MYANMAR_EXTENDED_A'],\n",
    "    \"Chinese\" : ['CJK_RADICALS_SUPPLEMENT', 'CJK_SYMBOLS_AND_PUNCTUATION', 'CJK_STROKES',\\\n",
    "                 'ENCLOSED_CJK_LETTERS_AND_MONTHS', 'CJK_COMPATIBILITY', 'CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A',\\\n",
    "                 'CJK_UNIFIED_IDEOGRAPHS', 'CJK_COMPATIBILITY_IDEOGRAPHS', 'CJK_COMPATIBILITY_FORMS',\\\n",
    "                 'CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B', 'CJK_UNIFIED_IDEOGRAPHS_EXTENSION_C',\\\n",
    "                 'CJK_UNIFIED_IDEOGRAPHS_EXTENSION_D', 'CJK_COMPATIBILITY_IDEOGRAPHS_SUPPLEMENT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_borrowed_words(dest, source, expected_unicode, invalid=[\"Unsupported titles/Space\"]):\n",
    "    title = f\"Category:{dest}_terms_borrowed_from_{source}\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'exintro': True,\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    url = f\"https://en.wiktionary.org/w/api.php?action=query&list=categorymembers&cmtitle={title}&cmlimit=max\"\n",
    "    \n",
    "    borrowed_words = []\n",
    "    while(True):\n",
    "        r = requests.get(url,params)\n",
    "        try:\n",
    "            for cmember in r.json()['query']['categorymembers']:\n",
    "                if len(cmember['title']) > 1 and \\\n",
    "                    not cmember['title'].startswith('-') and \\\n",
    "                    not cmember['title'].endswith('-') and \\\n",
    "                    cmember['title'] not in invalid and \\\n",
    "                    not any(c.isdigit() for c in cmember['title']) and \\\n",
    "                    not cmember['title'].isupper() and \\\n",
    "                    unicodeblock.blocks.of(cmember['title'][0]) not in ['BASIC_PUNCTUATION']:\n",
    "                    borrowed_words.append(cmember['title'])\n",
    "                    if len(borrowed_words) % 1000 == 0:\n",
    "                        print(f\"Got {len(borrowed_words)}\")\n",
    "            cmcontinue = r.json()['continue']['cmcontinue']\n",
    "            url = url.split(\"&cmcontinue\")[0]\n",
    "            url+=f\"&cmcontinue={cmcontinue}\"\n",
    "        except KeyError:\n",
    "            break\n",
    "    print(f\"Done, {len(borrowed_words)}\")\n",
    "    return borrowed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_all_loans_and_false_friends:\n",
    "    for each word in borrowed words:\n",
    "        1. get word page from wiktionary\n",
    "        2. extract source word from source language\n",
    "        3. extract false friends from language!=source language\n",
    "\n",
    "    return loan_pairs, false_friends\n",
    "'''\n",
    "\n",
    "def get_source_word(soup, src_lang, expected_unicode, lst_invalid_words):\n",
    "    for src_soup in soup.find_all(\"span\", class_=\"etyl\"):\n",
    "        if src_soup != None and src_soup.find(lambda tag: tag.name == 'a' and src_lang.lower() in tag.text.lower()):\n",
    "            \n",
    "            src_soup_final = src_soup.find_next(\"i\")\n",
    "            src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "            if any(inv in src_word for inv in lst_invalid_words):\n",
    "                src_soup_final = src_soup.find_next(\"strong\")\n",
    "                src_word = src_soup_final.text if src_soup_final != None else ''\n",
    "                source_word = re.split('[/,]',src_word)[0].strip()\n",
    "            else:\n",
    "                source_word = re.split('[/,]',src_word)[0].strip()                \n",
    "                \n",
    "            if [unicodeblock.blocks.of(c) in unicode_block_map[expected_unicode] for c in source_word].count(True) < len(source_word)*.8:\n",
    "                source_word = ''\n",
    "                \n",
    "            return source_word\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def get_false_friend(borrowed_word, soup, src_lang, dest_lang, lst_invalid_words):\n",
    "    all_false_friends = []\n",
    "    for header in soup.find_all(\"h2\", id=True):\n",
    "        if dest_lang.lower() in [header.get('id').lower(), header.text.lower()]:\n",
    "            try:\n",
    "                for next_header in header.parent.find_all('h3'):\n",
    "                    if re.match(\"Etymology [1-9]\",next_header.text):\n",
    "                        # get next sibling and then find_all span etyl?\n",
    "                        next_sibs = next_header.find_next_siblings()\n",
    "                        if len(next_sibs) > 0:\n",
    "                            all_etyms = next_sibs[0].find_all(\"span\", class_=\"etyl\")\n",
    "                            if len(all_etyms) > 0:\n",
    "                                etym = all_etyms[0]\n",
    "                                for tag in etym.find(lambda tag: tag.name == 'a'):\n",
    "\n",
    "                                    if src_lang.lower() not in tag.text.lower():\n",
    "                                        false_friend_soup = etym.find_next(\"i\")\n",
    "                                        ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                        ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "                                        if any(inv in ff_word for inv in lst_invalid_words):\n",
    "                                            false_friend_soup = etym.find_next(\"strong\")\n",
    "                                            ff_word = false_friend_soup.text if false_friend_soup != None else ''\n",
    "                                            ff_word = re.split('[,;:]',ff_word)[0].strip()\n",
    "\n",
    "                                        meaning = etym.parent.parent.find_all('ol')[0].text.split('\\n')[0]\n",
    "                                        meaning = re.sub(r'\\(.+?\\)',r'',meaning)\n",
    "                                        meaning = re.sub(r'\\[.+?\\]',r'',meaning)\n",
    "                                        meaning = meaning.replace('  ', ' ')\n",
    "                                        meaning = re.split('[.;:]',meaning)[0].strip()\n",
    "\n",
    "                                        all_false_friends.append([borrowed_word,ff_word,tag.text.lower(),meaning])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return all_false_friends\n",
    "\n",
    "            \n",
    "def get_all_loans_and_false_friends(borrowed_words, dest_lang, src_lang, expected_unicode, lst_invalid_words=['plural', 'not comparable', ' spelling'], min_timeout=5, timeout_after_words=200):\n",
    "    loan_pairs = [] # [[borrowed_word, source_word]]\n",
    "    false_friends = [] # \n",
    "    p_bar = tqdm(borrowed_words)\n",
    "    for i, word in enumerate(p_bar):\n",
    "        while True:\n",
    "            try:\n",
    "                params = {\n",
    "                    'action': 'query',\n",
    "                    'format': 'json',\n",
    "                    'prop': 'extracts',\n",
    "                    'exintro': True,\n",
    "                    'explaintext': True,\n",
    "                }\n",
    "\n",
    "                url = 'https://en.wiktionary.org/w/rest.php/v1/page/' + word + '/html'\n",
    "\n",
    "                response = requests.get(url, params, timeout=300)\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                source_word = get_source_word(\n",
    "                    soup, src_lang, expected_unicode, lst_invalid_words)\n",
    "                false_friend = get_false_friend(\n",
    "                    word, soup, src_lang, dest_lang, lst_invalid_words)\n",
    "                if source_word != '':\n",
    "                    loan_pairs.append([word, source_word])\n",
    "                false_friends.extend(false_friend)\n",
    "                p_bar.set_description(\"Processed word: {}\".format(word))\n",
    "                if i > 0 and i % timeout_after_words == 0:\n",
    "                    sleep_time = (random() * min_timeout) + min_timeout\n",
    "                    p_bar.set_description(\"Collected {} word pairs, sleeping for {} seconds\".format(i, sleep_time))\n",
    "                    time.sleep(sleep_time)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                p_bar.set_description(\"Word: {}, Error: {}, sleeping for 1 minute\".format(word, e))\n",
    "                time.sleep(60)\n",
    "\n",
    "    return loan_pairs, false_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi-Persian\n",
      "Got 1000\n",
      "Done, 1338\n",
      "Hindi-Persian.csv exists. Overwrite existing file? (y/n) y\n",
      "Hindi-Persian_false_friends.csv exists. Overwrite existing file? (y/n) y\n",
      "Getting loan pairs and false friends: 1338 candidates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed word: Category:Hindi terms calqued from Persian: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1338/1338 [10:36<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi-Persian done\n",
      "\n",
      "\n",
      "English-French\n",
      "Got 1000\n",
      "Got 2000\n",
      "Got 3000\n",
      "Got 4000\n",
      "Got 5000\n",
      "Done, 5206\n",
      "English-French.csv exists. Overwrite existing file? (y/n) y\n",
      "English-French_false_friends.csv exists. Overwrite existing file? (y/n) y\n",
      "Getting loan pairs and false friends: 5206 candidates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed word: Category:English unadapted borrowings from French: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5206/5206 [41:09<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-French done\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pair in links:\n",
    "    [dest, src] = pair.split('-')\n",
    "    print(pair)\n",
    "    words = get_all_borrowed_words(dest, src, expected_unicodes[pair], invalid=[\"Unsupported titles/Space\"])\n",
    "    \n",
    "    overwrite_loans = None\n",
    "    overwrite_ff = None\n",
    "    \n",
    "    if os.path.exists(\"results/{}.csv\".format(pair)):\n",
    "        overwrite_loans = input(\"{}.csv exists. Overwrite existing file? (y/n) \".format(pair))\n",
    "    else:\n",
    "        overwrite_loans = \"y\"\n",
    "        \n",
    "    if os.path.exists(\"results/{}_false_friends.csv\".format(pair)):\n",
    "        overwrite_ff = input(\"{}_false_friends.csv exists. Overwrite existing file? (y/n) \".format(pair))\n",
    "    else:\n",
    "        overwrite_ff = \"y\"\n",
    "    \n",
    "    if overwrite_loans == \"y\" or overwrite_ff == \"y\":\n",
    "        print(\"Getting loan pairs and false friends: {} candidates\".format(len(words)))\n",
    "        loan_words, false_friends = get_all_loans_and_false_friends(words, dest, src, expected_unicodes[pair])\n",
    "\n",
    "        df_loans = pd.DataFrame(loan_words, columns=['loan_word', 'original_word'])\n",
    "        df_false_friends = pd.DataFrame(false_friends, columns=['loan_word', 'original_word', 'other_etymology', 'other_meaning'])\n",
    "\n",
    "        if overwrite_loans == \"y\":\n",
    "            df_loans.to_csv(\"results/{}.csv\".format(pair), index=False)\n",
    "            \n",
    "        if overwrite_ff == \"y\":\n",
    "            df_false_friends.to_csv(\"results/{}_false_friends.csv\".format(pair), index=False)\n",
    "\n",
    "    print(pair, \"done\\n\")\n",
    "    print()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
